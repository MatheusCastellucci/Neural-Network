{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7659ebb7",
   "metadata": {},
   "source": [
    "# 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c420e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4583f",
   "metadata": {},
   "source": [
    "## Activity: Understanding Multi-Layer Perceptrons (MLPs)\n",
    "This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90855a34",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730f61a",
   "metadata": {},
   "source": [
    "Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): L = 1/N * (y - ŷ)², where ŷ is the network's output.\n",
    "\n",
    "For this exercise, use the following specific values:\n",
    "\n",
    "* Input and output vectors:\n",
    "    * X: [0.5, -0.2]\n",
    "    * Y: 1.0\n",
    "\n",
    "* Hidden layer weights:\n",
    "    * W¹ = [[0.3, -0.1], \n",
    "             [0.2, 0.4]]  (2x2 matrix)\n",
    " \n",
    "* Hidden layer biases:\n",
    "    * b¹ = [0.1, -0.2]  (1x2 vector)\n",
    "\n",
    "* Output layer weights:\n",
    "    * W² = [0.5, -0.3]\n",
    "\n",
    "* Output layer bias:\n",
    "    * b² = 0.2\n",
    "\n",
    "* Learning rate: \n",
    "    * η = 0.3\n",
    "\n",
    "* Activation function: tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae134e8f",
   "metadata": {},
   "source": [
    "Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   * Compute the hidden layer pre-activations: Z¹ = W¹ * X + b¹.\n",
    "   * Apply tanh to get hidden activations: a¹ = tanh(Z¹).\n",
    "   * Compute the output pre-activation: Z² = W² * a¹ + b².\n",
    "   * Compute the final output: ŷ = tanh(Z²).\n",
    "\n",
    "2. **Loss Calculation**:\n",
    "   * Compute the loss: L = 1/N * (Y - ŷ)².\n",
    "\n",
    "3. **Backward Pass (Backpropagation)**: Compute the gradients of the loss with respect to all weights and biases. Start with delL/delŷ then compute:\n",
    "   * delL/delZ² (using the tanh derivative: del/delZ tanh(Z) = 1 - tanh²(Z)).\n",
    "   * Gradients for output layer: delL/delW², delL/delb².\n",
    "   * Propagate to hidden layer: delL/delA¹, delL/delZ¹.\n",
    "   * Gradients for hidden layer: delL/delW¹, delL/delb¹.\n",
    "   * Show all intermediate steps and calculations.\n",
    "\n",
    "4. **Parameter Update**: Using the learning rate η = 0.1, update all weights and biases via gradient descent:\n",
    "   * W² <- W² - η * delL/delW²\n",
    "   * b² <- b² - η * delL/delb²\n",
    "   * W¹ <- W¹ - η * delL/delW¹\n",
    "   * b¹ <- b¹ - η * delL/delb¹\n",
    "   * Provide the numerical values for all updated parameters.\n",
    "\n",
    "**Submission Requirements**: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97f3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXERCÍCIO 1 — MLP (tanh, MSE) ===\n",
      "\n",
      "X: [0.500000 -0.200000]\n",
      "Y: 1.000000\n",
      "\n",
      "--- Parâmetros iniciais ---\n",
      "W1: [[0.300000 -0.100000]\n",
      " [0.200000 0.400000]]\n",
      "b1: [0.100000 -0.200000]\n",
      "W2: [0.500000 -0.300000]\n",
      "b2: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# --- Helpers ---\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(z):\n",
    "    return 1.0 - np.tanh(z)**2\n",
    "\n",
    "def fmt(x):\n",
    "    if isinstance(x, float):\n",
    "        return f\"{x:.6f}\"\n",
    "    arr = np.array(x, dtype=float)\n",
    "    return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},\n",
    "                           floatmode='maxprec', suppress_small=False)\n",
    "\n",
    "def p(title, value):\n",
    "    print(f\"{title}: {fmt(value)}\")\n",
    "\n",
    "# --- Dados do exercício ---\n",
    "X = np.array([0.5, -0.2], dtype=float)\n",
    "Y = 1.0 \n",
    "\n",
    "W1 = np.array([[0.3, -0.1],\n",
    "               [0.2,  0.4]], dtype=float)\n",
    "\n",
    "b1 = np.array([0.1, -0.2], dtype=float)\n",
    "\n",
    "W2 = np.array([0.5, -0.3], dtype=float)\n",
    "b2 = 0.2\n",
    "\n",
    "eta_update = 0.1\n",
    "\n",
    "print(\"=== EXERCÍCIO 1 — MLP (tanh, MSE) ===\\n\")\n",
    "p(\"X\", X); p(\"Y\", Y)\n",
    "print(\"\\n--- Parâmetros iniciais ---\")\n",
    "p(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f468ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Forward Pass ---\n",
      "Z1 = W1 @ X + b1: [0.270000 -0.180000]\n",
      "A1 = tanh(Z1): [0.263625 -0.178081]\n",
      "Z2 = W2 · A1 + b2: 0.385237\n",
      "ŷ = tanh(Z2): 0.367247\n",
      "\n",
      "--- Loss (MSE) ---\n",
      "L = (Y - ŷ)^2: 0.400377\n"
     ]
    }
   ],
   "source": [
    "# === 1) Forward pass ===\n",
    "Z1 = W1 @ X + b1\n",
    "A1 = tanh(Z1)\n",
    "Z2 = float(W2 @ A1 + b2)\n",
    "Y_hat = float(tanh(Z2))\n",
    "\n",
    "print(\"\\n--- Forward Pass ---\")\n",
    "p(\"Z1 = W1 @ X + b1\", Z1)\n",
    "p(\"A1 = tanh(Z1)\", A1)\n",
    "p(\"Z2 = W2 · A1 + b2\", Z2)\n",
    "p(\"ŷ = tanh(Z2)\", Y_hat)\n",
    "\n",
    "# === 2) Loss ===\n",
    "L = (Y - Y_hat)**2\n",
    "print(\"\\n--- Loss (MSE) ---\")\n",
    "p(\"L = (Y - ŷ)^2\", L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bec368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Saída --\n",
      "dL/dŷ = 2*(ŷ - Y): -1.265507\n",
      "dtanh(Z2) = 1 - tanh^2(Z2): 0.865130\n",
      "dL/dZ2: -1.094828\n",
      "\n",
      "-- Gradientes camada de saída --\n",
      "dL/dW2 = dL/dZ2 * A1: [-0.288624 0.194968]\n",
      "dL/db2 = dL/dZ2: -1.094828\n",
      "\n",
      "-- Propagação para a oculta --\n",
      "dL/dA1 = dL/dZ2 * W2: [-0.547414 0.328448]\n",
      "dtanh(Z1) = 1 - tanh^2(Z1): [0.930502 0.968287]\n",
      "dL/dZ1 = dL/dA1 ⊙ dtanh(Z1): [-0.509370 0.318032]\n",
      "\n",
      "-- Gradientes camada oculta --\n",
      "dL/dW1 = outer(dL/dZ1, X): [[-0.254685 0.101874]\n",
      " [0.159016 -0.063606]]\n",
      "dL/db1 = dL/dZ1: [-0.509370 0.318032]\n"
     ]
    }
   ],
   "source": [
    "# === 3) Backpropagation ===\n",
    "\n",
    "# Saída\n",
    "dL_dYhat = 2.0*(Y_hat - Y)\n",
    "dYhat_dZ2 = dtanh(Z2)\n",
    "dL_dZ2 = dL_dYhat * dYhat_dZ2\n",
    "\n",
    "print(\"\\n-- Saída --\")\n",
    "p(\"dL/dŷ = 2*(ŷ - Y)\", dL_dYhat)\n",
    "p(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2)\n",
    "p(\"dL/dZ2\", dL_dZ2)\n",
    "\n",
    "# Gradientes da camada de saída\n",
    "dL_dW2 = dL_dZ2 * A1            # (2,)\n",
    "dL_db2 = dL_dZ2                 # escalar\n",
    "\n",
    "print(\"\\n-- Gradientes camada de saída --\")\n",
    "p(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2)\n",
    "p(\"dL/db2 = dL/dZ2\", dL_db2)\n",
    "\n",
    "# Propagação p/ camada oculta\n",
    "dL_dA1 = dL_dZ2 * W2            # (2,)\n",
    "dA1_dZ1 = dtanh(Z1)             # (2,)\n",
    "dL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)\n",
    "\n",
    "print(\"\\n-- Propagação para a oculta --\")\n",
    "p(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1)\n",
    "p(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1)\n",
    "p(\"dL/dZ1 = dL/dA1 ⊙ dtanh(Z1)\", dL_dZ1)\n",
    "\n",
    "# Gradientes da camada oculta\n",
    "dL_dW1 = np.outer(dL_dZ1, X)    # (2,2)\n",
    "dL_db1 = dL_dZ1                 # (2,)\n",
    "\n",
    "print(\"\\n-- Gradientes camada oculta --\")\n",
    "p(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1)\n",
    "p(\"dL/db1 = dL/dZ1\", dL_db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37ab68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W2_new = W2 - η*dL/dW2: [0.528862 -0.319497]\n",
      "b2_new = b2 - η*dL/db2: 0.309483\n",
      "W1_new = W1 - η*dL/dW1: [[0.325468 -0.110187]\n",
      " [0.184098 0.406361]]\n",
      "b1_new = b1 - η*dL/db1: [0.150937 -0.231803]\n"
     ]
    }
   ],
   "source": [
    "# === 4) Atualização de parâmetros (η = 0.1) ===\n",
    "W2_new = W2 - eta_update * dL_dW2\n",
    "b2_new = b2 - eta_update * dL_db2\n",
    "W1_new = W1 - eta_update * dL_dW1\n",
    "b1_new = b1 - eta_update * dL_db1\n",
    "\n",
    "p(\"\\nW2_new = W2 - η*dL/dW2\", W2_new)\n",
    "p(\"b2_new = b2 - η*dL/db2\", b2_new)\n",
    "p(\"W1_new = W1 - η*dL/dW1\", W1_new)\n",
    "p(\"b1_new = b1 - η*dL/db1\", b1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e55e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ŷ (antes): 0.367247\n",
      "L (antes): 0.400377\n",
      "ŷ (depois): 0.500620\n",
      "L (depois): 0.249380\n"
     ]
    }
   ],
   "source": [
    "# === Checagem opcional: forward com parâmetros atualizados ===\n",
    "Z1_new = W1_new @ X + b1_new\n",
    "A1_new = tanh(Z1_new)\n",
    "Z2_new = float(W2_new @ A1_new + b2_new)\n",
    "Y_hat_new = float(tanh(Z2_new))\n",
    "L_new = (Y - Y_hat_new)**2\n",
    "\n",
    "p(\"\\nŷ (antes)\", Y_hat)\n",
    "p(\"L (antes)\", L)\n",
    "p(\"ŷ (depois)\", Y_hat_new)\n",
    "p(\"L (depois)\", L_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6d338",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e533c",
   "metadata": {},
   "source": [
    "Using the `make_classification` function from scikit-learn, generate a synthetic dataset with the following specifications:\n",
    "\n",
    "* Number of samples: 1000\n",
    "\n",
    "* Number of classes: 2\n",
    "\n",
    "* Number of clusters per class: Use the n_clusters_per_class parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).\n",
    "\n",
    "* Other parameters: Set `n_features=2` for easy visualization, `n_informative=2`, `n_redundant=0`, `random_state=42` for reproducibility, and adjust `class_sep` or `flip_y` as needed for a challenging but separable dataset.\n",
    "\n",
    "Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:\n",
    "\n",
    "* Number of hidden layers (at least 1)\n",
    "\n",
    "* Number of neurons per layer\n",
    "\n",
    "* Activation functions (e.g., sigmoid, ReLU, tanh)\n",
    "\n",
    "* Loss function (e.g., binary cross-entropy)\n",
    "\n",
    "* Optimizer (e.g., gradient descent, with a chosen learning rate)\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1. Generate and split the data into training (80%) and testing (20%) sets.\n",
    "\n",
    "2. Implement the forward pass, loss computation, backward pass, and parameter updates in code.\n",
    "\n",
    "3. Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.\n",
    "\n",
    "4. Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.\n",
    "\n",
    "5. Submit your code and results, including any visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c3fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
