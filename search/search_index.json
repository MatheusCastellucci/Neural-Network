{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ementa","text":"Teste GitHub Pages Teste com Lorem Ipsum <p>Esse \u00e9 um exemplo de p\u00e1gina simples para GitHub Pages.</p> Se\u00e7\u00e3o 1 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet felis in nunc fringilla ullamcorper. Proin non lacus vitae ligula pulvinar facilisis.</p> Se\u00e7\u00e3o 2 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vitae venenatis ligula. Ut malesuada augue nec mi tempor, eu malesuada libero hendrerit.</p> Se\u00e7\u00e3o 3 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tincidunt, ipsum at sagittis tincidunt, risus ipsum cursus lorem, non dictum ipsum sapien quis elit.</p> <p>Rodap\u00e9 - P\u00e1gina de Teste</p>"},{"location":"Exercicios/EX1/data/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class</p> In\u00a0[2]: Copied! <pre>np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.\n\n# M\u00e9dias e desvios para cada classe\nmeans = [(2, 3), (5, 6), (8, 1), (15, 4)]\nstds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]\n\ndata = []\nlabels = []\n\nfor i in range(len(means)):\n    mean = means[i]\n    std = stds[i]\n\n    x = np.random.normal(loc=mean[0], scale=std[0], size=100)\n    y = np.random.normal(loc=mean[1], scale=std[1], size=100)\n\n    points = []\n    for j in range(100):\n        points.append([x[j], y[j]])\n\n    data.extend(points)\n\n    for j in range(100):\n        labels.append(i)\n\ndata = np.array(data)\nlabels = np.array(labels)\n</pre> np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.  # M\u00e9dias e desvios para cada classe means = [(2, 3), (5, 6), (8, 1), (15, 4)] stds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]  data = [] labels = []  for i in range(len(means)):     mean = means[i]     std = stds[i]      x = np.random.normal(loc=mean[0], scale=std[0], size=100)     y = np.random.normal(loc=mean[1], scale=std[1], size=100)      points = []     for j in range(100):         points.append([x[j], y[j]])      data.extend(points)      for j in range(100):         labels.append(i)  data = np.array(data) labels = np.array(labels) <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nfor cls in range(4):\n    plt.scatter(data[labels == cls, 0],\n                data[labels == cls, 1],\n                label=f'Classe {cls}',\n                alpha=0.7)\nplt.legend()\nplt.xlabel('Eixo X')\nplt.ylabel('Eixo Y')\nplt.title('Dispers\u00e3o das 4 classes em 2D')\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) for cls in range(4):     plt.scatter(data[labels == cls, 0],                 data[labels == cls, 1],                 label=f'Classe {cls}',                 alpha=0.7) plt.legend() plt.xlabel('Eixo X') plt.ylabel('Eixo Y') plt.title('Dispers\u00e3o das 4 classes em 2D') plt.show() <p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p>Answers</p> <ol> <li>O scatter plot mostra que as quatro classes est\u00e3o distribuidas de forma relativamente clara, com alguma sobreposi\u00e7\u00e3o entre elas. As classes 0 e 1 est\u00e3o mais pr\u00f3ximas uma da outra, enquanto as classes 2 ainda encosta um pouco na classe 1, e longe de todas as outras temos a classe 3.</li> <li>N\u00e3o, uma fronteira linear simples n\u00e3o seria capaz de separar todas as classes de forma eficaz, especialmente devido \u00e0 sobreposi\u00e7\u00e3o entre as classes 0 e 1.</li> <li>Pode ser visto no gr\u00e1fico abaixo:</li> </ol> <p></p> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B.</p> In\u00a0[5]: Copied! <pre>mu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1.0]])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],\n                    [-0.7, 1.5,  0.4, 0.0, 0.0],\n                    [0.2,  0.4,  1.5, 0.6, 0.0],\n                    [0.0,  0.0,  0.6, 1.5, 0.3],\n                    [0.0,  0.0,  0.0, 0.3, 1.5]])\n\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)\n</pre> mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],                     [0.8, 1.0, 0.3, 0.0, 0.0],                     [0.1, 0.3, 1.0, 0.5, 0.0],                     [0.0, 0.0, 0.5, 1.0, 0.2],                     [0.0, 0.0, 0.0, 0.2, 1.0]])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],                     [-0.7, 1.5,  0.4, 0.0, 0.0],                     [0.2,  0.4,  1.5, 0.6, 0.0],                     [0.0,  0.0,  0.6, 1.5, 0.3],                     [0.0,  0.0,  0.0, 0.3, 1.5]])  XA = np.random.multivariate_normal(mu_A, Sigma_A, size=500) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack([XA, XB]) y = np.array([0]*500 + [1]*500) <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>PCA:</p> <ol> <li>Centralizar os dados (tirar a m\u00e9dia).</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Extrair autovalores e autovetores da matriz de covari\u00e2ncia.</li> <li>Ordenar autovetores pelos maiores autovalores.</li> <li>Projetar os dados nos autovetores escolhidos.</li> </ol> In\u00a0[6]: Copied! <pre>def my_pca(X, n_components=None):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered, rowvar=False)\n    autovalores, autovetores = np.linalg.eigh(cov_matrix)\n\n    sorted_idx = np.argsort(autovalores)[::-1]\n    autovalores = autovalores[sorted_idx]\n    autovetores = autovetores[:, sorted_idx]\n\n    total_var = np.sum(autovalores)\n\n    if n_components is not None:\n        autovetores = autovetores[:, :n_components]\n        autovalores = autovalores[:n_components]\n\n    X_pca = np.dot(X_centered, autovetores)\n\n    return X_pca, autovetores, autovalores, total_var\n</pre> def my_pca(X, n_components=None):     X_centered = X - np.mean(X, axis=0)     cov_matrix = np.cov(X_centered, rowvar=False)     autovalores, autovetores = np.linalg.eigh(cov_matrix)      sorted_idx = np.argsort(autovalores)[::-1]     autovalores = autovalores[sorted_idx]     autovetores = autovetores[:, sorted_idx]      total_var = np.sum(autovalores)      if n_components is not None:         autovetores = autovetores[:, :n_components]         autovalores = autovalores[:n_components]      X_pca = np.dot(X_centered, autovetores)      return X_pca, autovetores, autovalores, total_var In\u00a0[7]: Copied! <pre>Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\nplt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)')\nplt.legend(); plt.show()\n\nprint('Vari\u00e2ncia explicada:', autovalores / total_var)\nprint('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var))\n</pre> Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)  plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A') plt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B') plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)') plt.legend(); plt.show()  print('Vari\u00e2ncia explicada:', autovalores / total_var) print('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var)) <pre>Vari\u00e2ncia explicada: [0.52303265 0.15751841]\nVari\u00e2ncia Acumulada: 0.6805510605944183\n</pre> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> <p>Answers</p> <ol> <li>As duas classes est\u00e3o bem pr\u00f3ximas uma da outra, com uma quantidade significativa de sobreposi\u00e7\u00e3o. Caso n\u00e3o fossem duas classes diferentes, poder\u00edamos considerar que se tratam de uma \u00fanica classe.</li> <li>N\u00e3o \u00e9 possivel tra\u00e7ar uma linha que separa as duas classes de forma eficaz, sempre haver\u00e1 uma \u00e1rea de sobreposi\u00e7\u00e3o. Modelos lineares simples, como um Perceptron ou Regress\u00e3o Log\u00edstica, v\u00e3o ter dificuldade porque s\u00f3 conseguem aprender fronteiras lineares (hiperplanos). Eles errariam bastante nos pontos da regi\u00e3o central de overlap.</li> </ol> <p>2. Describe the Data:</p> <ol> <li>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</li> <li>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ol> <p>Answers:</p> <ol> <li><p>O dataset busca prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da Spaceship Titanic. Isso pode ser visto na coluna \"Transported\", que \u00e9 a coluna-alvo (sendo booleano).</p> </li> <li><p>Features: num\u00e9ricas vs categ\u00f3ricas</p> <p>Num\u00e9ricas</p> <ul> <li><p>Age \u2192 idade do passageiro</p> </li> <li><p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 valores gastos em diferentes servi\u00e7os</p> </li> </ul> <p>Categ\u00f3ricas</p> <ul> <li><p>HomePlanet \u2192 planeta de origem (ex: Earth, Europa, Mars)</p> </li> <li><p>CryoSleep \u2192 booleano (se o passageiro entrou em sono criog\u00eanico)</p> </li> <li><p>Cabin \u2192 cont\u00e9m m\u00faltiplas infos (deck/num/side). Pode ser decomposta em:</p> <ul> <li><p>Deck (categ\u00f3rica)</p> </li> <li><p>Num (num\u00e9rica)</p> </li> <li><p>Side (P/S \u2192 categ\u00f3rica bin\u00e1ria)</p> </li> </ul> </li> <li><p>Destination \u2192 destino da viagem (ex: TRAPPIST-1e, etc.)</p> </li> <li><p>VIP \u2192 booleano (pagou servi\u00e7o VIP)</p> </li> <li><p>Name \u2192 geralmente descartado (n\u00e3o tem valor preditivo direto)</p> </li> <li><p>PassengerId \u2192 identificador \u00fanico (n\u00e3o usado como feature)</p> </li> </ul> </li> <li><p>Colunas com valores faltantes:</p> </li> </ol> In\u00a0[8]: Copied! <pre>csv_path = \"train.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\nprint(\"Colunas:\", list(df.columns))\n</pre> csv_path = \"train.csv\" df = pd.read_csv(csv_path) print(\"Shape:\", df.shape) print(\"Colunas:\", list(df.columns)) <pre>Shape: (8693, 14)\nColunas: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n</pre> In\u00a0[9]: Copied! <pre>missing = df.isnull().sum()\nmissing_percent = 100 * missing / len(df)\nmissing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent})\nprint(missing_df[missing_df[\"Missing Values\"] &gt; 0])\n</pre> missing = df.isnull().sum() missing_percent = 100 * missing / len(df) missing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent}) print(missing_df[missing_df[\"Missing Values\"] &gt; 0]) <pre>              Missing Values   Percent\nHomePlanet               201  2.312205\nCryoSleep                217  2.496261\nCabin                    199  2.289198\nDestination              182  2.093639\nAge                      179  2.059128\nVIP                      203  2.335212\nRoomService              181  2.082135\nFoodCourt                183  2.105142\nShoppingMall             208  2.392730\nSpa                      183  2.105142\nVRDeck                   188  2.162660\nName                     200  2.300702\n</pre> <p>3. Preprocessing the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ol> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ol> In\u00a0[10]: Copied! <pre>df_processed = df.copy()\n\n\"\"\"Num\u00e9ricas: \nSolu\u00e7\u00e3o: Preencher com mediana, \nJustificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o.\n\"\"\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    median_val = df_processed[col].median()\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(median_val, inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")\n</pre> df_processed = df.copy()  \"\"\"Num\u00e9ricas:  Solu\u00e7\u00e3o: Preencher com mediana,  Justificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o. \"\"\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] for col in num_cols:     median_val = df_processed[col].median()     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(median_val, inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")  <pre>   - Age: 179 valores preenchidos com 27.00\n   - RoomService: 181 valores preenchidos com 0.00\n   - FoodCourt: 183 valores preenchidos com 0.00\n   - ShoppingMall: 208 valores preenchidos com 0.00\n   - Spa: 183 valores preenchidos com 0.00\n   - VRDeck: 188 valores preenchidos com 0.00\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\n</pre> In\u00a0[11]: Copied! <pre>\"\"\"Categ\u00f3ricas: \nSolu\u00e7\u00e3o: Preencher com \"Unknown\"\nJustificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios.\n\"\"\"\ncat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]\nfor col in cat_cols:\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(\"Unknown\", inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos\")\n</pre> \"\"\"Categ\u00f3ricas:  Solu\u00e7\u00e3o: Preencher com \"Unknown\" Justificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios. \"\"\" cat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"] for col in cat_cols:     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(\"Unknown\", inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos\")  <pre>   - HomePlanet: 201 valores preenchidos\n   - Destination: 182 valores preenchidos\n   - CryoSleep: 217 valores preenchidos\n   - VIP: 203 valores preenchidos\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\797796973.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(\"Unknown\", inplace=True)\n</pre> In\u00a0[12]: Copied! <pre>\"\"\"Cabin: \nSolu\u00e7\u00e3o: Separar em 3 colunas\nJustificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes.\n\"\"\"\ncabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True)\ndf_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\")\ndf_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")\n\n# Preencher CabinNum com mediana\ncabin_num_median = df_processed[\"CabinNum\"].median()\ndf_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> \"\"\"Cabin:  Solu\u00e7\u00e3o: Separar em 3 colunas Justificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes. \"\"\" cabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True) df_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\") df_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\") df_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")  # Preencher CabinNum com mediana cabin_num_median = df_processed[\"CabinNum\"].median() df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True) <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_20232\\2304158112.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> In\u00a0[13]: Copied! <pre>\"\"\"Name: \nSolu\u00e7\u00e3o: descartar\nJustificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido.\n\"\"\"\ndf_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)    \n\nprint(\"Ap\u00f3s tratamento:\", df_processed.shape)\n</pre> \"\"\"Name:  Solu\u00e7\u00e3o: descartar Justificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido. \"\"\" df_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)      print(\"Ap\u00f3s tratamento:\", df_processed.shape) <pre>Ap\u00f3s tratamento: (8693, 14)\n</pre> In\u00a0[14]: Copied! <pre>df_encoded = df_processed.copy()\n\n# Mapeamento booleano/tri-estado\nboolean_mappings = {\n    \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},\n    \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1}\n}\nfor col, mapping in boolean_mappings.items():\n    if col in df_encoded.columns:\n        df_encoded[col] = df_encoded[col].astype(str).map(mapping)\n        print(f\"   - {col}: {mapping}\")\n\n# One-hot para categ\u00f3ricas\ncategorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns]\nfor col in categorical_cols:\n    unique_values = df_encoded[col].astype(str).unique()\n    print(f\"   - {col}: {len(unique_values)} categorias\")\n\ndf_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False)\nprint(\"Ap\u00f3s encoding:\", df_encoded.shape)\n</pre> df_encoded = df_processed.copy()  # Mapeamento booleano/tri-estado boolean_mappings = {     \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},     \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1} } for col, mapping in boolean_mappings.items():     if col in df_encoded.columns:         df_encoded[col] = df_encoded[col].astype(str).map(mapping)         print(f\"   - {col}: {mapping}\")  # One-hot para categ\u00f3ricas categorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns] for col in categorical_cols:     unique_values = df_encoded[col].astype(str).unique()     print(f\"   - {col}: {len(unique_values)} categorias\")  df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False) print(\"Ap\u00f3s encoding:\", df_encoded.shape) <pre>   - CryoSleep: {'True': 1, 'False': 0, 'Unknown': -1}\n   - VIP: {'True': 1, 'False': 0, 'Unknown': -1}\n   - HomePlanet: 4 categorias\n   - Destination: 4 categorias\n   - Deck: 9 categorias\n   - Side: 3 categorias\nAp\u00f3s encoding: (8693, 30)\n</pre> In\u00a0[15]: Copied! <pre>print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\")\nprint(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")\n\ndef minmax_scale_to_neg1_pos1(series):\n    return 2 * ((series - series.min()) / (series.max() - series.min())) - 1\n\ndf_normalized = df_encoded.copy()\noriginal_data = {}\n\nscaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns]\nfor col in scaling_cols:\n    original_data[col] = df_normalized[col].copy()\n    original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"\n    df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])\n    normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"\n    print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")\n\nprint(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape)\n</pre> print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\") print(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")  def minmax_scale_to_neg1_pos1(series):     return 2 * ((series - series.min()) / (series.max() - series.min())) - 1  df_normalized = df_encoded.copy() original_data = {}  scaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns] for col in scaling_cols:     original_data[col] = df_normalized[col].copy()     original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"     df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])     normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"     print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")  print(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape) <pre>\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\nM\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\n   - Age: [0.0, 79.0] \u2192 [-1.000, 1.000]\n   - RoomService: [0.0, 14327.0] \u2192 [-1.000, 1.000]\n   - FoodCourt: [0.0, 29813.0] \u2192 [-1.000, 1.000]\n   - ShoppingMall: [0.0, 23492.0] \u2192 [-1.000, 1.000]\n   - Spa: [0.0, 22408.0] \u2192 [-1.000, 1.000]\n   - VRDeck: [0.0, 24133.0] \u2192 [-1.000, 1.000]\n   - CabinNum: [0.0, 1894.0] \u2192 [-1.000, 1.000]\nAp\u00f3s normaliza\u00e7\u00e3o: (8693, 30)\n</pre> In\u00a0[16]: Copied! <pre>target_col = \"Transported\"\nX = df_normalized.drop(columns=[target_col]).values\ny = df_normalized[target_col].map({True: 1, False: 0}).values\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n</pre> target_col = \"Transported\" X = df_normalized.drop(columns=[target_col]).values y = df_normalized[target_col].map({True: 1, False: 0}).values print(\"X shape:\", X.shape, \"| y shape:\", y.shape) <pre>X shape: (8693, 29) | y shape: (8693,)\n</pre> <p>4. Visualize the Data:</p> <ul> <li>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation</li> </ul> In\u00a0[17]: Copied! <pre>for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:\n    plt.figure(figsize=(12, 5))\n\n    # Antes\n    plt.subplot(1, 2, 1)\n    plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    # Depois\n    plt.subplot(1, 2, 2)\n    plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (normalizado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:     plt.figure(figsize=(12, 5))      # Antes     plt.subplot(1, 2, 1)     plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (original)\")     plt.ylabel(\"Frequ\u00eancia\")      # Depois     plt.subplot(1, 2, 2)     plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (normalizado)\")     plt.ylabel(\"Frequ\u00eancia\")      plt.tight_layout()     plt.show()"},{"location":"Exercicios/EX1/data/#1-data","title":"1. Data\u00b6","text":""},{"location":"Exercicios/EX1/data/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks\u00b6","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"Exercicios/EX1/data/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/EX1/data/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/EX1/data/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Preparing Real-World Data for a Neural Network This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"Exercicios/EX2/perceptron/","title":"2. Perceptron","text":"In\u00a0[141]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [1.5, 1.5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [5, 5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]].</li> </ul> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> In\u00a0[142]: Copied! <pre>n_por_classe=1000\nnp.random.seed(42)\n\nmean0 = np.array([1.5, 1.5])\nmean1 = np.array([5, 5])\n\ncov = np.array([[0.5, 0], \n                [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia\n\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\nidx = np.random.permutation(len(X))\nX, y = X[idx], y[idx]\n</pre> n_por_classe=1000 np.random.seed(42)  mean0 = np.array([1.5, 1.5]) mean1 = np.array([5, 5])  cov = np.array([[0.5, 0],                  [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia  X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  idx = np.random.permutation(len(X)) X, y = X[idx], y[idx] In\u00a0[143]: Copied! <pre>plt.figure(figsize=(6, 6))\nplt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0')\nplt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Dados 2D - duas classes')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(6, 6)) plt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0') plt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1') plt.xlabel('x1') plt.ylabel('x2') plt.title('Dados 2D - duas classes') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li><p>Initialize weights (w) as a 2D vector (plus a bias term b).</p> </li> <li><p>Use the perceptron learning rule: For each misclassified sample <code>(x, y)</code>, update <code>w = w + \u03b7 * y * x</code> and <code>b = b + \u03b7 * y</code>, where <code>\u03b7</code> is the learning rate (start with 0.1).</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</p> </li> <li><p>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by <code>w * x + b = 0</code>) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> In\u00a0[144]: Copied! <pre>y_pm1 = np.where(y == 1, 1, -1).astype(int)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\nprint(\"w inicial:\", w, \"b inicial:\", b)\n\n# --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) ---\nscores = X @ w + b\npreds = np.where(scores &gt;= 0.0, 1, -1)\n\nprint(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5])\n</pre> y_pm1 = np.where(y == 1, 1, -1).astype(int) w = np.zeros(2, dtype=float) b = 0.0  print(\"w inicial:\", w, \"b inicial:\", b)  # --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) --- scores = X @ w + b preds = np.where(scores &gt;= 0.0, 1, -1)  print(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5]) <pre>w inicial: [0. 0.] b inicial: 0.0\nprimeiras predi\u00e7\u00f5es (sem treino): [1 1 1 1 1]\n</pre> In\u00a0[145]: Copied! <pre># --- Checar se um ponto foi mal classificado ---\nx0 = X[0]\ny0 = y_pm1[0]\nmisclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0\nprint(\"primeiro ponto est\u00e1 errado?\", misclassified)\n\n# --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) ---\neta = 0.1\nif misclassified:\n    w = w + eta * y0 * x0\n    b = b + eta * y0\n\nprint(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b)\n</pre> # --- Checar se um ponto foi mal classificado --- x0 = X[0] y0 = y_pm1[0] misclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0 print(\"primeiro ponto est\u00e1 errado?\", misclassified)  # --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) --- eta = 0.1 if misclassified:     w = w + eta * y0 * x0     b = b + eta * y0  print(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b) <pre>primeiro ponto est\u00e1 errado? True\nw ap\u00f3s poss\u00edvel update: [0.51299906 0.69042624] b: 0.1\n</pre> In\u00a0[146]: Copied! <pre># Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# Hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\nnp.random.seed(42)\n\n# Inicializa\u00e7\u00e3o\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:            # misclassified (ou na margem)\n            w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso\n            b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s\n            updates += 1\n\n    # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)\n    scores = X @ w + b\n    y_pred = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred == y_pm1).mean()\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    if updates == 0:\n        print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")\n        break\n\n# Resultados finais\nfinal_epoch = len(accuracies)\nfinal_acc = accuracies[-1]\nprint(\"\\n--------- Finais ---------\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas:\", final_epoch)\nprint(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\")\nprint(\"--------------------------\")\n</pre> # Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1 y_pm1 = np.where(y == 1, 1, -1).astype(int)  # Hiperpar\u00e2metros eta = 0.01 max_epochs = 100 np.random.seed(42)  # Inicializa\u00e7\u00e3o w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:            # misclassified (ou na margem)             w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso             b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s             updates += 1      # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)     scores = X @ w + b     y_pred = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred == y_pm1).mean()     accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      if updates == 0:         print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")         break  # Resultados finais final_epoch = len(accuracies) final_acc = accuracies[-1] print(\"\\n--------- Finais ---------\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas:\", final_epoch) print(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\") print(\"--------------------------\")  <pre>\u00c9poca   1 | updates:   60 | acc: 0.9905\n\u00c9poca   2 | updates:   24 | acc: 0.9995\n\u00c9poca   3 | updates:    8 | acc: 0.9950\n\u00c9poca   4 | updates:    6 | acc: 1.0000\n\u00c9poca   5 | updates:    0 | acc: 1.0000\nConverg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\n\n--------- Finais ---------\nw: [0.0643648  0.04329078]\nb: -0.36000000000000015\n\u00e9pocas: 5\nacur\u00e1cia final: 1.0000\n--------------------------\n</pre> In\u00a0[147]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# --- 3A) Acur\u00e1cia por \u00e9poca ---\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca')\naxes[0].grid(True, alpha=0.3)\n\n# --- 3B) Fronteira de decis\u00e3o sobre os dados + erros ---\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\n# fronteira\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\n# erros\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1],\n                    s=40, marker='x', linewidths=1.5,\n                    label=f'Erros ({mis_idx.size})')\n\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(\"w final:\", w, \"| b final:\", b)\n</pre> fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # --- 3A) Acur\u00e1cia por \u00e9poca --- axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca') axes[0].grid(True, alpha=0.3)  # --- 3B) Fronteira de decis\u00e3o sobre os dados + erros --- scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  # fronteira x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  # erros if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1],                     s=40, marker='x', linewidths=1.5,                     label=f'Erros ({mis_idx.size})')  axes[1].set_title('Perceptron \u2014 Dados e Fronteira') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show()  print(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\") print(\"w final:\", w, \"| b final:\", b)  <pre>Total de pontos: 2000 | Erros: 0 | Acur\u00e1cia final: 1.0000\nw final: [0.0643648  0.04329078] | b final: -0.36000000000000015\n</pre> <p>Answer: O perceptron funciona muito bem quando h\u00e1 separabilidade linear com boa margem, convergindo r\u00e1pido e com fronteira simples. Logo, os dados gerados s\u00e3o ideais para o perceptron, que consegue encontrar uma fronteira linear eficaz. A baixa vari\u00e2ncia e a dist\u00e2ncia entre as m\u00e9dias das classes minimizam sobreposi\u00e7\u00f5es, facilitando a classifica\u00e7\u00e3o correta. Assim, o perceptron atinge alta acur\u00e1cia rapidamente, demonstrando sua efic\u00e1cia em cen\u00e1rios de separabilidade linear clara.</p> <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [3, 3],</li> <li>Covariance = [[1.5, 0], [0, 1.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [4, 4],</li> <li>Covariance = [[1.5, 0], [0, 1.5]].</li> </ul> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> In\u00a0[148]: Copied! <pre># ---- Par\u00e2metros do Ex.2 ----\nnp.random.seed(42)\nn_por_classe = 1000\n\nmean0 = np.array([3.0, 3.0])\nmean1 = np.array([4.0, 4.0])\n\ncov = np.array([[1.5, 0.0],\n                [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap\n\n# ---- Amostragem (sem fun\u00e7\u00f5es) ----\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\n# Embaralha para uso posterior\nidx = np.random.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</pre> # ---- Par\u00e2metros do Ex.2 ---- np.random.seed(42) n_por_classe = 1000  mean0 = np.array([3.0, 3.0]) mean1 = np.array([4.0, 4.0])  cov = np.array([[1.5, 0.0],                 [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap  # ---- Amostragem (sem fun\u00e7\u00f5es) ---- X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  # Embaralha para uso posterior idx = np.random.permutation(len(X)) X = X[idx] y = y[idx]  In\u00a0[149]: Copied! <pre># ---- Visualiza\u00e7\u00e3o: overlap entre classes ----\nplt.figure(figsize=(6,6))\nplt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\nplt.xlabel('x1'); plt.ylabel('x2')\nplt.title('Ex.2 \u2014 Dados 2D (overlap parcial)')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # ---- Visualiza\u00e7\u00e3o: overlap entre classes ---- plt.figure(figsize=(6,6)) plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8) plt.xlabel('x1'); plt.ylabel('x2') plt.title('Ex.2 \u2014 Dados 2D (overlap parcial)') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li><p>Follow the same initialization, update rule, and training process.</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</p> </li> <li><p>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> In\u00a0[150]: Copied! <pre>np.random.seed(42)\n\n# r\u00f3tulos em {-1,+1}\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\n\n# inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    # embaralhar a ordem a cada \u00e9poca\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra (regra do perceptron)\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:           # misclassified\n            w = w + eta * yi * xi\n            b = b + eta * yi\n            updates += 1\n\n    # acur\u00e1cia na \u00e9poca (dataset completo)\n    scores = X @ w + b\n    y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred_pm1 == y_pm1).mean()\n\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)\n    if updates == 0:\n        print(\"Parada por aus\u00eancia de updates (raro com overlap).\")\n        break\n\n# m\u00e9tricas finais\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\nprint(\"\\n--- Finais ---\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas executadas:\", len(accuracies))\nprint(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(f\"erros: {mis_idx.size} de {len(X)}\")\n\n# ---------- Plots lado a lado ----------\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# (A) acur\u00e1cia por \u00e9poca\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)')\naxes[0].grid(True, alpha=0.3)\n\n# (B) fronteira de decis\u00e3o + erros\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',\n                    linewidths=1.5, label=f'Erros ({mis_idx.size})')\n\naxes[1].set_xlabel('x1'); axes[1].set_ylabel('x2')\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n</pre> np.random.seed(42)  # r\u00f3tulos em {-1,+1} y_pm1 = np.where(y == 1, 1, -1).astype(int)  # hiperpar\u00e2metros eta = 0.01 max_epochs = 100  # inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser) w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     # embaralhar a ordem a cada \u00e9poca     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra (regra do perceptron)     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:           # misclassified             w = w + eta * yi * xi             b = b + eta * yi             updates += 1      # acur\u00e1cia na \u00e9poca (dataset completo)     scores = X @ w + b     y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred_pm1 == y_pm1).mean()      accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)     if updates == 0:         print(\"Parada por aus\u00eancia de updates (raro com overlap).\")         break  # m\u00e9tricas finais scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  print(\"\\n--- Finais ---\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas executadas:\", len(accuracies)) print(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\") print(f\"erros: {mis_idx.size} de {len(X)}\")  # ---------- Plots lado a lado ---------- fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # (A) acur\u00e1cia por \u00e9poca axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)') axes[0].grid(True, alpha=0.3)  # (B) fronteira de decis\u00e3o + erros axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',                     linewidths=1.5, label=f'Erros ({mis_idx.size})')  axes[1].set_xlabel('x1'); axes[1].set_ylabel('x2') axes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show() <pre>\u00c9poca   1 | updates:  849 | acc: 0.6915\n\u00c9poca   2 | updates:  768 | acc: 0.6670\n\u00c9poca   3 | updates:  816 | acc: 0.6550\n\u00c9poca   4 | updates:  799 | acc: 0.5845\n\u00c9poca   5 | updates:  768 | acc: 0.6940\n\u00c9poca   6 | updates:  780 | acc: 0.6590\n\u00c9poca   7 | updates:  799 | acc: 0.6025\n\u00c9poca   8 | updates:  764 | acc: 0.5430\n\u00c9poca   9 | updates:  784 | acc: 0.6835\n\u00c9poca  10 | updates:  757 | acc: 0.5690\n\u00c9poca  11 | updates:  743 | acc: 0.5015\n\u00c9poca  12 | updates:  755 | acc: 0.6795\n\u00c9poca  13 | updates:  748 | acc: 0.6630\n\u00c9poca  14 | updates:  752 | acc: 0.5005\n\u00c9poca  15 | updates:  777 | acc: 0.5320\n\u00c9poca  16 | updates:  737 | acc: 0.6680\n\u00c9poca  17 | updates:  773 | acc: 0.5640\n\u00c9poca  18 | updates:  790 | acc: 0.5865\n\u00c9poca  19 | updates:  790 | acc: 0.6645\n\u00c9poca  20 | updates:  798 | acc: 0.6375\n\u00c9poca  21 | updates:  782 | acc: 0.7050\n\u00c9poca  22 | updates:  768 | acc: 0.6740\n\u00c9poca  23 | updates:  750 | acc: 0.5000\n\u00c9poca  24 | updates:  776 | acc: 0.5000\n\u00c9poca  25 | updates:  741 | acc: 0.5000\n\u00c9poca  26 | updates:  762 | acc: 0.6285\n\u00c9poca  27 | updates:  778 | acc: 0.6725\n\u00c9poca  28 | updates:  770 | acc: 0.6980\n\u00c9poca  29 | updates:  720 | acc: 0.6210\n\u00c9poca  30 | updates:  761 | acc: 0.5010\n\u00c9poca  31 | updates:  776 | acc: 0.6595\n\u00c9poca  32 | updates:  761 | acc: 0.5620\n\u00c9poca  33 | updates:  775 | acc: 0.5000\n\u00c9poca  34 | updates:  772 | acc: 0.5000\n\u00c9poca  35 | updates:  761 | acc: 0.6105\n\u00c9poca  36 | updates:  779 | acc: 0.6980\n\u00c9poca  37 | updates:  778 | acc: 0.5815\n\u00c9poca  38 | updates:  742 | acc: 0.6785\n\u00c9poca  39 | updates:  785 | acc: 0.6925\n\u00c9poca  40 | updates:  749 | acc: 0.6985\n\u00c9poca  41 | updates:  746 | acc: 0.6550\n\u00c9poca  42 | updates:  799 | acc: 0.5900\n\u00c9poca  43 | updates:  780 | acc: 0.5145\n\u00c9poca  44 | updates:  745 | acc: 0.5010\n\u00c9poca  45 | updates:  770 | acc: 0.6335\n\u00c9poca  46 | updates:  784 | acc: 0.5845\n\u00c9poca  47 | updates:  789 | acc: 0.6975\n\u00c9poca  48 | updates:  767 | acc: 0.5185\n\u00c9poca  49 | updates:  751 | acc: 0.5820\n\u00c9poca  50 | updates:  759 | acc: 0.5000\n\u00c9poca  51 | updates:  762 | acc: 0.5020\n\u00c9poca  52 | updates:  753 | acc: 0.5230\n\u00c9poca  53 | updates:  765 | acc: 0.5780\n\u00c9poca  54 | updates:  778 | acc: 0.5505\n\u00c9poca  55 | updates:  728 | acc: 0.6365\n\u00c9poca  56 | updates:  780 | acc: 0.6035\n\u00c9poca  57 | updates:  789 | acc: 0.7045\n\u00c9poca  58 | updates:  761 | acc: 0.6790\n\u00c9poca  59 | updates:  755 | acc: 0.6195\n\u00c9poca  60 | updates:  755 | acc: 0.5770\n\u00c9poca  61 | updates:  789 | acc: 0.6650\n\u00c9poca  62 | updates:  768 | acc: 0.6520\n\u00c9poca  63 | updates:  776 | acc: 0.6225\n\u00c9poca  64 | updates:  768 | acc: 0.6635\n\u00c9poca  65 | updates:  800 | acc: 0.6935\n\u00c9poca  66 | updates:  780 | acc: 0.6580\n\u00c9poca  67 | updates:  773 | acc: 0.6560\n\u00c9poca  68 | updates:  768 | acc: 0.5915\n\u00c9poca  69 | updates:  777 | acc: 0.6385\n\u00c9poca  70 | updates:  762 | acc: 0.5000\n\u00c9poca  71 | updates:  811 | acc: 0.6360\n\u00c9poca  72 | updates:  761 | acc: 0.5865\n\u00c9poca  73 | updates:  762 | acc: 0.6370\n\u00c9poca  74 | updates:  767 | acc: 0.6595\n\u00c9poca  75 | updates:  771 | acc: 0.7005\n\u00c9poca  76 | updates:  775 | acc: 0.7005\n\u00c9poca  77 | updates:  783 | acc: 0.5265\n\u00c9poca  78 | updates:  786 | acc: 0.6995\n\u00c9poca  79 | updates:  763 | acc: 0.5000\n\u00c9poca  80 | updates:  773 | acc: 0.6415\n\u00c9poca  81 | updates:  748 | acc: 0.5685\n\u00c9poca  82 | updates:  738 | acc: 0.6575\n\u00c9poca  83 | updates:  793 | acc: 0.5815\n\u00c9poca  84 | updates:  764 | acc: 0.5745\n\u00c9poca  85 | updates:  779 | acc: 0.5870\n\u00c9poca  86 | updates:  765 | acc: 0.5395\n\u00c9poca  87 | updates:  787 | acc: 0.6665\n\u00c9poca  88 | updates:  770 | acc: 0.5475\n\u00c9poca  89 | updates:  753 | acc: 0.5015\n\u00c9poca  90 | updates:  779 | acc: 0.5735\n\u00c9poca  91 | updates:  801 | acc: 0.5005\n\u00c9poca  92 | updates:  787 | acc: 0.6385\n\u00c9poca  93 | updates:  747 | acc: 0.7025\n\u00c9poca  94 | updates:  793 | acc: 0.5675\n\u00c9poca  95 | updates:  768 | acc: 0.6485\n\u00c9poca  96 | updates:  778 | acc: 0.6695\n\u00c9poca  97 | updates:  760 | acc: 0.6520\n\u00c9poca  98 | updates:  777 | acc: 0.7025\n\u00c9poca  99 | updates:  784 | acc: 0.6280\n\u00c9poca 100 | updates:  807 | acc: 0.5035\n\n--- Finais ---\nw: [0.02953414 0.05292094]\nb: -0.5200000000000002\n\u00e9pocas executadas: 100\nacur\u00e1cia final: 0.5035\nerros: 993 de 2000\n</pre> <p>Answer: O overlap introduzido no Ex.2 mostra uma limita\u00e7\u00e3o fundamental do perceptron cl\u00e1ssico: ele s\u00f3 converge se os dados forem linearmente separ\u00e1veis. Em cen\u00e1rios mais realistas, com ru\u00eddo ou vari\u00e2ncia maior, o perceptron n\u00e3o converge e a acur\u00e1cia fica limitada. Isso motiva a ado\u00e7\u00e3o de variantes como o Perceptron com Margem, regress\u00e3o log\u00edstica, SVM ou redes neurais multicamadas, que lidam melhor com dados n\u00e3o separ\u00e1veis.</p>"},{"location":"Exercicios/EX2/perceptron/#2-perceptron","title":"2. Perceptron\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#activity-understanding-perceptrons-and-their-limitations","title":"Activity: Understanding Perceptrons and Their Limitations\u00b6","text":"<p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"Exercicios/EX2/perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/","title":"3. MLP","text":"In\u00a0[22]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): L = 1/N * (y - \u0177)\u00b2, where \u0177 is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li><p>Input and output vectors:</p> <ul> <li>X: [0.5, -0.2]</li> <li>Y: 1.0</li> </ul> </li> <li><p>Hidden layer weights:</p> <ul> <li>W\u00b9 = [[0.3, -0.1], [0.2, 0.4]]  (2x2 matrix)</li> </ul> </li> <li><p>Hidden layer biases:</p> <ul> <li>b\u00b9 = [0.1, -0.2]  (1x2 vector)</li> </ul> </li> <li><p>Output layer weights:</p> <ul> <li>W\u00b2 = [0.5, -0.3]</li> </ul> </li> <li><p>Output layer bias:</p> <ul> <li>b\u00b2 = 0.2</li> </ul> </li> <li><p>Learning rate:</p> <ul> <li>\u03b7 = 0.3</li> </ul> </li> <li><p>Activation function: tanh</p> </li> </ul> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li><p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: Z\u00b9 = W\u00b9 * X + b\u00b9.</li> <li>Apply tanh to get hidden activations: a\u00b9 = tanh(Z\u00b9).</li> <li>Compute the output pre-activation: Z\u00b2 = W\u00b2 * a\u00b9 + b\u00b2.</li> <li>Compute the final output: \u0177 = tanh(Z\u00b2).</li> </ul> </li> <li><p>Loss Calculation:</p> <ul> <li>Compute the loss: L = 1/N * (Y - \u0177)\u00b2.</li> </ul> </li> <li><p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with delL/del\u0177 then compute:</p> <ul> <li>delL/delZ\u00b2 (using the tanh derivative: del/delZ tanh(Z) = 1 - tanh\u00b2(Z)).</li> <li>Gradients for output layer: delL/delW\u00b2, delL/delb\u00b2.</li> <li>Propagate to hidden layer: delL/delA\u00b9, delL/delZ\u00b9.</li> <li>Gradients for hidden layer: delL/delW\u00b9, delL/delb\u00b9.</li> <li>Show all intermediate steps and calculations.</li> </ul> </li> <li><p>Parameter Update: Using the learning rate \u03b7 = 0.1, update all weights and biases via gradient descent:</p> <ul> <li>W\u00b2 &lt;- W\u00b2 - \u03b7 * delL/delW\u00b2</li> <li>b\u00b2 &lt;- b\u00b2 - \u03b7 * delL/delb\u00b2</li> <li>W\u00b9 &lt;- W\u00b9 - \u03b7 * delL/delW\u00b9</li> <li>b\u00b9 &lt;- b\u00b9 - \u03b7 * delL/delb\u00b9</li> <li>Provide the numerical values for all updated parameters.</li> </ul> </li> </ol> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p> In\u00a0[23]: Copied! <pre># --- Helpers ---\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(z):\n    return 1.0 - np.tanh(z)**2\n\ndef fmt(x):\n    if isinstance(x, float):\n        return f\"{x:.6f}\"\n    arr = np.array(x, dtype=float)\n    return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},\n                           floatmode='maxprec', suppress_small=False)\n\ndef p(title, value):\n    print(f\"{title}: {fmt(value)}\")\n\n# --- Dados do exerc\u00edcio ---\nX = np.array([0.5, -0.2], dtype=float)\nY = 1.0 \n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)\n\nb1 = np.array([0.1, -0.2], dtype=float)\n\nW2 = np.array([0.5, -0.3], dtype=float)\nb2 = 0.2\n\neta_update = 0.1\n\nprint(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\")\np(\"X\", X); p(\"Y\", Y)\nprint(\"\\n--- Par\u00e2metros iniciais ---\")\np(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)\n</pre> # --- Helpers --- def tanh(x):     return np.tanh(x)  def dtanh(z):     return 1.0 - np.tanh(z)**2  def fmt(x):     if isinstance(x, float):         return f\"{x:.6f}\"     arr = np.array(x, dtype=float)     return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},                            floatmode='maxprec', suppress_small=False)  def p(title, value):     print(f\"{title}: {fmt(value)}\")  # --- Dados do exerc\u00edcio --- X = np.array([0.5, -0.2], dtype=float) Y = 1.0   W1 = np.array([[0.3, -0.1],                [0.2,  0.4]], dtype=float)  b1 = np.array([0.1, -0.2], dtype=float)  W2 = np.array([0.5, -0.3], dtype=float) b2 = 0.2  eta_update = 0.1  print(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\") p(\"X\", X); p(\"Y\", Y) print(\"\\n--- Par\u00e2metros iniciais ---\") p(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)  <pre>=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\n\nX: [0.500000 -0.200000]\nY: 1.000000\n\n--- Par\u00e2metros iniciais ---\nW1: [[0.300000 -0.100000]\n [0.200000 0.400000]]\nb1: [0.100000 -0.200000]\nW2: [0.500000 -0.300000]\nb2: 0.200000\n</pre> In\u00a0[24]: Copied! <pre># === 1) Forward pass ===\nZ1 = W1 @ X + b1\nA1 = tanh(Z1)\nZ2 = float(W2 @ A1 + b2)\nY_hat = float(tanh(Z2))\n\nprint(\"\\n--- Forward Pass ---\")\np(\"Z1 = W1 @ X + b1\", Z1)\np(\"A1 = tanh(Z1)\", A1)\np(\"Z2 = W2 \u00b7 A1 + b2\", Z2)\np(\"\u0177 = tanh(Z2)\", Y_hat)\n\n# === 2) Loss ===\nL = (Y - Y_hat)**2\nprint(\"\\n--- Loss (MSE) ---\")\np(\"L = (Y - \u0177)^2\", L)\n</pre> # === 1) Forward pass === Z1 = W1 @ X + b1 A1 = tanh(Z1) Z2 = float(W2 @ A1 + b2) Y_hat = float(tanh(Z2))  print(\"\\n--- Forward Pass ---\") p(\"Z1 = W1 @ X + b1\", Z1) p(\"A1 = tanh(Z1)\", A1) p(\"Z2 = W2 \u00b7 A1 + b2\", Z2) p(\"\u0177 = tanh(Z2)\", Y_hat)  # === 2) Loss === L = (Y - Y_hat)**2 print(\"\\n--- Loss (MSE) ---\") p(\"L = (Y - \u0177)^2\", L)  <pre>\n--- Forward Pass ---\nZ1 = W1 @ X + b1: [0.270000 -0.180000]\nA1 = tanh(Z1): [0.263625 -0.178081]\nZ2 = W2 \u00b7 A1 + b2: 0.385237\n\u0177 = tanh(Z2): 0.367247\n\n--- Loss (MSE) ---\nL = (Y - \u0177)^2: 0.400377\n</pre> In\u00a0[25]: Copied! <pre># === 3) Backpropagation ===\n\n# Sa\u00edda\ndL_dYhat = 2.0*(Y_hat - Y)\ndYhat_dZ2 = dtanh(Z2)\ndL_dZ2 = dL_dYhat * dYhat_dZ2\n\nprint(\"\\n-- Sa\u00edda --\")\np(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat)\np(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2)\np(\"dL/dZ2\", dL_dZ2)\n\n# Gradientes da camada de sa\u00edda\ndL_dW2 = dL_dZ2 * A1            # (2,)\ndL_db2 = dL_dZ2                 # escalar\n\nprint(\"\\n-- Gradientes camada de sa\u00edda --\")\np(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2)\np(\"dL/db2 = dL/dZ2\", dL_db2)\n\n# Propaga\u00e7\u00e3o p/ camada oculta\ndL_dA1 = dL_dZ2 * W2            # (2,)\ndA1_dZ1 = dtanh(Z1)             # (2,)\ndL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)\n\nprint(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\")\np(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1)\np(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1)\np(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)\n\n# Gradientes da camada oculta\ndL_dW1 = np.outer(dL_dZ1, X)    # (2,2)\ndL_db1 = dL_dZ1                 # (2,)\n\nprint(\"\\n-- Gradientes camada oculta --\")\np(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1)\np(\"dL/db1 = dL/dZ1\", dL_db1)\n</pre> # === 3) Backpropagation ===  # Sa\u00edda dL_dYhat = 2.0*(Y_hat - Y) dYhat_dZ2 = dtanh(Z2) dL_dZ2 = dL_dYhat * dYhat_dZ2  print(\"\\n-- Sa\u00edda --\") p(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat) p(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2) p(\"dL/dZ2\", dL_dZ2)  # Gradientes da camada de sa\u00edda dL_dW2 = dL_dZ2 * A1            # (2,) dL_db2 = dL_dZ2                 # escalar  print(\"\\n-- Gradientes camada de sa\u00edda --\") p(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2) p(\"dL/db2 = dL/dZ2\", dL_db2)  # Propaga\u00e7\u00e3o p/ camada oculta dL_dA1 = dL_dZ2 * W2            # (2,) dA1_dZ1 = dtanh(Z1)             # (2,) dL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)  print(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\") p(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1) p(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1) p(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)  # Gradientes da camada oculta dL_dW1 = np.outer(dL_dZ1, X)    # (2,2) dL_db1 = dL_dZ1                 # (2,)  print(\"\\n-- Gradientes camada oculta --\") p(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1) p(\"dL/db1 = dL/dZ1\", dL_db1) <pre>\n-- Sa\u00edda --\ndL/d\u0177 = 2*(\u0177 - Y): -1.265507\ndtanh(Z2) = 1 - tanh^2(Z2): 0.865130\ndL/dZ2: -1.094828\n\n-- Gradientes camada de sa\u00edda --\ndL/dW2 = dL/dZ2 * A1: [-0.288624 0.194968]\ndL/db2 = dL/dZ2: -1.094828\n\n-- Propaga\u00e7\u00e3o para a oculta --\ndL/dA1 = dL/dZ2 * W2: [-0.547414 0.328448]\ndtanh(Z1) = 1 - tanh^2(Z1): [0.930502 0.968287]\ndL/dZ1 = dL/dA1 \u2299 dtanh(Z1): [-0.509370 0.318032]\n\n-- Gradientes camada oculta --\ndL/dW1 = outer(dL/dZ1, X): [[-0.254685 0.101874]\n [0.159016 -0.063606]]\ndL/db1 = dL/dZ1: [-0.509370 0.318032]\n</pre> In\u00a0[26]: Copied! <pre># === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) ===\nW2_new = W2 - eta_update * dL_dW2\nb2_new = b2 - eta_update * dL_db2\nW1_new = W1 - eta_update * dL_dW1\nb1_new = b1 - eta_update * dL_db1\n\np(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new)\np(\"b2_new = b2 - \u03b7*dL/db2\", b2_new)\np(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new)\np(\"b1_new = b1 - \u03b7*dL/db1\", b1_new)\n</pre> # === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) === W2_new = W2 - eta_update * dL_dW2 b2_new = b2 - eta_update * dL_db2 W1_new = W1 - eta_update * dL_dW1 b1_new = b1 - eta_update * dL_db1  p(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new) p(\"b2_new = b2 - \u03b7*dL/db2\", b2_new) p(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new) p(\"b1_new = b1 - \u03b7*dL/db1\", b1_new) <pre>\nW2_new = W2 - \u03b7*dL/dW2: [0.528862 -0.319497]\nb2_new = b2 - \u03b7*dL/db2: 0.309483\nW1_new = W1 - \u03b7*dL/dW1: [[0.325468 -0.110187]\n [0.184098 0.406361]]\nb1_new = b1 - \u03b7*dL/db1: [0.150937 -0.231803]\n</pre> In\u00a0[27]: Copied! <pre># === Checagem opcional: forward com par\u00e2metros atualizados ===\nZ1_new = W1_new @ X + b1_new\nA1_new = tanh(Z1_new)\nZ2_new = float(W2_new @ A1_new + b2_new)\nY_hat_new = float(tanh(Z2_new))\nL_new = (Y - Y_hat_new)**2\n\np(\"\\n\u0177 (antes)\", Y_hat)\np(\"L (antes)\", L)\np(\"\u0177 (depois)\", Y_hat_new)\np(\"L (depois)\", L_new)\n</pre> # === Checagem opcional: forward com par\u00e2metros atualizados === Z1_new = W1_new @ X + b1_new A1_new = tanh(Z1_new) Z2_new = float(W2_new @ A1_new + b2_new) Y_hat_new = float(tanh(Z2_new)) L_new = (Y - Y_hat_new)**2  p(\"\\n\u0177 (antes)\", Y_hat) p(\"L (antes)\", L) p(\"\u0177 (depois)\", Y_hat_new) p(\"L (depois)\", L_new)  <pre>\n\u0177 (antes): 0.367247\nL (antes): 0.400377\n\u0177 (depois): 0.500620\nL (depois): 0.249380\n</pre> <p>Using the <code>make_classification</code> function from scikit-learn, generate a synthetic dataset with the following specifications:</p> <ul> <li><p>Number of samples: 1000</p> </li> <li><p>Number of classes: 2</p> </li> <li><p>Number of clusters per class: Use the n_clusters_per_class parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</p> </li> <li><p>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</p> </li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li><p>Number of hidden layers (at least 1)</p> </li> <li><p>Number of neurons per layer</p> </li> <li><p>Activation functions (e.g., sigmoid, ReLU, tanh)</p> </li> <li><p>Loss function (e.g., binary cross-entropy)</p> </li> <li><p>Optimizer (e.g., gradient descent, with a chosen learning rate)</p> </li> </ul> <p>Steps to follow:</p> <ol> <li><p>Generate and split the data into training (80%) and testing (20%) sets.</p> </li> <li><p>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</p> </li> <li><p>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</p> </li> <li><p>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</p> </li> <li><p>Submit your code and results, including any visualizations.</p> </li> </ol> In\u00a0[28]: Copied! <pre>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(42)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid(a):\n    # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)\n    return a * (1.0 - a)\n\ndef bce_loss(y_true, y_pred, eps=1e-12):\n    # y_true, y_pred com shape (m,1)\n    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n    return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))\n\ndef accuracy(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    return (y_hat == y_true).mean()\n\ndef confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())\n    tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())\n    fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())\n    fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())\n    return np.array([[tn, fp],\n                     [fn, tp]])\n</pre> from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  np.random.seed(42)  def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))  def dsigmoid(a):     # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)     return a * (1.0 - a)  def bce_loss(y_true, y_pred, eps=1e-12):     # y_true, y_pred com shape (m,1)     y_pred = np.clip(y_pred, eps, 1.0 - eps)     return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))  def accuracy(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     return (y_hat == y_true).mean()  def confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())     tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())     fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())     fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())     return np.array([[tn, fp],                      [fn, tp]]) In\u00a0[29]: Copied! <pre># Par\u00e2metros gerais\nN_total = 1000\nclass_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador\nflip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados)\nrandom_state = 42\n\n# 1) Classe 0 com 1 cluster\nX0, y0 = make_classification(\n    n_samples=N_total//2, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=1, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state\n)\n# Filtra apenas a classe 0\nX0 = X0[y0 == 0]\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1\nX1_full, y1_full = make_classification(\n    n_samples=N_total, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=2, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state + 1\n)\nX1 = X1_full[y1_full == 1]\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Balanceia o tamanho: escolhe min entre os dois lados\nn = min(len(X0), len(X1))\nX0 = X0[:n]\ny0 = y0[:n]\nX1 = X1[:n]\ny1 = y1[:n]\n\n# Combina\nX = np.vstack([X0, X1])\ny = np.concatenate([y0, y1])\n\n# Embaralha\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")\n\n# Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o)\nplt.figure()\nplt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8)\nplt.title(\"Dados sint\u00e9ticos (antes do split)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Par\u00e2metros gerais N_total = 1000 class_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador flip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados) random_state = 42  # 1) Classe 0 com 1 cluster X0, y0 = make_classification(     n_samples=N_total//2, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=1, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state ) # Filtra apenas a classe 0 X0 = X0[y0 == 0] y0 = np.zeros((X0.shape[0],), dtype=int)  # 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1 X1_full, y1_full = make_classification(     n_samples=N_total, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=2, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state + 1 ) X1 = X1_full[y1_full == 1] y1 = np.ones((X1.shape[0],), dtype=int)  # Balanceia o tamanho: escolhe min entre os dois lados n = min(len(X0), len(X1)) X0 = X0[:n] y0 = y0[:n] X1 = X1[:n] y1 = y1[:n]  # Combina X = np.vstack([X0, X1]) y = np.concatenate([y0, y1])  # Embaralha perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")  # Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o) plt.figure() plt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8) plt.title(\"Dados sint\u00e9ticos (antes do split)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>X shape: (498, 2), y shape: (498,), classe 0: 249, classe 1: 249\n</pre> In\u00a0[30]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Ajustar shapes de y p/ coluna\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  # Ajustar shapes de y p/ coluna y_train = y_train.reshape(-1, 1) y_test = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) <pre>(398, 2) (398, 1) (100, 2) (100, 1)\n</pre> In\u00a0[31]: Copied! <pre># Arquitetura\nn_in = 2\nn_hidden = 16\nn_out = 1\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef forward(Xb):\n    # Xb: (m, 2)\n    z1 = Xb @ W1 + b1          # (m, hidden)\n    a1 = tanh(z1)              # (m, hidden)\n    z2 = a1 @ W2 + b2          # (m, 1)\n    a2 = sigmoid(z2)           # (m, 1)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb):\n    # yb: (m,1)\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m\n    dz2 = (a2 - yb) / m                     # (m,1)\n    dW2 = a1.T @ dz2                        # (hidden,1)\n    db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)\n\n    da1 = dz2 @ W2.T                        # (m,hidden)\n    dz1 = da1 * dtanh(a1)                   # (m,hidden)\n    dW1 = Xb.T @ dz1                        # (2,hidden)\n    db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)\n\n    return dW1, db1, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = 2 n_hidden = 16 n_out = 1  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def forward(Xb):     # Xb: (m, 2)     z1 = Xb @ W1 + b1          # (m, hidden)     a1 = tanh(z1)              # (m, hidden)     z2 = a1 @ W2 + b2          # (m, 1)     a2 = sigmoid(z2)           # (m, 1)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb):     # yb: (m,1)     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m     dz2 = (a2 - yb) / m                     # (m,1)     dW2 = a1.T @ dz2                        # (hidden,1)     db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)      da1 = dz2 @ W2.T                        # (m,hidden)     dz1 = da1 * dtanh(a1)                   # (m,hidden)     dW1 = Xb.T @ dz1                        # (2,hidden)     db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)      return dW1, db1, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[32]: Copied! <pre># Hiperpar\u00e2metros de treino\nepochs = 300\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nindices = np.arange(m)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(indices)\n    X_train_shuf = X_train[indices]\n    y_train_shuf = y_train[indices]\n\n    # mini-batches\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = X_train_shuf[start:end]\n        yb = y_train_shuf[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = bce_loss(yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        # Acur\u00e1cia de treino r\u00e1pida\n        yhat_full, _ = forward(X_train)\n        acc_tr = accuracy(y_train, yhat_full)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\n# Curva de loss\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist, marker=None)\nplt.title(\"Loss (treino) por \u00e9poca\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Binary Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Hiperpar\u00e2metros de treino epochs = 300 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] indices = np.arange(m)  for ep in range(1, epochs+1):     np.random.shuffle(indices)     X_train_shuf = X_train[indices]     y_train_shuf = y_train[indices]      # mini-batches     ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = X_train_shuf[start:end]         yb = y_train_shuf[start:end]          yhat, cache = forward(Xb)         loss = bce_loss(yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         # Acur\u00e1cia de treino r\u00e1pida         yhat_full, _ = forward(X_train)         acc_tr = accuracy(y_train, yhat_full)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  # Curva de loss plt.figure() plt.plot(range(1, epochs+1), loss_hist, marker=None) plt.title(\"Loss (treino) por \u00e9poca\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Binary Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.5989 | acc_train=0.9271\nEpoch   25 | loss=0.1912 | acc_train=0.9372\nEpoch   50 | loss=0.1536 | acc_train=0.9422\nEpoch   75 | loss=0.1555 | acc_train=0.9372\nEpoch  100 | loss=0.1495 | acc_train=0.9397\nEpoch  125 | loss=0.1472 | acc_train=0.9397\nEpoch  150 | loss=0.1438 | acc_train=0.9422\nEpoch  175 | loss=0.1459 | acc_train=0.9422\nEpoch  200 | loss=0.1496 | acc_train=0.9397\nEpoch  225 | loss=0.1481 | acc_train=0.9422\nEpoch  250 | loss=0.1531 | acc_train=0.9422\nEpoch  275 | loss=0.1605 | acc_train=0.9422\nEpoch  300 | loss=0.1393 | acc_train=0.9397\n</pre> In\u00a0[33]: Copied! <pre># Predi\u00e7\u00f5es no teste\nyprob_test, _ = forward(X_test)\nacc_te = accuracy(y_test, yprob_test)\ncm = confusion_matrix_manual(y_test, yprob_test)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\")\nprint(cm)\n</pre> # Predi\u00e7\u00f5es no teste yprob_test, _ = forward(X_test) acc_te = accuracy(y_test, yprob_test) cm = confusion_matrix_manual(y_test, yprob_test)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\") print(cm) <pre>Acur\u00e1cia (teste): 0.9600\nMatriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\n[[47  3]\n [ 1 49]]\n</pre> In\u00a0[34]: Copied! <pre># Grade para visualizar a fronteira\nx_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0\ny_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                     np.linspace(y_min, y_max, 300))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Lembre: padronizamos com 'scaler'\ngrid_std = scaler.transform(grid)\nprobs, _ = forward(grid_std)\nZZ = probs.reshape(xx.shape)\n\nplt.figure()\nplt.contourf(xx, yy, ZZ, levels=50, alpha=0.6)\nplt.colorbar(label=\"p(y=1)\")\nplt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8)\nplt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Grade para visualizar a fronteira x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0 y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                      np.linspace(y_min, y_max, 300)) grid = np.c_[xx.ravel(), yy.ravel()]  # Lembre: padronizamos com 'scaler' grid_std = scaler.transform(grid) probs, _ = forward(grid_std) ZZ = probs.reshape(xx.shape)  plt.figure() plt.contourf(xx, yy, ZZ, levels=50, alpha=0.6) plt.colorbar(label=\"p(y=1)\") plt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8) plt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Synthetic Data and Reusable MLP</p> <p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol> In\u00a0[35]: Copied! <pre>from sklearn.decomposition import PCA\n\nnp.random.seed(42)\nplt.rcParams[\"figure.figsize\"] = (6, 5)\n# --- Ativa\u00e7\u00f5es ---\ndef softmax(z):\n    # z: (m, K)\n    z_shift = z - np.max(z, axis=1, keepdims=True)\n    e = np.exp(z_shift)\n    return e / np.sum(e, axis=1, keepdims=True)\n\n# --- Loss e m\u00e9tricas (multi-classe) ---\ndef cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):\n    # y_true_onehot, y_prob: (m, K)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))\n\ndef accuracy_multiclass(y_true, y_prob):\n    # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}\n    y_pred = np.argmax(y_prob, axis=1)\n    return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()\n\ndef confusion_matrix_k(y_true, y_prob, K):\n    y_pred = np.argmax(y_prob, axis=1)\n    cm = np.zeros((K, K), dtype=int)\n    for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):\n        cm[t, p] += 1\n    return cm\n</pre> from sklearn.decomposition import PCA  np.random.seed(42) plt.rcParams[\"figure.figsize\"] = (6, 5) # --- Ativa\u00e7\u00f5es --- def softmax(z):     # z: (m, K)     z_shift = z - np.max(z, axis=1, keepdims=True)     e = np.exp(z_shift)     return e / np.sum(e, axis=1, keepdims=True)  # --- Loss e m\u00e9tricas (multi-classe) --- def cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):     # y_true_onehot, y_prob: (m, K)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))  def accuracy_multiclass(y_true, y_prob):     # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}     y_pred = np.argmax(y_prob, axis=1)     return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()  def confusion_matrix_k(y_true, y_prob, K):     y_pred = np.argmax(y_prob, axis=1)     cm = np.zeros((K, K), dtype=int)     for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):         cm[t, p] += 1     return cm In\u00a0[36]: Copied! <pre>N_total = 1500\nK = 3\nn_features = 4\nclass_sep = 1.8\nflip_y = 0.00\n\n# Queremos tamanhos semelhantes por classe\ntarget_per_class = N_total // K  # 500\n\ndef gen_class_subset(target, n_clusters, rs):\n    # como vamos filtrar apenas a classe positiva, geramos o dobro\n    n_samples_tmp = target * 2\n    X_tmp, y_tmp = make_classification(\n        n_samples=n_samples_tmp,\n        n_features=n_features,\n        n_informative=n_features,\n        n_redundant=0,\n        n_classes=2,\n        n_clusters_per_class=n_clusters,\n        class_sep=class_sep,\n        flip_y=flip_y,\n        random_state=rs\n    )\n    X_pos = X_tmp[y_tmp == 1]\n    if len(X_pos) &lt; target:\n        # se por algum motivo veio menos, reamostrar com rs+1\n        X_more, y_more = make_classification(\n            n_samples=n_samples_tmp,\n            n_features=n_features,\n            n_informative=n_features,\n            n_redundant=0,\n            n_classes=2,\n            n_clusters_per_class=n_clusters,\n            class_sep=class_sep,\n            flip_y=flip_y,\n            random_state=rs+101\n        )\n        X_pos = np.vstack([X_pos, X_more[y_more == 1]])\n    return X_pos[:target]\n\n# Classe 0 -&gt; 2 clusters\nX0 = gen_class_subset(target_per_class, n_clusters=2, rs=42)\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# Classe 1 -&gt; 3 clusters\nX1 = gen_class_subset(target_per_class, n_clusters=3, rs=43)\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Classe 2 -&gt; 4 clusters\nX2 = gen_class_subset(target_per_class, n_clusters=4, rs=44)\ny2 = np.full((X2.shape[0],), 2, dtype=int)\n\n# Combine e embaralhe\nX = np.vstack([X0, X1, X2])\ny = np.concatenate([y0, y1, y2])\n\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])\n\n# Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar\npca = PCA(n_components=2, random_state=0)\nX_2d = pca.fit_transform(X)\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c)\nplt.title(\"Dados (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> N_total = 1500 K = 3 n_features = 4 class_sep = 1.8 flip_y = 0.00  # Queremos tamanhos semelhantes por classe target_per_class = N_total // K  # 500  def gen_class_subset(target, n_clusters, rs):     # como vamos filtrar apenas a classe positiva, geramos o dobro     n_samples_tmp = target * 2     X_tmp, y_tmp = make_classification(         n_samples=n_samples_tmp,         n_features=n_features,         n_informative=n_features,         n_redundant=0,         n_classes=2,         n_clusters_per_class=n_clusters,         class_sep=class_sep,         flip_y=flip_y,         random_state=rs     )     X_pos = X_tmp[y_tmp == 1]     if len(X_pos) &lt; target:         # se por algum motivo veio menos, reamostrar com rs+1         X_more, y_more = make_classification(             n_samples=n_samples_tmp,             n_features=n_features,             n_informative=n_features,             n_redundant=0,             n_classes=2,             n_clusters_per_class=n_clusters,             class_sep=class_sep,             flip_y=flip_y,             random_state=rs+101         )         X_pos = np.vstack([X_pos, X_more[y_more == 1]])     return X_pos[:target]  # Classe 0 -&gt; 2 clusters X0 = gen_class_subset(target_per_class, n_clusters=2, rs=42) y0 = np.zeros((X0.shape[0],), dtype=int)  # Classe 1 -&gt; 3 clusters X1 = gen_class_subset(target_per_class, n_clusters=3, rs=43) y1 = np.ones((X1.shape[0],), dtype=int)  # Classe 2 -&gt; 4 clusters X2 = gen_class_subset(target_per_class, n_clusters=4, rs=44) y2 = np.full((X2.shape[0],), 2, dtype=int)  # Combine e embaralhe X = np.vstack([X0, X1, X2]) y = np.concatenate([y0, y1, y2])  perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])  # Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar pca = PCA(n_components=2, random_state=0) X_2d = pca.fit_transform(X) plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c) plt.title(\"Dados (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>Shapes: (1500, 4) (1500,) | classes: [np.int64(500), np.int64(500), np.int64(500)]\n</pre> In\u00a0[37]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[38]: Copied! <pre># Arquitetura\nn_in = n_features\nn_hidden = 32\nn_out = K\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    # y: (m,1)  -&gt; (m,K)\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    z1 = Xb @ W1 + b1         # (m, hidden)\n    a1 = tanh(z1)             # (m, hidden)\n    z2 = a1 @ W2 + b2         # (m, K)\n    a2 = softmax(z2)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m\n    dz2 = (a2 - yb_onehot) / m          # (m,K)\n    dW2 = a1.T @ dz2                    # (hidden,K)\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    da1 = dz2 @ W2.T                    # (m,hidden)\n    dz1 = da1 * dtanh(a1)               # (m,hidden)\n    dW1 = Xb.T @ dz1                    # (n_in,hidden)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)\n\n    return dW1, db1_, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = n_features n_hidden = 32 n_out = K  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def onehot(y, K):     # y: (m,1)  -&gt; (m,K)     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     z1 = Xb @ W1 + b1         # (m, hidden)     a1 = tanh(z1)             # (m, hidden)     z2 = a1 @ W2 + b2         # (m, K)     a2 = softmax(z2)          # (m, K)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m     dz2 = (a2 - yb_onehot) / m          # (m,K)     dW2 = a1.T @ dz2                    # (hidden,K)     db2 = np.sum(dz2, axis=0, keepdims=True)      da1 = dz2 @ W2.T                    # (m,hidden)     dz1 = da1 * dtanh(a1)               # (m,hidden)     dW1 = Xb.T @ dz1                    # (n_in,hidden)     db1_ = np.sum(dz1, axis=0, keepdims=True)      return dW1, db1_, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[39]: Copied! <pre>epochs = 350\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\n\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 350 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m)  Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9847 | acc_train=0.5717\nEpoch   25 | loss=0.7307 | acc_train=0.6967\nEpoch   50 | loss=0.5286 | acc_train=0.8008\nEpoch   75 | loss=0.4153 | acc_train=0.8483\nEpoch  100 | loss=0.3526 | acc_train=0.8850\nEpoch  125 | loss=0.3169 | acc_train=0.8983\nEpoch  150 | loss=0.2904 | acc_train=0.9025\nEpoch  175 | loss=0.2701 | acc_train=0.9025\nEpoch  200 | loss=0.2559 | acc_train=0.9117\nEpoch  225 | loss=0.2439 | acc_train=0.9167\nEpoch  250 | loss=0.2333 | acc_train=0.9150\nEpoch  275 | loss=0.2225 | acc_train=0.9192\nEpoch  300 | loss=0.2137 | acc_train=0.9208\nEpoch  325 | loss=0.2055 | acc_train=0.9283\nEpoch  350 | loss=0.1999 | acc_train=0.9267\n</pre> In\u00a0[40]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9200\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[93  7  0]\n [ 6 90  4]\n [ 6  1 93]]\n</pre> In\u00a0[41]: Copied! <pre># Projeta teste em 2D para visualizar predi\u00e7\u00f5es\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Projeta teste em 2D para visualizar predi\u00e7\u00f5es pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Deeper MLP Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p> In\u00a0[43]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[44]: Copied! <pre># Arquitetura (duas ocultas)\nK = 3                 # n\u00famero de classes\nn_in = X_train.shape[1]\nn_hidden1 = 64\nn_hidden2 = 32\nn_out = K\n\n# Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden1))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1))\nb1 = np.zeros((1, n_hidden1))\n\nlimit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2))\nb2 = np.zeros((1, n_hidden2))\n\nlimit3 = np.sqrt(6.0 / (n_hidden2 + n_out))\nW3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out))\nb3 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    # camada 1\n    z1 = Xb @ W1 + b1         # (m, h1)\n    a1 = tanh(z1)             # (m, h1)\n    # camada 2\n    z2 = a1 @ W2 + b2         # (m, h2)\n    a2 = tanh(z2)             # (m, h2)\n    # sa\u00edda\n    z3 = a2 @ W3 + b3         # (m, K)\n    a3 = softmax(z3)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2, z3, a3)\n    return a3, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2, z3, a3 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE\n    dz3 = (a3 - yb_onehot) / m               # (m,K)\n    dW3 = a2.T @ dz3                         # (h2,K)\n    db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)\n\n    da2 = dz3 @ W3.T                         # (m,h2)\n    dz2 = da2 * dtanh(a2)                    # (m,h2)\n    dW2 = a1.T @ dz2                         # (h1,h2)\n    db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)\n\n    da1 = dz2 @ W2.T                         # (m,h1)\n    dz1 = da1 * dtanh(a1)                    # (m,h1)\n    dW1 = Xb.T @ dz1                         # (n_in,h1)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)\n\n    return dW1, db1_, dW2, db2_, dW3, db3\n\ndef update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):\n    global W1, b1, W2, b2, W3, b3\n    W1 -= lr * dW1; b1 -= lr * db1_\n    W2 -= lr * dW2; b2 -= lr * db2_\n    W3 -= lr * dW3; b3 -= lr * db3_\n</pre> # Arquitetura (duas ocultas) K = 3                 # n\u00famero de classes n_in = X_train.shape[1] n_hidden1 = 64 n_hidden2 = 32 n_out = K  # Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden1)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1)) b1 = np.zeros((1, n_hidden1))  limit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2)) b2 = np.zeros((1, n_hidden2))  limit3 = np.sqrt(6.0 / (n_hidden2 + n_out)) W3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out)) b3 = np.zeros((1, n_out))  def onehot(y, K):     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     # camada 1     z1 = Xb @ W1 + b1         # (m, h1)     a1 = tanh(z1)             # (m, h1)     # camada 2     z2 = a1 @ W2 + b2         # (m, h2)     a2 = tanh(z2)             # (m, h2)     # sa\u00edda     z3 = a2 @ W3 + b3         # (m, K)     a3 = softmax(z3)          # (m, K)     cache = (Xb, z1, a1, z2, a2, z3, a3)     return a3, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2, z3, a3 = cache     m = Xb.shape[0]      # Softmax + CE     dz3 = (a3 - yb_onehot) / m               # (m,K)     dW3 = a2.T @ dz3                         # (h2,K)     db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)      da2 = dz3 @ W3.T                         # (m,h2)     dz2 = da2 * dtanh(a2)                    # (m,h2)     dW2 = a1.T @ dz2                         # (h1,h2)     db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)      da1 = dz2 @ W2.T                         # (m,h1)     dz1 = da1 * dtanh(a1)                    # (m,h1)     dW1 = Xb.T @ dz1                         # (n_in,h1)     db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)      return dW1, db1_, dW2, db2_, dW3, db3  def update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):     global W1, b1, W2, b2, W3, b3     W1 -= lr * dW1; b1 -= lr * db1_     W2 -= lr * dW2; b2 -= lr * db2_     W3 -= lr * dW3; b3 -= lr * db3_  In\u00a0[45]: Copied! <pre>epochs = 400\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 400 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m) Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9843 | acc_train=0.5967\nEpoch   25 | loss=0.5223 | acc_train=0.8083\nEpoch   50 | loss=0.3642 | acc_train=0.8733\nEpoch   75 | loss=0.2886 | acc_train=0.9083\nEpoch  100 | loss=0.2422 | acc_train=0.9167\nEpoch  125 | loss=0.2163 | acc_train=0.9200\nEpoch  150 | loss=0.1987 | acc_train=0.9217\nEpoch  175 | loss=0.1820 | acc_train=0.9292\nEpoch  200 | loss=0.1738 | acc_train=0.9358\nEpoch  225 | loss=0.1579 | acc_train=0.9408\nEpoch  250 | loss=0.1528 | acc_train=0.9450\nEpoch  275 | loss=0.1401 | acc_train=0.9492\nEpoch  300 | loss=0.1403 | acc_train=0.9433\nEpoch  325 | loss=0.1281 | acc_train=0.9500\nEpoch  350 | loss=0.1219 | acc_train=0.9525\nEpoch  375 | loss=0.1154 | acc_train=0.9533\nEpoch  400 | loss=0.1118 | acc_train=0.9583\n</pre> In\u00a0[46]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9533\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[94  6  0]\n [ 2 96  2]\n [ 4  0 96]]\n</pre> In\u00a0[47]: Copied! <pre># Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n</pre> # Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  In\u00a0[48]: Copied! <pre># === Avalia\u00e7\u00e3o treino vs teste ===\nyprob_tr, _ = forward(X_train)\nyprob_te, _ = forward(X_test)\n\nacc_tr = accuracy_multiclass(y_train, yprob_tr)\nacc_te = accuracy_multiclass(y_test, yprob_te)\n\nprint(f\"Acc TREINO: {acc_tr:.4f}\")\nprint(f\"Acc TESTE : {acc_te:.4f}\")\n\nloss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr)\nloss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te)\nprint(f\"Loss TREINO: {loss_tr:.4f}\")\nprint(f\"Loss TESTE : {loss_te:.4f}\")\n</pre> # === Avalia\u00e7\u00e3o treino vs teste === yprob_tr, _ = forward(X_train) yprob_te, _ = forward(X_test)  acc_tr = accuracy_multiclass(y_train, yprob_tr) acc_te = accuracy_multiclass(y_test, yprob_te)  print(f\"Acc TREINO: {acc_tr:.4f}\") print(f\"Acc TESTE : {acc_te:.4f}\")  loss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr) loss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te) print(f\"Loss TREINO: {loss_tr:.4f}\") print(f\"Loss TESTE : {loss_te:.4f}\") <pre>Acc TREINO: 0.9583\nAcc TESTE : 0.9533\nLoss TREINO: 0.1090\nLoss TESTE : 0.1596\n</pre> <p><code>Accuracy Train</code> foi maior que <code>Accuracy Test</code>, indicando que o modelo est\u00e1 sofrendo de overfitting.</p>"},{"location":"Exercicios/EX3/MLP/#3-mlp","title":"3. MLP\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#activity-understanding-multi-layer-perceptrons-mlps","title":"Activity: Understanding Multi-Layer Perceptrons (MLPs)\u00b6","text":"<p>This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs).</p>"},{"location":"Exercicios/EX3/MLP/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-4","title":"Exercise 4\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#verficacao-de-overfitting-do-ex4","title":"Verfica\u00e7\u00e3o de Overfitting do EX4\u00b6","text":""},{"location":"Exercicios/EX4/VAE/","title":"4. Variational Autoencoder (VAE)","text":"In\u00a0[9]: Copied! <pre>import sys, os, math, random, json, time\nfrom dataclasses import dataclass\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n</pre> import sys, os, math, random, json, time from dataclasses import dataclass import numpy as np  import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader  SEED = 42 random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED) device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') device Out[9]: <pre>device(type='cpu')</pre> <ol> <li><p>Data Preparation:</p> <ul> <li>Load the MNIST/Fashion MNIST dataset;</li> <li>Normalize the images to the range [0, 1];</li> <li>Split the dataset into training and validation sets.</li> </ul> </li> </ol> In\u00a0[10]: Copied! <pre>from torchvision import datasets, transforms\n\nIMG_SIZE = 28\nIMG_CH = 1\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])\n\n# Fashion-MNIST\n#train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n#test_ds  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n# MNIST\ntrain_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_ds  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\nBATCH_SIZE = 128\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nlen(train_ds), len(test_ds)\n</pre> from torchvision import datasets, transforms  IMG_SIZE = 28 IMG_CH = 1  transform = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ])  # Fashion-MNIST #train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform) #test_ds  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)  # MNIST train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform) test_ds  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  BATCH_SIZE = 128 train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True) test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)  len(train_ds), len(test_ds)  Out[10]: <pre>(60000, 10000)</pre> In\u00a0[11]: Copied! <pre>class Encoder(nn.Module):\n    def __init__(self, z_dim=2):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(IMG_CH*IMG_SIZE*IMG_SIZE, 512)\n        self.fc_mu = nn.Linear(512, z_dim)\n        self.fc_logvar = nn.Linear(512, z_dim)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\nclass Decoder(nn.Module):\n    def __init__(self, z_dim=2):\n        super().__init__()\n        self.fc1 = nn.Linear(z_dim, 512)\n        self.fc_out = nn.Linear(512, IMG_CH*IMG_SIZE*IMG_SIZE)\n\n    def forward(self, z):\n        h = F.relu(self.fc1(z))\n        x_hat = torch.sigmoid(self.fc_out(h))\n        x_hat = x_hat.view(-1, IMG_CH, IMG_SIZE, IMG_SIZE)\n        return x_hat\n\nclass VAE(nn.Module):\n    def __init__(self, z_dim=2):\n        super().__init__()\n        self.enc = Encoder(z_dim=z_dim)\n        self.dec = Decoder(z_dim=z_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        mu, logvar = self.enc(x)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.dec(z)\n        return x_hat, mu, logvar\n</pre> class Encoder(nn.Module):     def __init__(self, z_dim=2):         super().__init__()         self.flatten = nn.Flatten()         self.fc1 = nn.Linear(IMG_CH*IMG_SIZE*IMG_SIZE, 512)         self.fc_mu = nn.Linear(512, z_dim)         self.fc_logvar = nn.Linear(512, z_dim)      def forward(self, x):         x = self.flatten(x)         h = F.relu(self.fc1(x))         mu = self.fc_mu(h)         logvar = self.fc_logvar(h)         return mu, logvar  class Decoder(nn.Module):     def __init__(self, z_dim=2):         super().__init__()         self.fc1 = nn.Linear(z_dim, 512)         self.fc_out = nn.Linear(512, IMG_CH*IMG_SIZE*IMG_SIZE)      def forward(self, z):         h = F.relu(self.fc1(z))         x_hat = torch.sigmoid(self.fc_out(h))         x_hat = x_hat.view(-1, IMG_CH, IMG_SIZE, IMG_SIZE)         return x_hat  class VAE(nn.Module):     def __init__(self, z_dim=2):         super().__init__()         self.enc = Encoder(z_dim=z_dim)         self.dec = Decoder(z_dim=z_dim)      def reparameterize(self, mu, logvar):         std = torch.exp(0.5 * logvar)         eps = torch.randn_like(std)         return mu + eps * std      def forward(self, x):         mu, logvar = self.enc(x)         z = self.reparameterize(mu, logvar)         x_hat = self.dec(z)         return x_hat, mu, logvar  In\u00a0[12]: Copied! <pre>def vae_loss(x, x_hat, mu, logvar, recon='bce'):\n    if recon == 'bce':\n        recon_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')\n    else:\n        recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return (recon_loss + kl), recon_loss, kl\n</pre> def vae_loss(x, x_hat, mu, logvar, recon='bce'):     if recon == 'bce':         recon_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')     else:         recon_loss = F.mse_loss(x_hat, x, reduction='sum')     kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())     return (recon_loss + kl), recon_loss, kl In\u00a0[13]: Copied! <pre>def train_epoch(model, loader, opt):\n    model.train()\n    total, recon_total, kl_total = 0., 0., 0.\n    for x, _ in loader:\n        x = x.to(device)\n        opt.zero_grad()\n        x_hat, mu, logvar = model(x)\n        loss, recon, kl = vae_loss(x, x_hat, mu, logvar, recon='bce')\n        loss.backward()\n        opt.step()\n        total += loss.item(); recon_total += recon.item(); kl_total += kl.item()\n    n = len(loader.dataset)\n    return total/n, recon_total/n, kl_total/n\n\n@torch.no_grad()\ndef eval_epoch(model, loader):\n    model.eval()\n    total, recon_total, kl_total = 0., 0., 0.\n    for x, _ in loader:\n        x = x.to(device)\n        x_hat, mu, logvar = model(x)\n        loss, recon, kl = vae_loss(x, x_hat, mu, logvar, recon='bce')\n        total += loss.item(); recon_total += recon.item(); kl_total += kl.item()\n    n = len(loader.dataset)\n    return total/n, recon_total/n, kl_total/n\n</pre> def train_epoch(model, loader, opt):     model.train()     total, recon_total, kl_total = 0., 0., 0.     for x, _ in loader:         x = x.to(device)         opt.zero_grad()         x_hat, mu, logvar = model(x)         loss, recon, kl = vae_loss(x, x_hat, mu, logvar, recon='bce')         loss.backward()         opt.step()         total += loss.item(); recon_total += recon.item(); kl_total += kl.item()     n = len(loader.dataset)     return total/n, recon_total/n, kl_total/n  @torch.no_grad() def eval_epoch(model, loader):     model.eval()     total, recon_total, kl_total = 0., 0., 0.     for x, _ in loader:         x = x.to(device)         x_hat, mu, logvar = model(x)         loss, recon, kl = vae_loss(x, x_hat, mu, logvar, recon='bce')         total += loss.item(); recon_total += recon.item(); kl_total += kl.item()     n = len(loader.dataset)     return total/n, recon_total/n, kl_total/n In\u00a0[14]: Copied! <pre># Par\u00e2metros usados para treinar o modelo (pode mudar todos para testar)\nz_dim = 2\nlr = 1e-3\nepochs = 5\nmodel = VAE(z_dim=z_dim).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=lr)\n\nfor epoch in range(1, epochs+1):\n    tr = train_epoch(model, train_loader, opt)\n    va = eval_epoch(model, test_loader)\n    print(f\"Epoch {epoch:02d} | train: loss={tr[0]:.4f} recon={tr[1]:.4f} kl={tr[2]:.4f} | \"\n          f\"val: loss={va[0]:.4f} recon={va[1]:.4f} kl={va[2]:.4f}\")\n</pre> # Par\u00e2metros usados para treinar o modelo (pode mudar todos para testar) z_dim = 2 lr = 1e-3 epochs = 5 model = VAE(z_dim=z_dim).to(device) opt = torch.optim.Adam(model.parameters(), lr=lr)  for epoch in range(1, epochs+1):     tr = train_epoch(model, train_loader, opt)     va = eval_epoch(model, test_loader)     print(f\"Epoch {epoch:02d} | train: loss={tr[0]:.4f} recon={tr[1]:.4f} kl={tr[2]:.4f} | \"           f\"val: loss={va[0]:.4f} recon={va[1]:.4f} kl={va[2]:.4f}\") <pre>Epoch 01 | train: loss=185.8311 recon=180.6728 kl=5.1583 | val: loss=168.3167 recon=163.4006 kl=4.9161\nEpoch 02 | train: loss=165.5745 recon=160.4278 kl=5.1467 | val: loss=162.9210 recon=157.8218 kl=5.0992\nEpoch 03 | train: loss=162.0745 recon=156.7865 kl=5.2880 | val: loss=160.3517 recon=154.9527 kl=5.3990\nEpoch 04 | train: loss=159.9896 recon=154.5625 kl=5.4271 | val: loss=158.3804 recon=152.9518 kl=5.4286\nEpoch 05 | train: loss=158.2535 recon=152.7199 kl=5.5336 | val: loss=157.3469 recon=151.7043 kl=5.6426\n</pre> In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n\n@torch.no_grad()\ndef show_reconstructions(model, loader, num=8):\n    model.eval()\n    x, _ = next(iter(loader))\n    x = x.to(device)[:num]\n    x_hat, _, _ = model(x)\n    x = x.cpu().numpy(); x_hat = x_hat.cpu().numpy()\n    plt.figure(figsize=(num, 2))\n    for i in range(num):\n        plt.subplot(2, num, i+1); plt.imshow(x[i].squeeze(), cmap='gray'); plt.axis('off')\n        plt.subplot(2, num, num+i+1); plt.imshow(x_hat[i].squeeze(), cmap='gray'); plt.axis('off')\n    plt.show()\n</pre> import matplotlib.pyplot as plt  @torch.no_grad() def show_reconstructions(model, loader, num=8):     model.eval()     x, _ = next(iter(loader))     x = x.to(device)[:num]     x_hat, _, _ = model(x)     x = x.cpu().numpy(); x_hat = x_hat.cpu().numpy()     plt.figure(figsize=(num, 2))     for i in range(num):         plt.subplot(2, num, i+1); plt.imshow(x[i].squeeze(), cmap='gray'); plt.axis('off')         plt.subplot(2, num, num+i+1); plt.imshow(x_hat[i].squeeze(), cmap='gray'); plt.axis('off')     plt.show() <ol> <li><p>Visualization:</p> <ul> <li>Visualize original and reconstructed images;</li> <li>Visualize the latent space (in case of a latent space until 3-D, otherwise use a reduced visualization, e.g., using t-SNE, UMAP or PCA).</li> </ul> </li> </ol> In\u00a0[16]: Copied! <pre>show_reconstructions(model, test_loader, num=8)\n</pre> show_reconstructions(model, test_loader, num=8) <pre>c:\\Users\\matra\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> In\u00a0[17]: Copied! <pre>@torch.no_grad()\ndef plot_latent_space(model, loader, num_batches=100):\n    model.eval()\n    zs, labels = [], []\n    for i, (x, y) in enumerate(loader):\n        x = x.to(device)\n        mu, logvar = model.enc(x)\n        z = model.reparameterize(mu, logvar)\n        zs.append(z.cpu())\n        labels.append(y)\n        if i &gt;= num_batches:\n            break\n    zs = torch.cat(zs)\n    labels = torch.cat(labels)\n\n    plt.figure(figsize=(8,6))\n    scatter = plt.scatter(zs[:,0], zs[:,1], c=labels, cmap='tab10', s=8, alpha=0.7)\n    plt.colorbar(scatter, ticks=range(10))\n    plt.title(\"Espa\u00e7o Latente (z\u2081 vs z\u2082)\")\n    plt.xlabel(\"z\u2081\")\n    plt.ylabel(\"z\u2082\")\n    plt.show()\n\nplot_latent_space(model, test_loader)\n</pre> @torch.no_grad() def plot_latent_space(model, loader, num_batches=100):     model.eval()     zs, labels = [], []     for i, (x, y) in enumerate(loader):         x = x.to(device)         mu, logvar = model.enc(x)         z = model.reparameterize(mu, logvar)         zs.append(z.cpu())         labels.append(y)         if i &gt;= num_batches:             break     zs = torch.cat(zs)     labels = torch.cat(labels)      plt.figure(figsize=(8,6))     scatter = plt.scatter(zs[:,0], zs[:,1], c=labels, cmap='tab10', s=8, alpha=0.7)     plt.colorbar(scatter, ticks=range(10))     plt.title(\"Espa\u00e7o Latente (z\u2081 vs z\u2082)\")     plt.xlabel(\"z\u2081\")     plt.ylabel(\"z\u2082\")     plt.show()  plot_latent_space(model, test_loader)"},{"location":"Exercicios/EX4/VAE/#4-variational-autoencoder-vae","title":"4. Variational Autoencoder (VAE)\u00b6","text":""},{"location":"Exercicios/EX4/VAE/#activity-vae-implementation","title":"Activity: VAE Implementation\u00b6","text":"<p>In this exercise, you will implement and evaluate a Variational Autoencoder (VAE) on the MNIST or Fashion MNIST dataset. The goal is to understand the architecture, training process, and performance of VAEs.</p>"},{"location":"Exercicios/EX4/VAE/#parte-1-requisitos-dados","title":"Parte 1 \u2014 Requisitos &amp; Dados\u00b6","text":""},{"location":"Exercicios/EX4/VAE/#parte-2-arquitetura-do-vae","title":"Parte 2 \u2014 Arquitetura do VAE\u00b6","text":"<ol> <li><p>Model Implementation:</p> <ul> <li>Define the VAE architecture, including the encoder and decoder networks;</li> <li>Implement the reparameterization trick.</li> </ul> </li> </ol>"},{"location":"Exercicios/EX4/VAE/#parte-3-funcoes-de-perda-elbo","title":"Parte 3 \u2014 Fun\u00e7\u00f5es de Perda (ELBO)\u00b6","text":"<ol> <li>Training:<ul> <li>Define the loss function (reconstruction loss + KL divergence);</li> <li>Implement the training loop.</li> </ul> </li> </ol>"},{"location":"Exercicios/EX4/VAE/#parte-4-treinamento","title":"Parte 4 \u2014 Treinamento\u00b6","text":""},{"location":"Exercicios/EX4/VAE/#parte-5-avaliacao-visualizacao","title":"Parte 5 \u2014 Avalia\u00e7\u00e3o &amp; Visualiza\u00e7\u00e3o\u00b6","text":"<ol> <li><p>Evaluation:</p> <ul> <li>Evaluate the VAE's performance on the validation set;</li> <li>Generate new samples from the learned latent space.</li> </ul> </li> </ol>"},{"location":"Exercicios/EX4/VAE/#parte-6-report","title":"Parte 6 \u2014 Report\u00b6","text":""},{"location":"Exercicios/EX4/VAE/#metodologia","title":"Metodologia\u00b6","text":"<ul> <li>Dataset: MNIST (60.000 imagens de treino, 28\u00d728, escala de cinza).</li> <li>Modelo: Encoder e decoder totalmente conectados (MLP).</li> <li>Latent dimension (z_dim): 2 \u2192 permite visualiza\u00e7\u00e3o direta em 2D.</li> <li>Loss: Binary Cross Entropy (reconstru\u00e7\u00e3o) + KL Divergence.</li> <li>Otimiza\u00e7\u00e3o: Adam (lr = 1e-3), 5 \u00e9pocas iniciais.</li> <li>Framework: PyTorch.</li> </ul>"},{"location":"Exercicios/EX4/VAE/#resultados","title":"Resultados\u00b6","text":"<p>O modelo conseguiu reconstruir imagens de forma coerente, preservando bem a estrutura geral dos d\u00edgitos originais. As reconstru\u00e7\u00f5es ainda apresentaram borr\u00f5es leves, principalmente em d\u00edgitos curvos, o que \u00e9 esperado em um modelo com poucos par\u00e2metros e espa\u00e7o latente de baixa dimens\u00e3o.</p> <p>O espa\u00e7o latente em duas dimens\u00f5es mostrou agrupamentos relativamente bem definidos, com regi\u00f5es correspondentes a diferentes classes de d\u00edgitos. Embora exista sobreposi\u00e7\u00e3o entre alguns grupos, a separa\u00e7\u00e3o geral demonstra que o VAE aprendeu uma representa\u00e7\u00e3o cont\u00ednua e estruturada dos dados.</p>"},{"location":"Exercicios/EX4/VAE/#desafios-e-aprendizados","title":"Desafios e Aprendizados\u00b6","text":"<ul> <li>Borr\u00f5es nas reconstru\u00e7\u00f5es: consequ\u00eancia do equil\u00edbrio entre reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o (termo KL).</li> <li>Latente 2D: limita a capacidade de capturar varia\u00e7\u00f5es mais complexas de estilo e formato.</li> <li>Melhorias poss\u00edveis:<ul> <li>Aumentar <code>z_dim</code> (por exemplo, 8 ou 16);</li> <li>Usar arquitetura convolucional (ConvVAE);</li> <li>Aplicar annealing do KL (\u03b2-VAE);</li> <li>Treinar por mais \u00e9pocas (20\u201350) para estabilidade e converg\u00eancia.</li> </ul> </li> </ul>"},{"location":"Exercicios/EX4/VAE/#conclusao","title":"Conclus\u00e3o\u00b6","text":"<p>O modelo conseguiu aprender representa\u00e7\u00f5es latentes significativas dos d\u00edgitos MNIST. Mesmo com arquitetura simples e espa\u00e7o latente de apenas duas dimens\u00f5es, o VAE foi capaz de gerar reconstru\u00e7\u00f5es reconhec\u00edveis e organizar o espa\u00e7o latente de forma coerente. Esses resultados demonstram a capacidade dos VAEs de capturar padr\u00f5es estruturais nos dados e gerar representa\u00e7\u00f5es cont\u00ednuas e interpret\u00e1veis, servindo como base para modelos mais sofisticados.</p>"},{"location":"Projetos/Projeto1/notebook/","title":"Multi-Layer Perceptron (MLP) Classification","text":"In\u00a0[3]: Copied! <pre># Basic imports &amp; reproducibility\nimport os\nimport random\nfrom pathlib import Path\nimport warnings\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport copy\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n# stats / util\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# preprocessing &amp; modeling\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score, classification_report\n)\n\n# notebook display\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\nwarnings.filterwarnings('ignore')\n\n# plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\n# seeds for reproducibility (use consistently across notebook)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n</pre> # Basic imports &amp; reproducibility import os import random from pathlib import Path import warnings  import math import numpy as np import pandas as pd import itertools import copy  # plotting import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import MaxNLocator  # stats / util from scipy import stats from tqdm.auto import tqdm  # preprocessing &amp; modeling from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.utils.class_weight import compute_class_weight  # metrics from sklearn.metrics import (     accuracy_score, precision_score, recall_score, f1_score,     confusion_matrix, roc_auc_score, roc_curve,     precision_recall_curve, average_precision_score, classification_report )  # notebook display %matplotlib inline pd.set_option('display.max_columns', None) pd.set_option('display.width', 200) warnings.filterwarnings('ignore')  # plotting style plt.style.use('seaborn-v0_8-whitegrid') sns.set_context('notebook')  # seeds for reproducibility (use consistently across notebook) SEED = 42 random.seed(SEED) np.random.seed(SEED) os.environ['PYTHONHASHSEED'] = str(SEED) In\u00a0[4]: Copied! <pre># Load dataset\nDATA_DIR = Path(\"data\")\ntrain = pd.read_csv(DATA_DIR / \"train.csv\")\ntest = pd.read_csv(DATA_DIR / \"test.csv\")\n\nprint(\"Dataset Shape:\")\nprint(f\"  Training: {train.shape}\")\nprint(f\"  Test: {test.shape}\")\n\nprint(\"\\nFirst 5 rows:\")\ndisplay(train.head())\n\nprint(\"\\nData Types:\")\nprint(train.dtypes)\n</pre> # Load dataset DATA_DIR = Path(\"data\") train = pd.read_csv(DATA_DIR / \"train.csv\") test = pd.read_csv(DATA_DIR / \"test.csv\")  print(\"Dataset Shape:\") print(f\"  Training: {train.shape}\") print(f\"  Test: {test.shape}\")  print(\"\\nFirst 5 rows:\") display(train.head())  print(\"\\nData Types:\") print(train.dtypes) <pre>Dataset Shape:\n  Training: (750000, 18)\n  Test: (250000, 17)\n\nFirst 5 rows:\n</pre> id age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 0 42 technician married secondary no 7 no no cellular 25 aug 117 3 -1 0 unknown 0 1 1 38 blue-collar married secondary no 514 no no unknown 18 jun 185 1 -1 0 unknown 0 2 2 36 blue-collar married secondary no 602 yes no unknown 14 may 111 2 -1 0 unknown 0 3 3 27 student single secondary no 34 yes no unknown 28 may 10 2 -1 0 unknown 0 4 4 26 technician married secondary no 889 yes no cellular 3 feb 902 1 -1 0 unknown 1 <pre>\nData Types:\nid            int64\nage           int64\njob          object\nmarital      object\neducation    object\ndefault      object\nbalance       int64\nhousing      object\nloan         object\ncontact      object\nday           int64\nmonth        object\nduration      int64\ncampaign      int64\npdays         int64\nprevious      int64\npoutcome     object\ny             int64\ndtype: object\n</pre> In\u00a0[5]: Copied! <pre># Identify feature types\ntarget_col = 'y'\nid_col = 'id'\n\nnumerical_features = train.select_dtypes(include=['number']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n</pre> # Identify feature types target_col = 'y' id_col = 'id'  numerical_features = train.select_dtypes(include=['number']).columns.tolist() numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]  categorical_features = train.select_dtypes(include=['object']).columns.tolist()  print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\") print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\") <pre>Numerical features (7): ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n\nCategorical features (9): ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n</pre> In\u00a0[6]: Copied! <pre># Summary statistics\ndisplay(train[numerical_features].describe())\n\nprint(\"\\nTarget Variable Distribution:\")\nprint(train[target_col].value_counts())\nprint(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\")\n</pre> # Summary statistics display(train[numerical_features].describe())  print(\"\\nTarget Variable Distribution:\") print(train[target_col].value_counts()) print(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\") age balance day duration campaign pdays previous count 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 mean 40.926395 1204.067397 16.117209 256.229144 2.577008 22.412733 0.298545 std 10.098829 2836.096759 8.250832 272.555662 2.718514 77.319998 1.335926 min 18.000000 -8019.000000 1.000000 1.000000 1.000000 -1.000000 0.000000 25% 33.000000 0.000000 9.000000 91.000000 1.000000 -1.000000 0.000000 50% 39.000000 634.000000 17.000000 133.000000 2.000000 -1.000000 0.000000 75% 48.000000 1390.000000 21.000000 361.000000 3.000000 -1.000000 0.000000 max 95.000000 99717.000000 31.000000 4918.000000 63.000000 871.000000 200.000000 <pre>\nTarget Variable Distribution:\ny\n0    659512\n1     90488\nName: count, dtype: int64\n\nClass Balance: y\n0    0.879349\n1    0.120651\nName: proportion, dtype: float64\n</pre> In\u00a0[7]: Copied! <pre># Check for missing values\nprint(\"Missing Values:\")\nprint(train.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n</pre> # Check for missing values print(\"Missing Values:\") print(train.isnull().sum())  # Check for duplicates print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\") <pre>Missing Values:\nid           0\nage          0\njob          0\nmarital      0\neducation    0\ndefault      0\nbalance      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\ny            0\ndtype: int64\n\nDuplicate Rows: 0\n</pre> In\u00a0[8]: Copied! <pre># Check for outliers (beyond 3 standard deviations)\noutlier_summary = {}\nfor col in numerical_features:\n    z = np.abs(stats.zscore(train[col]))\n    outlier_summary[col] = (z &gt; 3).mean() * 100\n\noutlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers'])\ndisplay(outlier_df.sort_values('% Outliers', ascending=False))\n</pre> # Check for outliers (beyond 3 standard deviations) outlier_summary = {} for col in numerical_features:     z = np.abs(stats.zscore(train[col]))     outlier_summary[col] = (z &gt; 3).mean() * 100  outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers']) display(outlier_df.sort_values('% Outliers', ascending=False)) % Outliers pdays 4.114667 campaign 2.006000 duration 1.752133 previous 1.625867 balance 1.238267 age 0.547867 day 0.000000 In\u00a0[9]: Copied! <pre># Feature skewness\nskewness = train[numerical_features].skew().sort_values(ascending=False)\nprint(\"Feature Skewness:\")\ndisplay(skewness)\n</pre> # Feature skewness skewness = train[numerical_features].skew().sort_values(ascending=False) print(\"Feature Skewness:\") display(skewness) <pre>Feature Skewness:\n</pre> <pre>previous    13.749885\nbalance     12.304123\ncampaign     4.810437\npdays        3.625049\nduration     2.048776\nage          0.586137\nday          0.054014\ndtype: float64</pre> In\u00a0[10]: Copied! <pre>fig, axes = plt.subplots(3, 3, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_features):\n    data = train[col].dropna()\n    # histogram with density overlay (optional)\n    axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)\n    axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)\n    axes[i].grid(alpha=0.3)\n    if data.max() &gt; 5000:\n        axes[i].set_xscale(\"log\")\n    axes[i].set_xlabel(\"\")\n    axes[i].set_ylabel(\"\")\n\n# hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(3, 3, figsize=(15, 12)) axes = axes.flatten()  for i, col in enumerate(numerical_features):     data = train[col].dropna()     # histogram with density overlay (optional)     axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)     axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)     axes[i].grid(alpha=0.3)     if data.max() &gt; 5000:         axes[i].set_xscale(\"log\")     axes[i].set_xlabel(\"\")     axes[i].set_ylabel(\"\")  # hide any unused subplots for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[11]: Copied! <pre># Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation = train[numerical_features + [target_col]].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\nplt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost correlated features with target:\")\nprint(correlation[target_col].sort_values(ascending=False))\n</pre> # Correlation matrix plt.figure(figsize=(10, 8)) correlation = train[numerical_features + [target_col]].corr() sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True) plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()  print(\"\\nMost correlated features with target:\") print(correlation[target_col].sort_values(ascending=False)) <pre>\nMost correlated features with target:\ny           1.000000\nduration    0.519283\nbalance     0.122513\nprevious    0.119552\npdays       0.089277\nage         0.009523\nday        -0.049625\ncampaign   -0.075829\nName: y, dtype: float64\n</pre> In\u00a0[12]: Copied! <pre>detailed = []\nfor c in categorical_features:\n    vc = train[c].value_counts(dropna=False)\n    top1 = f\"{vc.index[0]}\"\n    detailed.append({\n        'feature': c,\n        'n_unique': train[c].nunique(dropna=False),\n        'most_freq': top1,\n        'total_rows': len(train)\n    })\ncat_summary_full = pd.DataFrame(detailed).set_index('feature')\ndisplay(cat_summary_full)\n</pre> detailed = [] for c in categorical_features:     vc = train[c].value_counts(dropna=False)     top1 = f\"{vc.index[0]}\"     detailed.append({         'feature': c,         'n_unique': train[c].nunique(dropna=False),         'most_freq': top1,         'total_rows': len(train)     }) cat_summary_full = pd.DataFrame(detailed).set_index('feature') display(cat_summary_full) n_unique most_freq total_rows feature job 12 management 750000 marital 3 married 750000 education 4 secondary 750000 default 2 no 750000 housing 2 yes 750000 loan 2 no 750000 contact 3 cellular 750000 month 12 may 750000 poutcome 4 unknown 750000 In\u00a0[13]: Copied! <pre>cat_cols = categorical_features\nn_features = len(cat_cols)\n\n# Choose subplot grid layout (3 columns works well)\ncols = 3\nrows = math.ceil(n_features / cols)\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4))\naxes = axes.flatten()\n\nfor i, c in enumerate(cat_cols):\n    ax = axes[i]\n    vals = train[c].value_counts(dropna=False)\n    categories = vals.index.astype(str).tolist()\n    counts = vals.values\n\n    # Compute target rate per category (align with counts)\n    rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values\n\n    # Plot count bars\n    ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")\n\n    ax.set_xticks(range(len(categories)))\n    ax.set_xticklabels(categories, rotation=90, fontsize=7)\n    ax.set_ylabel(\"count\")\n    ax.set_title(f'{c} (n={len(categories)})', fontsize=10)\n    ax.grid(alpha=0.2)\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n# Hide unused subplots if any\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> cat_cols = categorical_features n_features = len(cat_cols)  # Choose subplot grid layout (3 columns works well) cols = 3 rows = math.ceil(n_features / cols)  fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4)) axes = axes.flatten()  for i, c in enumerate(cat_cols):     ax = axes[i]     vals = train[c].value_counts(dropna=False)     categories = vals.index.astype(str).tolist()     counts = vals.values      # Compute target rate per category (align with counts)     rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values      # Plot count bars     ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")      ax.set_xticks(range(len(categories)))     ax.set_xticklabels(categories, rotation=90, fontsize=7)     ax.set_ylabel(\"count\")     ax.set_title(f'{c} (n={len(categories)})', fontsize=10)     ax.grid(alpha=0.2)     ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # Hide unused subplots if any for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[14]: Copied! <pre>plt.figure(figsize=(5,4))\nsns.barplot(x=train[target_col].value_counts().index, \n            y=train[target_col].value_counts().values, palette='pastel')\nplt.title('Target Variable Distribution')\nplt.xlabel('Subscribed (y)')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No (0)', 'Yes (1)'])\nplt.grid(alpha=0.2)\nplt.show()\n</pre> plt.figure(figsize=(5,4)) sns.barplot(x=train[target_col].value_counts().index,              y=train[target_col].value_counts().values, palette='pastel') plt.title('Target Variable Distribution') plt.xlabel('Subscribed (y)') plt.ylabel('Count') plt.xticks([0, 1], ['No (0)', 'Yes (1)']) plt.grid(alpha=0.2) plt.show() In\u00a0[15]: Copied! <pre># Cap outliers (1st/99th percentile)\ntrain_work = train.copy()\n\nfor col in numerical_features:\n    lower, upper = np.percentile(train_work[col], [1, 99])\n    train_work[col] = np.clip(train_work[col], lower, upper)\n</pre> # Cap outliers (1st/99th percentile) train_work = train.copy()  for col in numerical_features:     lower, upper = np.percentile(train_work[col], [1, 99])     train_work[col] = np.clip(train_work[col], lower, upper) In\u00a0[16]: Copied! <pre># Handle skewness with log1p\n# Replace -1 with 0 for pdays (no previous contact)\nif \"pdays\" in train_work.columns:\n    train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)\n\n# Automatically detect skewed columns (|skew| \u2265 1)\nskewness = train_work[numerical_features].skew()\nskewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist()\ntrain_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))\n\nprint(f\"Highly skewed features transformed: {skewed_cols}\")\n</pre> # Handle skewness with log1p # Replace -1 with 0 for pdays (no previous contact) if \"pdays\" in train_work.columns:     train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)  # Automatically detect skewed columns (|skew| \u2265 1) skewness = train_work[numerical_features].skew() skewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist() train_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))  print(f\"Highly skewed features transformed: {skewed_cols}\") <pre>Highly skewed features transformed: ['balance', 'duration', 'campaign', 'pdays', 'previous']\n</pre> In\u00a0[17]: Copied! <pre># Define features and target\nX_full = train_work.drop(columns=[target_col, id_col])\ny_full = train_work[target_col].values\n</pre> # Define features and target X_full = train_work.drop(columns=[target_col, id_col]) y_full = train_work[target_col].values In\u00a0[18]: Copied! <pre># Split into train / val / test\n# First split train+val vs test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED\n)\n# Then split train vs val (from the remaining 85%)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED\n)  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall\n\nprint(\"Final splits:\")\nprint(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\nprint(\" Class balance (train):\", np.bincount(y_train))\nprint(\" Class balance (val):  \", np.bincount(y_val))\nprint(\" Class balance (test): \", np.bincount(y_test))\n</pre> # Split into train / val / test # First split train+val vs test X_temp, X_test, y_temp, y_test = train_test_split(     X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED ) # Then split train vs val (from the remaining 85%) X_train, X_val, y_train, y_val = train_test_split(     X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED )  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall  print(\"Final splits:\") print(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\") print(\" Class balance (train):\", np.bincount(y_train)) print(\" Class balance (val):  \", np.bincount(y_val)) print(\" Class balance (test): \", np.bincount(y_test)) <pre>Final splits:\n Train: (524981, 16), Val: (112519, 16), Test: (112500, 16)\n Class balance (train): [461642  63339]\n Class balance (val):   [98943 13576]\n Class balance (test):  [98927 13573]\n</pre> In\u00a0[19]: Copied! <pre># Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numerical_features),\n        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\n# Fit only on training data\npreprocessor.fit(X_train)\n\n# Transform splits\nX_train_proc = preprocessor.transform(X_train)\nX_val_proc   = preprocessor.transform(X_val)\nX_test_proc  = preprocessor.transform(X_test)\n\nprint(\"\\nProcessed feature dimensions:\")\nprint(f\" Train: {X_train_proc.shape}\")\nprint(f\" Val:   {X_val_proc.shape}\")\nprint(f\" Test:  {X_test_proc.shape}\")\n</pre> # Preprocessing pipeline preprocessor = ColumnTransformer(     transformers=[         (\"num\", StandardScaler(), numerical_features),         (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),     ],     remainder=\"drop\", )  # Fit only on training data preprocessor.fit(X_train)  # Transform splits X_train_proc = preprocessor.transform(X_train) X_val_proc   = preprocessor.transform(X_val) X_test_proc  = preprocessor.transform(X_test)  print(\"\\nProcessed feature dimensions:\") print(f\" Train: {X_train_proc.shape}\") print(f\" Val:   {X_val_proc.shape}\") print(f\" Test:  {X_test_proc.shape}\") <pre>\nProcessed feature dimensions:\n Train: (524981, 42)\n Val:   (112519, 42)\n Test:  (112500, 42)\n</pre> In\u00a0[20]: Copied! <pre># Class weights (for imbalance)\ncw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weight = {0: cw[0], 1: cw[1]}\nprint(\"\\nClass weights:\", class_weight)\n</pre> # Class weights (for imbalance) cw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train) class_weight = {0: cw[0], 1: cw[1]} print(\"\\nClass weights:\", class_weight) <pre>\nClass weights: {0: np.float64(0.5686018603160025), 1: np.float64(4.144216043827657)}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[21]: Copied! <pre>class MLP:\n    \"\"\"\n    Multi-Layer Perceptron (NumPy) for binary classification.\n    Architecture: Input \u2192 Hidden Layer(s) \u2192 Output\n    Activation: ReLU (hidden), Sigmoid (output)\n    Loss: Binary Cross-Entropy with optional L2 regularization\n    Optimizer: SGD (parameter updates handled externally)\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):\n        np.random.seed(random_state)\n        self.l2_lambda = l2_lambda\n        \n        # Define layer sizes: input, hidden(s), output\n        layer_sizes = [input_dim] + hidden_sizes + [1]\n        self.num_layers = len(layer_sizes) - 1\n        \n        # Initialize parameters with He initialization (good for ReLU)\n        self.weights = []\n        self.biases = []\n        for i in range(self.num_layers):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            std = np.sqrt(2.0 / n_in)\n            self.weights.append(np.random.randn(n_in, n_out) * std)\n            self.biases.append(np.zeros((1, n_out)))\n\n    # Activation functions\n    def relu(self, x):\n        \"\"\"ReLU activation\"\"\"\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (x &gt; 0).astype(float)\n\n    def sigmoid(self, x):\n        \"\"\"Numerically stable sigmoid\"\"\"\n        out = np.empty_like(x)\n        pos = x &gt;= 0\n        out[pos] = 1 / (1 + np.exp(-x[pos]))\n        neg = ~pos\n        exp_x = np.exp(x[neg])\n        out[neg] = exp_x / (1 + exp_x)\n        return out\n\n    # Forward propagation\n    def forward(self, X):\n        \"\"\"\n        Forward pass through all layers.\n        Returns:\n            y_pred: predicted probabilities\n            cache: activations for backpropagation\n        \"\"\"\n        cache = {'A0': X}\n        A = X\n        for i in range(self.num_layers - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            cache[f'Z{i+1}'] = Z\n            cache[f'A{i+1}'] = A\n        Z_out = A @ self.weights[-1] + self.biases[-1]\n        A_out = self.sigmoid(Z_out)\n        cache[f'Z{self.num_layers}'] = Z_out\n        cache[f'A{self.num_layers}'] = A_out\n        return A_out, cache\n\n    # Loss computation\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Binary Cross-Entropy + optional L2 penalty.\n        \"\"\"\n        m = y_true.shape[0]\n        y_true = y_true.reshape(-1, 1)\n        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n        bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))\n        return bce + l2\n\n    # Backpropagation\n    def backward(self, cache, y_true):\n        \"\"\"\n        Compute gradients for all parameters using backpropagation.\n        \"\"\"\n        y_true = y_true.reshape(-1, 1)\n        m = y_true.shape[0]\n        grads_w, grads_b = [], []\n\n        # Gradient for output layer\n        y_pred = cache[f'A{self.num_layers}']\n        dZ = y_pred - y_true\n\n        for i in range(self.num_layers - 1, -1, -1):\n            A_prev = cache[f'A{i}']\n            dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]\n            db = np.mean(dZ, axis=0, keepdims=True)\n            grads_w.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = dZ @ self.weights[i].T\n                dZ = dA * self.relu_derivative(cache[f'Z{i}'])\n\n        return grads_w, grads_b\n\n    # Parameter update\n    def update_parameters(self, grads_w, grads_b, learning_rate):\n        \"\"\"Apply gradient descent step.\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= learning_rate * grads_w[i]\n            self.biases[i]  -= learning_rate * grads_b[i]\n\n        # Prediction helpers\n    def predict_proba(self, X):\n        \"\"\"\n        Compute output probabilities for given inputs.\n        \"\"\"\n        y_pred, _ = self.forward(X)\n        return y_pred.reshape(-1)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"\n        Predict binary class labels (0/1).\n        \"\"\"\n        return (self.predict_proba(X) &gt;= threshold).astype(int)\n\n\nprint(\"MLP class implemented successfully.\")\n</pre> class MLP:     \"\"\"     Multi-Layer Perceptron (NumPy) for binary classification.     Architecture: Input \u2192 Hidden Layer(s) \u2192 Output     Activation: ReLU (hidden), Sigmoid (output)     Loss: Binary Cross-Entropy with optional L2 regularization     Optimizer: SGD (parameter updates handled externally)     \"\"\"          def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):         np.random.seed(random_state)         self.l2_lambda = l2_lambda                  # Define layer sizes: input, hidden(s), output         layer_sizes = [input_dim] + hidden_sizes + [1]         self.num_layers = len(layer_sizes) - 1                  # Initialize parameters with He initialization (good for ReLU)         self.weights = []         self.biases = []         for i in range(self.num_layers):             n_in, n_out = layer_sizes[i], layer_sizes[i+1]             std = np.sqrt(2.0 / n_in)             self.weights.append(np.random.randn(n_in, n_out) * std)             self.biases.append(np.zeros((1, n_out)))      # Activation functions     def relu(self, x):         \"\"\"ReLU activation\"\"\"         return np.maximum(0, x)      def relu_derivative(self, x):         \"\"\"Derivative of ReLU\"\"\"         return (x &gt; 0).astype(float)      def sigmoid(self, x):         \"\"\"Numerically stable sigmoid\"\"\"         out = np.empty_like(x)         pos = x &gt;= 0         out[pos] = 1 / (1 + np.exp(-x[pos]))         neg = ~pos         exp_x = np.exp(x[neg])         out[neg] = exp_x / (1 + exp_x)         return out      # Forward propagation     def forward(self, X):         \"\"\"         Forward pass through all layers.         Returns:             y_pred: predicted probabilities             cache: activations for backpropagation         \"\"\"         cache = {'A0': X}         A = X         for i in range(self.num_layers - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             cache[f'Z{i+1}'] = Z             cache[f'A{i+1}'] = A         Z_out = A @ self.weights[-1] + self.biases[-1]         A_out = self.sigmoid(Z_out)         cache[f'Z{self.num_layers}'] = Z_out         cache[f'A{self.num_layers}'] = A_out         return A_out, cache      # Loss computation     def compute_loss(self, y_true, y_pred):         \"\"\"         Binary Cross-Entropy + optional L2 penalty.         \"\"\"         m = y_true.shape[0]         y_true = y_true.reshape(-1, 1)         y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)         bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))         l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))         return bce + l2      # Backpropagation     def backward(self, cache, y_true):         \"\"\"         Compute gradients for all parameters using backpropagation.         \"\"\"         y_true = y_true.reshape(-1, 1)         m = y_true.shape[0]         grads_w, grads_b = [], []          # Gradient for output layer         y_pred = cache[f'A{self.num_layers}']         dZ = y_pred - y_true          for i in range(self.num_layers - 1, -1, -1):             A_prev = cache[f'A{i}']             dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]             db = np.mean(dZ, axis=0, keepdims=True)             grads_w.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = dZ @ self.weights[i].T                 dZ = dA * self.relu_derivative(cache[f'Z{i}'])          return grads_w, grads_b      # Parameter update     def update_parameters(self, grads_w, grads_b, learning_rate):         \"\"\"Apply gradient descent step.\"\"\"         for i in range(self.num_layers):             self.weights[i] -= learning_rate * grads_w[i]             self.biases[i]  -= learning_rate * grads_b[i]          # Prediction helpers     def predict_proba(self, X):         \"\"\"         Compute output probabilities for given inputs.         \"\"\"         y_pred, _ = self.forward(X)         return y_pred.reshape(-1)      def predict(self, X, threshold=0.5):         \"\"\"         Predict binary class labels (0/1).         \"\"\"         return (self.predict_proba(X) &gt;= threshold).astype(int)   print(\"MLP class implemented successfully.\") <pre>MLP class implemented successfully.\n</pre> In\u00a0[22]: Copied! <pre>def train_mlp(model, X_train, y_train, X_val, y_val,\n              epochs=50, batch_size=512, learning_rate=1e-2,\n              early_stopping=5, verbose=True):\n    \"\"\"\n    Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.\n    \"\"\"\n    # --- Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed\n    import pandas as pd\n    if isinstance(X_train, pd.DataFrame):\n        X_train = X_train.values\n    if isinstance(X_val, pd.DataFrame):\n        X_val = X_val.values\n\n    if hasattr(X_train, \"toarray\"):\n        X_train = X_train.toarray()\n    if hasattr(X_val, \"toarray\"):\n        X_val = X_val.toarray()\n\n    y_train = np.asarray(y_train).reshape(-1, 1)\n    y_val   = np.asarray(y_val).reshape(-1, 1)\n\n    n = X_train.shape[0]\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}\n    best_val_loss = np.inf\n    patience = 0\n    best_weights = None\n\n    for epoch in range(1, epochs + 1):\n        # Shuffle training data (use permutation index and apply to numpy arrays)\n        idx = np.random.permutation(n)\n        X_shuf = X_train[idx]\n        y_shuf = y_train[idx]\n\n        epoch_loss = 0.0\n\n        # Mini-batch training\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            Xb = X_shuf[start:end]\n            yb = y_shuf[start:end]\n\n            y_pred, cache = model.forward(Xb)\n            loss = model.compute_loss(yb, y_pred)\n            epoch_loss += loss * (end - start)\n\n            grads_w, grads_b = model.backward(cache, yb)\n            # model.update_parameters may expect learning_rate argument depending on your implementation:\n            # if your model.update_parameters signature accepts learning_rate, call with it; else set lr in model.\n            try:\n                model.update_parameters(grads_w, grads_b, learning_rate)\n            except TypeError:\n                # older signature without learning_rate; assume model has .lr attribute\n                model.update_parameters(grads_w, grads_b)\n\n        epoch_loss /= n\n        history['train_loss'].append(epoch_loss)\n\n        # Validation\n        y_val_pred, _ = model.forward(X_val)\n        val_loss = model.compute_loss(y_val, y_val_pred)\n        try:\n            val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())\n            val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())\n        except Exception:\n            val_auc = np.nan\n            val_ap  = np.nan\n\n        history['val_loss'].append(val_loss)\n        history['val_auc'].append(val_auc)\n        history['val_ap'].append(val_ap)\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"\n                  f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")\n\n        # Early stopping\n        if val_loss &lt; best_val_loss - 1e-6:\n            best_val_loss = val_loss\n            best_weights = ([W.copy() for W in model.weights],\n                            [b.copy() for b in model.biases])\n            patience = 0\n        else:\n            patience += 1\n        if patience &gt;= early_stopping:\n            if verbose: print(\"Early stopping triggered.\")\n            break\n\n    if best_weights is not None:\n        model.weights, model.biases = best_weights\n\n    return history\n</pre> def train_mlp(model, X_train, y_train, X_val, y_val,               epochs=50, batch_size=512, learning_rate=1e-2,               early_stopping=5, verbose=True):     \"\"\"     Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.     \"\"\"     # --- Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed     import pandas as pd     if isinstance(X_train, pd.DataFrame):         X_train = X_train.values     if isinstance(X_val, pd.DataFrame):         X_val = X_val.values      if hasattr(X_train, \"toarray\"):         X_train = X_train.toarray()     if hasattr(X_val, \"toarray\"):         X_val = X_val.toarray()      y_train = np.asarray(y_train).reshape(-1, 1)     y_val   = np.asarray(y_val).reshape(-1, 1)      n = X_train.shape[0]     history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}     best_val_loss = np.inf     patience = 0     best_weights = None      for epoch in range(1, epochs + 1):         # Shuffle training data (use permutation index and apply to numpy arrays)         idx = np.random.permutation(n)         X_shuf = X_train[idx]         y_shuf = y_train[idx]          epoch_loss = 0.0          # Mini-batch training         for start in range(0, n, batch_size):             end = min(start + batch_size, n)             Xb = X_shuf[start:end]             yb = y_shuf[start:end]              y_pred, cache = model.forward(Xb)             loss = model.compute_loss(yb, y_pred)             epoch_loss += loss * (end - start)              grads_w, grads_b = model.backward(cache, yb)             # model.update_parameters may expect learning_rate argument depending on your implementation:             # if your model.update_parameters signature accepts learning_rate, call with it; else set lr in model.             try:                 model.update_parameters(grads_w, grads_b, learning_rate)             except TypeError:                 # older signature without learning_rate; assume model has .lr attribute                 model.update_parameters(grads_w, grads_b)          epoch_loss /= n         history['train_loss'].append(epoch_loss)          # Validation         y_val_pred, _ = model.forward(X_val)         val_loss = model.compute_loss(y_val, y_val_pred)         try:             val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())             val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())         except Exception:             val_auc = np.nan             val_ap  = np.nan          history['val_loss'].append(val_loss)         history['val_auc'].append(val_auc)         history['val_ap'].append(val_ap)          if verbose:             print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"                   f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")          # Early stopping         if val_loss &lt; best_val_loss - 1e-6:             best_val_loss = val_loss             best_weights = ([W.copy() for W in model.weights],                             [b.copy() for b in model.biases])             patience = 0         else:             patience += 1         if patience &gt;= early_stopping:             if verbose: print(\"Early stopping triggered.\")             break      if best_weights is not None:         model.weights, model.biases = best_weights      return history In\u00a0[23]: Copied! <pre># Initialize and train MLP\nmlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5)\nhistory = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,\n                    epochs=100, batch_size=256, learning_rate=1e-2,\n                    early_stopping=7, verbose=False)\n</pre> # Initialize and train MLP mlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5) history = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,                     epochs=100, batch_size=256, learning_rate=1e-2,                     early_stopping=7, verbose=False) In\u00a0[24]: Copied! <pre># Plot training history\nepochs = np.arange(1, len(history['train_loss']) + 1)\n\nplt.figure(figsize=(12,4))\n\n# Loss plot\nplt.subplot(1,2,1)\nplt.plot(epochs, history['train_loss'], label='train_loss')\nplt.plot(epochs, history['val_loss'],   label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\n# Validation metrics plot (AUC and Average Precision)\nplt.subplot(1,2,2)\nif 'val_auc' in history and any(~np.isnan(history['val_auc'])):\n    plt.plot(epochs, history['val_auc'], label='val_ROC_AUC')\nif 'val_ap' in history and any(~np.isnan(history['val_ap'])):\n    plt.plot(epochs, history['val_ap'], label='val_PR_AUC')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.ylim(0.0, 1.0)\nplt.title('Validation AUC / PR-AUC vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot training history epochs = np.arange(1, len(history['train_loss']) + 1)  plt.figure(figsize=(12,4))  # Loss plot plt.subplot(1,2,1) plt.plot(epochs, history['train_loss'], label='train_loss') plt.plot(epochs, history['val_loss'],   label='val_loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Loss vs Epochs') plt.legend() plt.grid(alpha=0.2)  # Validation metrics plot (AUC and Average Precision) plt.subplot(1,2,2) if 'val_auc' in history and any(~np.isnan(history['val_auc'])):     plt.plot(epochs, history['val_auc'], label='val_ROC_AUC') if 'val_ap' in history and any(~np.isnan(history['val_ap'])):     plt.plot(epochs, history['val_ap'], label='val_PR_AUC') plt.xlabel('Epoch') plt.ylabel('Score') plt.ylim(0.0, 1.0) plt.title('Validation AUC / PR-AUC vs Epochs') plt.legend() plt.grid(alpha=0.2)  plt.tight_layout() plt.show()  <p>Analysis of Training Curves</p> <p>Both training and validation losses decrease steadily and plateau after ~80 epochs, indicating smooth convergence without instability. The small gap between the two curves suggests minimal overfitting.</p> <p>Validation ROC-AUC remains consistently high (~0.96) throughout training, while PR-AUC improves gradually before stabilizing\u2014showing the model learns to balance precision and recall effectively. Overall, the curves demonstrate a well-behaved optimization process and good generalization to unseen data.</p> In\u00a0[25]: Copied! <pre># Convert test data to dense if sparse\nX_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc\n\n# Predict probabilities and labels\ny_test_probs = mlp_model.predict_proba(X_test_eval)\ny_test_pred = (y_test_probs &gt;= 0.5).astype(int)\n\n# Metrics\nacc  = accuracy_score(y_test, y_test_pred)\nprec = precision_score(y_test, y_test_pred, zero_division=0)\nrec  = recall_score(y_test, y_test_pred, zero_division=0)\nf1   = f1_score(y_test, y_test_pred, zero_division=0)\nroc  = roc_auc_score(y_test, y_test_probs)\nap   = average_precision_score(y_test, y_test_probs)\n\nprint(\"=== Test Set Performance ===\")\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc:.4f}\")\nprint(f\"PR-AUC (AP): {ap:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})')\nplt.plot([0,1],[0,1],'--',color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n# Precision-Recall Curve\nprec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n</pre> # Convert test data to dense if sparse X_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc  # Predict probabilities and labels y_test_probs = mlp_model.predict_proba(X_test_eval) y_test_pred = (y_test_probs &gt;= 0.5).astype(int)  # Metrics acc  = accuracy_score(y_test, y_test_pred) prec = precision_score(y_test, y_test_pred, zero_division=0) rec  = recall_score(y_test, y_test_pred, zero_division=0) f1   = f1_score(y_test, y_test_pred, zero_division=0) roc  = roc_auc_score(y_test, y_test_probs) ap   = average_precision_score(y_test, y_test_probs)  print(\"=== Test Set Performance ===\") print(f\"Accuracy: {acc:.4f}\") print(f\"Precision: {prec:.4f}\") print(f\"Recall: {rec:.4f}\") print(f\"F1-score: {f1:.4f}\") print(f\"ROC-AUC: {roc:.4f}\") print(f\"PR-AUC (AP): {ap:.4f}\")  # Confusion matrix cm = confusion_matrix(y_test, y_test_pred) plt.figure(figsize=(5,4)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.title(\"Confusion Matrix (Test Set)\") plt.xlabel(\"Predicted\") plt.ylabel(\"Actual\") plt.show()  # ROC Curve fpr, tpr, _ = roc_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})') plt.plot([0,1],[0,1],'--',color='gray') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC Curve') plt.legend() plt.grid(alpha=0.2) plt.show()  # Precision-Recall Curve prec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})') plt.xlabel('Recall') plt.ylabel('Precision') plt.title('Precision-Recall Curve') plt.legend() plt.grid(alpha=0.2) plt.show() <pre>=== Test Set Performance ===\nAccuracy: 0.9292\nPrecision: 0.7276\nRecall: 0.6601\nF1-score: 0.6922\nROC-AUC: 0.9606\nPR-AUC (AP): 0.7663\n</pre> <p>Precision: 0.7276 Recall: 0.6601 F1-score: 0.6922 ROC-AUC: 0.9606 PR-AUC (AP): 0.7663</p> <p>On the test set, the MLP achieved 92.92% accuracy, 0.7276 precision, 0.6601 recall, and 0.6922 F1-score, with a strong ROC-AUC = 0.9606 and PR-AUC = 0.7663. These results confirm that the network discriminates well between classes while maintaining reasonable balance between false positives and false negatives. The high ROC-AUC indicates excellent overall separability, while the moderate PR-AUC reflects the challenge of the class imbalance. Overall, the model generalizes effectively to unseen data without significant overfitting, demonstrating a robust fit to this binary classification task.</p> In\u00a0[26]: Copied! <pre># Prepare test set for submission\n# Apply the EXACT same preprocessing as training data\ntest_work = test.copy()\n\n# Cap outliers (1st/99th percentile)\nfor col in numerical_features:\n    lower, upper = np.percentile(train[col], [1, 99])\n    test_work[col] = np.clip(test_work[col], lower, upper)\n\n# Handle pdays special case (replace -1 with 0)\nif \"pdays\" in test_work.columns:\n    test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)\n\n# Apply log transform to skewed features\ntest_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))\n\n# Apply the fitted preprocessor (StandardScaler + OneHotEncoder)\ntest_work_proc = preprocessor.transform(test_work)\n\n# Convert to dense if sparse\ntest_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc\n\n# Predict probabilities\ntest_probs = mlp_model.predict_proba(test_work_proc)\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    'id': test[id_col],\n    'y': test_probs\n})\nsubmission.to_csv(\"mlp_submission.csv\", index=False)\n</pre> # Prepare test set for submission # Apply the EXACT same preprocessing as training data test_work = test.copy()  # Cap outliers (1st/99th percentile) for col in numerical_features:     lower, upper = np.percentile(train[col], [1, 99])     test_work[col] = np.clip(test_work[col], lower, upper)  # Handle pdays special case (replace -1 with 0) if \"pdays\" in test_work.columns:     test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)  # Apply log transform to skewed features test_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))  # Apply the fitted preprocessor (StandardScaler + OneHotEncoder) test_work_proc = preprocessor.transform(test_work)  # Convert to dense if sparse test_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc  # Predict probabilities test_probs = mlp_model.predict_proba(test_work_proc)  # Prepare submission submission = pd.DataFrame({     'id': test[id_col],     'y': test_probs }) submission.to_csv(\"mlp_submission.csv\", index=False) In\u00a0[29]: Copied! <pre># Submitting to Kaggle\n\n!kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\"\n</pre> # Submitting to Kaggle  !kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\" <pre>'kaggle' n\ufffdo \ufffd reconhecido como um comando interno\nou externo, um programa oper\ufffdvel ou um arquivo em lotes.\n</pre> <p>Kaggle score: 0.96158 (public leaderboard)</p> <ul> <li>Leaderboard position: Within the top 68% of all public submissions.</li> <li>Reason for not reaching top 50%:<ul> <li>The MLP was implemented from scratch using only NumPy, without high-level optimization libraries like PyTorch or TensorFlow.</li> <li>State-of-the-art Kaggle submissions typically rely on ensemble methods (e.g., LightGBM, CatBoost) and extensive feature engineering, giving them a natural advantage on tabular datasets.</li> </ul> </li> </ul> <p></p>"},{"location":"Projetos/Projeto1/notebook/#multi-layer-perceptron-mlp-classification","title":"Multi-Layer Perceptron (MLP) Classification\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#project-overview","title":"Project Overview\u00b6","text":"<p>This notebook implements a Multi-Layer Perceptron (MLP) neural network from scratch for binary classification on a real-world banking dataset.</p> <p>Authors: Rodrigo Medeiros, Matheus Castellucci, Jo\u00e3o Pedro Rodrigues</p>"},{"location":"Projetos/Projeto1/notebook/#1-dataset-selection","title":"1. Dataset Selection\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#dataset-information","title":"Dataset Information\u00b6","text":"<p>Name: Binary Classification with a Bank Dataset Source: Kaggle Playground Series S5E8 Original Dataset: Bank Marketing Dataset</p> <p>Size:</p> <ul> <li>Training set: 750,000 rows \u00d7 18 columns</li> <li>Test set: 250,000 rows \u00d7 17 columns</li> <li>Features: 16 (7 numerical + 9 categorical)</li> <li>Target: Binary (subscription to term deposit: yes/no)</li> </ul> <p>Why this dataset?</p> <ul> <li>Real-world banking application (predicting term deposit subscriptions)</li> <li>Sufficient complexity with mixed feature types</li> <li>Class imbalance - realistic scenario</li> <li>Relevant to marketing and customer behavior prediction</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#2-dataset-explanation","title":"2. Dataset Explanation\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Numerical Features (7):</p> <ul> <li><code>age</code>: Client's age</li> <li><code>balance</code>: Average yearly balance (euros)</li> <li><code>day</code>: Last contact day of month</li> <li><code>duration</code>: Last contact duration (seconds)</li> <li><code>campaign</code>: Number of contacts during this campaign</li> <li><code>pdays</code>: Days since last contact from previous campaign (-1 = not contacted)</li> <li><code>previous</code>: Number of contacts before this campaign</li> </ul> <p>Categorical Features (9):</p> <ul> <li><code>job</code>: Type of job</li> <li><code>marital</code>: Marital status</li> <li><code>education</code>: Education level</li> <li><code>default</code>: Has credit in default?</li> <li><code>housing</code>: Has housing loan?</li> <li><code>loan</code>: Has personal loan?</li> <li><code>contact</code>: Contact communication type</li> <li><code>month</code>: Last contact month</li> <li><code>poutcome</code>: Outcome of previous campaign</li> </ul> <p>Target Variable:</p> <ul> <li><code>y</code>: Subscribed to term deposit? (0 = no, 1 = yes)</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#target-variable-distribution","title":"Target Variable Distribution\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#potential-issues-identified","title":"Potential Issues Identified\u00b6","text":"<ul> <li><p>Class Imbalance: ~88% negative class, ~12% positive class</p> </li> <li><p>Outliers: Several numerical features have extreme values (detected via histograms)</p> </li> <li><p>Skewed Distributions: Most numerical features are right-skewed</p> </li> <li><p>Mixed Feature Types: Requires encoding for categorical variables</p> </li> <li><p>Missing Values: No missing values detected</p> </li> <li><p>Duplicates: No duplicate rows detected</p> </li> </ul>"},{"location":"Projetos/Projeto1/notebook/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#preprocessing-justification","title":"Preprocessing Justification\u00b6","text":"<ul> <li><p>Outlier Capping (1st\u201399th Percentile): Limits the influence of extreme values while preserving all rows, avoiding bias and data loss.</p> </li> <li><p>Log Transform (|skew| \u2265 1): Reduces heavy right skew in numerical features for more stable gradient updates.</p> </li> <li><p>StandardScaler: Centers and scales numeric features to improve MLP convergence.</p> </li> <li><p>OneHotEncoder: Encodes categorical variables as binary vectors; drop='first' prevents redundancy and handle_unknown='ignore' ensures consistency on new data.</p> </li> <li><p>Sparse Output: Saves memory with large one-hot matrices, converted to dense only if needed for training.</p> </li> <li><p>Stratified 70/15/15 Split: Preserves class proportions across train, validation, and test sets for unbiased evaluation.</p> </li> <li><p>Class Weights: Counteracts class imbalance (~12% positive) by adjusting loss contributions between classes.</p> </li> </ul>"},{"location":"Projetos/Projeto1/notebook/#4-mlp-implementation","title":"4. MLP Implementation\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#hyperparameters","title":"Hyperparameters\u00b6","text":"<ul> <li><p>Input Layer: matches the number of preprocessed features.</p> </li> <li><p>Hidden Layers: three fully connected layers with 128 and 64 neurons, each using ReLU activation to introduce non-linearity and mitigate vanishing gradients.</p> </li> <li><p>Output Layer: a single neuron with sigmoid activation to output probabilities for the binary target.</p> </li> <li><p>Initialization: weights are initialized with He initialization (N(0, sqrt(2/n_in))) to maintain stable activation variance across layers.</p> </li> <li><p>Loss Function: Binary Cross-Entropy (BCE) with optional L2 regularization, penalizing large weights to improve generalization.</p> </li> <li><p>Optimizer: standard Stochastic Gradient Descent (SGD) is used for parameter updates; gradient descent steps are handled in the training loop.</p> </li> </ul>"},{"location":"Projetos/Projeto1/notebook/#5-model-training","title":"5. Model Training\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#training-procedure","title":"Training Procedure\u00b6","text":"<p>The MLP was trained using mini-batch Stochastic Gradient Descent (SGD) implemented from scratch. Each epoch consists of the following steps:</p> <ol> <li><p>Data Shuffling and Mini-Batches \u2014 At the start of every epoch, the training data are randomly shuffled and divided into batches to improve gradient estimation and generalization.</p> </li> <li><p>Forward Propagation \u2014 For each batch, the model performs matrix multiplications and ReLU activations across layers to compute predicted probabilities.</p> </li> <li><p>Loss Computation \u2014 The Binary Cross-Entropy (BCE) loss is computed between predictions and true labels, with an additional L2 penalty on the weights to discourage overfitting.</p> </li> <li><p>Backpropagation \u2014 Gradients of the BCE + L2 loss with respect to every weight and bias are obtained via the chain rule.</p> <ul> <li>The ReLU derivative (<code>1 if z &gt; 0 else 0</code>) prevents vanishing gradients common in sigmoid/tanh activations.</li> <li>Intermediate activations are stored in a <code>cache</code> dictionary for reuse during gradient computation.</li> </ul> </li> <li><p>Parameter Update \u2014 Each layer\u2019s weights and biases are updated by</p> <p>$ W \\leftarrow W - \\eta\\,\\nabla_W L,\\qquad b \\leftarrow b - \\eta\\,\\nabla_b L $</p> <p>where the learning rate controls the step size.</p> </li> <li><p>Validation Evaluation \u2014 After every epoch, the model runs a forward pass on the validation set to track loss, ROC-AUC, and PR-AUC.</p> </li> <li><p>Early Stopping \u2014 If the validation loss fails to improve for a number of epochs, training stops and the best weights (lowest validation loss) are restored.</p> </li> </ol>"},{"location":"Projetos/Projeto1/notebook/#training-challenges-and-solutions","title":"Training Challenges and Solutions\u00b6","text":"<ul> <li>Vanishing Gradients: Addressed by using ReLU activations and He initialization, which preserve gradient scale across layers.</li> <li>Overfitting: Controlled with L2 regularization and early stopping; validation loss and AUC were monitored each epoch.</li> <li>Instability in Loss: Occasional fluctuations caused by mini-batch noise; mitigated by averaging losses over all batches per epoch.</li> <li>Imbalanced Classes: Since positives represent only \u2248 12 % of the data, performance was evaluated primarily with ROC-AUC and PR-AUC rather than accuracy.</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy\u00b6","text":"<p>The dataset was already divided into train / validation / test sets using a 70 / 15 / 15 split. All splits were stratified to preserve the original class distribution (~12 % positive class).</p>"},{"location":"Projetos/Projeto1/notebook/#validation-role-in-hyperparameter-tuning","title":"Validation role in hyperparameter tuning\u00b6","text":"<ul> <li>The validation set is crucial for evaluating alternative model configurations such as learning rate, hidden layer size, L2 regularization strength, and batch size.</li> <li>The main selection metric is validation ROC-AUC (<code>val_auc</code>), since accuracy is unreliable under class imbalance. PR-AUC serves as a secondary indicator of precision\u2013recall trade-offs.</li> <li>Typical tuning workflow:<ol> <li>Train each candidate model on the training set only.</li> <li>Evaluate on the validation set after every epoch to track learning progress.</li> <li>Select the configuration that yields the highest validation ROC-AUC.</li> <li>Retrain the best model on the combined training + validation data before the final test evaluation.</li> </ol> </li> <li>Early stopping monitors the same validation metric (<code>val_auc</code>) and halts training when no improvement is observed for a fixed number of epochs (patience = 7), avoiding unnecessary computation and overfitting.</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#8-evaluation-metrics","title":"8. Evaluation Metrics\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#kaggle-submission","title":"Kaggle Submission\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#conclusion-and-references","title":"Conclusion and References\u00b6","text":""},{"location":"Projetos/Projeto1/notebook/#overall-findings","title":"Overall Findings\u00b6","text":"<p>This project successfully implemented a Multi-Layer Perceptron (MLP) from scratch using NumPy for binary classification on a real-world bank marketing dataset from Kaggle. The model achieved strong generalization with a ROC-AUC of 0.96 and a PR-AUC of 0.77 on the test set \u2014 values that demonstrate reliable separation of the two classes despite significant class imbalance (\u224812% positives).</p> <p>The MLP\u2019s final configuration \u2014 <code>[128, 64, 32]</code> hidden layers, ReLU activations, He initialization, L2 regularization, and early stopping \u2014 provided an effective balance between learning capacity and overfitting control. Training was stable, and validation curves showed smooth convergence without divergence or oscillation. Using validation ROC-AUC as the monitored metric ensured that model selection aligned with the project\u2019s primary goal: robust ranking performance under imbalance.</p>"},{"location":"Projetos/Projeto1/notebook/#limitations","title":"Limitations\u00b6","text":"<ul> <li>Model simplicity: The MLP, while effective, lacks the representational efficiency of more advanced architectures like gradient-boosted trees (XGBoost/LightGBM) or deep ensembles commonly used for tabular data.</li> <li>Manual optimization: Without adaptive optimizers (e.g., Adam) or learning-rate schedules beyond simple decay, convergence speed and optimality might be limited.</li> <li>Feature interactions: The MLP relied solely on basic preprocessing; engineered interactions or embeddings could further enhance predictive performance.</li> <li>Computation time: Implementing backpropagation purely in NumPy is slower than using vectorized deep learning frameworks (e.g., PyTorch, TensorFlow).</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#references","title":"References\u00b6","text":"<ul> <li>Kaggle: Playground Series S5E8 \u2014 Bank Marketing Dataset</li> <li>UCI Machine Learning Repository: Bank Marketing Data Set</li> </ul>"},{"location":"Projetos/Projeto1/notebook/#ai-usage","title":"AI Usage\u00b6","text":"<p>AI was used to help organize the notebook and debug code. Everything was verified and corrected by the authors.</p>"},{"location":"Projetos/Projeto2/notebook/","title":"Multi-Layer Perceptron (MLP) Regression","text":"In\u00a0[3]: Copied! <pre># Basic imports &amp; reproducibility\nimport os\nimport random\nfrom pathlib import Path\nimport warnings\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport copy\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n# stats / util\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# preprocessing &amp; modeling\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n# metrics for regression\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n)\n\n# notebook display\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\nwarnings.filterwarnings('ignore')\n\n# plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\n# seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n</pre> # Basic imports &amp; reproducibility import os import random from pathlib import Path import warnings  import math import numpy as np import pandas as pd import itertools import copy  # plotting import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import MaxNLocator  # stats / util from scipy import stats from tqdm.auto import tqdm  # preprocessing &amp; modeling from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split  # metrics for regression from sklearn.metrics import (     mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error )  # notebook display %matplotlib inline pd.set_option('display.max_columns', None) pd.set_option('display.width', 200) warnings.filterwarnings('ignore')  # plotting style plt.style.use('seaborn-v0_8-whitegrid') sns.set_context('notebook')  # seeds for reproducibility SEED = 42 random.seed(SEED) np.random.seed(SEED) os.environ['PYTHONHASHSEED'] = str(SEED) In\u00a0[4]: Copied! <pre># Load dataset\nDATA_DIR = Path(\"data\")\ntrain = pd.read_csv(DATA_DIR / \"train.csv\")\ntest = pd.read_csv(DATA_DIR / \"test.csv\")\n\nprint(\"Dataset Shape:\")\nprint(f\"  Training: {train.shape}\")\nprint(f\"  Test: {test.shape}\")\n\nprint(\"\\nFirst 5 rows:\")\ndisplay(train.head())\n\nprint(\"\\nData Types:\")\nprint(train.dtypes)\n</pre> # Load dataset DATA_DIR = Path(\"data\") train = pd.read_csv(DATA_DIR / \"train.csv\") test = pd.read_csv(DATA_DIR / \"test.csv\")  print(\"Dataset Shape:\") print(f\"  Training: {train.shape}\") print(f\"  Test: {test.shape}\")  print(\"\\nFirst 5 rows:\") display(train.head())  print(\"\\nData Types:\") print(train.dtypes) <pre>Dataset Shape:\n  Training: (750000, 9)\n  Test: (250000, 8)\n\nFirst 5 rows:\n</pre> id Sex Age Height Weight Duration Heart_Rate Body_Temp Calories 0 0 male 36 189.0 82.0 26.0 101.0 41.0 150.0 1 1 female 64 163.0 60.0 8.0 85.0 39.7 34.0 2 2 female 51 161.0 64.0 7.0 84.0 39.8 29.0 3 3 male 20 192.0 90.0 25.0 105.0 40.7 140.0 4 4 female 38 166.0 61.0 25.0 102.0 40.6 146.0 <pre>\nData Types:\nid              int64\nSex            object\nAge             int64\nHeight        float64\nWeight        float64\nDuration      float64\nHeart_Rate    float64\nBody_Temp     float64\nCalories      float64\ndtype: object\n</pre> In\u00a0[5]: Copied! <pre># Identify feature types\ntarget_col = 'Calories'\nid_col = 'id'\n\nnumerical_features = train.select_dtypes(include=['number']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n</pre> # Identify feature types target_col = 'Calories' id_col = 'id'  numerical_features = train.select_dtypes(include=['number']).columns.tolist() numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]  categorical_features = train.select_dtypes(include=['object']).columns.tolist()  print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\") print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\") <pre>Numerical features (6): ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n\nCategorical features (1): ['Sex']\n</pre> In\u00a0[6]: Copied! <pre># Summary statistics\ndisplay(train[numerical_features].describe())\n\n# Check for missing values\nprint(\"Missing Values:\")\nprint(train.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n</pre> # Summary statistics display(train[numerical_features].describe())  # Check for missing values print(\"Missing Values:\") print(train.isnull().sum())  # Check for duplicates print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\") Age Height Weight Duration Heart_Rate Body_Temp count 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 mean 41.420404 174.697685 75.145668 15.421015 95.483995 40.036253 std 15.175049 12.824496 13.982704 8.354095 9.449845 0.779875 min 20.000000 126.000000 36.000000 1.000000 67.000000 37.100000 25% 28.000000 164.000000 63.000000 8.000000 88.000000 39.600000 50% 40.000000 174.000000 74.000000 15.000000 95.000000 40.300000 75% 52.000000 185.000000 87.000000 23.000000 103.000000 40.700000 max 79.000000 222.000000 132.000000 30.000000 128.000000 41.500000 <pre>Missing Values:\nid            0\nSex           0\nAge           0\nHeight        0\nWeight        0\nDuration      0\nHeart_Rate    0\nBody_Temp     0\nCalories      0\ndtype: int64\n\nDuplicate Rows: 0\n</pre> In\u00a0[7]: Copied! <pre># Check for outliers (beyond 3 standard deviations)\noutlier_summary = {}\nfor col in numerical_features:\n    z = np.abs(stats.zscore(train[col]))\n    outlier_summary[col] = (z &gt; 3).mean() * 100\n\noutlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers'])\ndisplay(outlier_df.sort_values('% Outliers', ascending=False))\n</pre> # Check for outliers (beyond 3 standard deviations) outlier_summary = {} for col in numerical_features:     z = np.abs(stats.zscore(train[col]))     outlier_summary[col] = (z &gt; 3).mean() * 100  outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers']) display(outlier_df.sort_values('% Outliers', ascending=False)) % Outliers Body_Temp 0.449067 Heart_Rate 0.023333 Weight 0.013867 Height 0.005067 Age 0.000000 Duration 0.000000 In\u00a0[8]: Copied! <pre>fig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_features):\n    data = train[col].dropna()\n    # histogram with density overlay (optional)\n    axes[i].hist(data, bins=40, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)\n    axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)\n    axes[i].grid(alpha=0.3)\n    if data.max() &gt; 5000:\n        axes[i].set_xscale(\"log\")\n    axes[i].set_xlabel(\"\")\n    axes[i].set_ylabel(\"\")\n\n# hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(3, 2, figsize=(15, 12)) axes = axes.flatten()  for i, col in enumerate(numerical_features):     data = train[col].dropna()     # histogram with density overlay (optional)     axes[i].hist(data, bins=40, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)     axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)     axes[i].grid(alpha=0.3)     if data.max() &gt; 5000:         axes[i].set_xscale(\"log\")     axes[i].set_xlabel(\"\")     axes[i].set_ylabel(\"\")  # hide any unused subplots for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[9]: Copied! <pre># Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation = train[numerical_features + [target_col]].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\nplt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost correlated features with target:\")\nprint(correlation[target_col].sort_values(ascending=False))\n</pre> # Correlation matrix plt.figure(figsize=(10, 8)) correlation = train[numerical_features + [target_col]].corr() sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True) plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()  print(\"\\nMost correlated features with target:\") print(correlation[target_col].sort_values(ascending=False)) <pre>\nMost correlated features with target:\nCalories      1.000000\nDuration      0.959908\nHeart_Rate    0.908748\nBody_Temp     0.828671\nAge           0.145683\nWeight        0.015863\nHeight       -0.004026\nName: Calories, dtype: float64\n</pre> In\u00a0[10]: Copied! <pre># Sex distribution just print as percentages\nprint(\"\\nSex Distribution:\")\nprint(train['Sex'].value_counts(normalize=True) * 100)\n</pre> # Sex distribution just print as percentages print(\"\\nSex Distribution:\") print(train['Sex'].value_counts(normalize=True) * 100) <pre>\nSex Distribution:\nSex\nfemale    50.096133\nmale      49.903867\nName: proportion, dtype: float64\n</pre> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(5,4))\nplt.hist(train[target_col], bins=30, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)\nplt.title(f\"{target_col} Distribution\", fontsize=14, fontweight=\"bold\")\nplt.xlabel(target_col)\nplt.ylabel(\"Frequency\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(5,4)) plt.hist(train[target_col], bins=30, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7) plt.title(f\"{target_col} Distribution\", fontsize=14, fontweight=\"bold\") plt.xlabel(target_col) plt.ylabel(\"Frequency\") plt.grid(alpha=0.3) plt.tight_layout() plt.show() <p>There are no missing values and no duplicates in the dataset. Most features are close to normal (low skewness). No transformations are applied.</p> In\u00a0[12]: Copied! <pre>import numpy as np\n\n# --- Feature Engineering ---\n# Keep ALL original features and add only GOOD engineered features\ntrain_fe = train.copy()\n\n# BMI: kg/m^2 - body composition indicator\ntrain_fe[\"BMI\"] = train_fe[\"Weight\"] / (train_fe[\"Height\"] / 100) ** 2\n\n# Workload index: combination of duration and heart rate (0.977 correlation with target)\ntrain_fe[\"Workload_Index\"] = train_fe[\"Duration\"] * train_fe[\"Heart_Rate\"]\n\n# Temperature difference from normal body temp (37\u00b0C)\ntrain_fe[\"Temp_Diff\"] = train_fe[\"Body_Temp\"] - 37.0\n\n# One-hot encode Sex (keep both columns for more information)\ntrain_fe = pd.get_dummies(train_fe, columns=[\"Sex\"], drop_first=False)\n\nprint(f\"After feature engineering: {train_fe.shape}\")\nprint(f\"Features: {train_fe.columns.tolist()}\")\ntrain_fe.head()\n</pre> import numpy as np  # --- Feature Engineering --- # Keep ALL original features and add only GOOD engineered features train_fe = train.copy()  # BMI: kg/m^2 - body composition indicator train_fe[\"BMI\"] = train_fe[\"Weight\"] / (train_fe[\"Height\"] / 100) ** 2  # Workload index: combination of duration and heart rate (0.977 correlation with target) train_fe[\"Workload_Index\"] = train_fe[\"Duration\"] * train_fe[\"Heart_Rate\"]  # Temperature difference from normal body temp (37\u00b0C) train_fe[\"Temp_Diff\"] = train_fe[\"Body_Temp\"] - 37.0  # One-hot encode Sex (keep both columns for more information) train_fe = pd.get_dummies(train_fe, columns=[\"Sex\"], drop_first=False)  print(f\"After feature engineering: {train_fe.shape}\") print(f\"Features: {train_fe.columns.tolist()}\") train_fe.head() <pre>After feature engineering: (750000, 13)\nFeatures: ['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Calories', 'BMI', 'Workload_Index', 'Temp_Diff', 'Sex_female', 'Sex_male']\n</pre> Out[12]: id Age Height Weight Duration Heart_Rate Body_Temp Calories BMI Workload_Index Temp_Diff Sex_female Sex_male 0 0 36 189.0 82.0 26.0 101.0 41.0 150.0 22.955684 2626.0 4.0 False True 1 1 64 163.0 60.0 8.0 85.0 39.7 34.0 22.582709 680.0 2.7 True False 2 2 51 161.0 64.0 7.0 84.0 39.8 29.0 24.690405 588.0 2.8 True False 3 3 20 192.0 90.0 25.0 105.0 40.7 140.0 24.414062 2625.0 3.7 False True 4 4 38 166.0 61.0 25.0 102.0 40.6 146.0 22.136740 2550.0 3.6 True False In\u00a0[13]: Copied! <pre># Check for invalid values\nprint(\"Checking for NaN values:\")\nprint(train_fe.isna().sum().sort_values(ascending=False).head(10))\n\nprint(\"\\nNew engineered features statistics:\")\nprint(train_fe[[\"BMI\",\"Temp_Diff\",\"Workload_Index\"]].describe())\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.histplot(train_fe[\"BMI\"], bins=40, kde=True, ax=axes[0])\nsns.histplot(train_fe[\"Temp_Diff\"], bins=40, kde=True, ax=axes[1])\nsns.histplot(train_fe[\"Workload_Index\"], bins=40, kde=True, ax=axes[2])\nplt.tight_layout()\nplt.show()\n</pre> # Check for invalid values print(\"Checking for NaN values:\") print(train_fe.isna().sum().sort_values(ascending=False).head(10))  print(\"\\nNew engineered features statistics:\") print(train_fe[[\"BMI\",\"Temp_Diff\",\"Workload_Index\"]].describe())  fig, axes = plt.subplots(1, 3, figsize=(15, 4)) sns.histplot(train_fe[\"BMI\"], bins=40, kde=True, ax=axes[0]) sns.histplot(train_fe[\"Temp_Diff\"], bins=40, kde=True, ax=axes[1]) sns.histplot(train_fe[\"Workload_Index\"], bins=40, kde=True, ax=axes[2]) plt.tight_layout() plt.show() <pre>Checking for NaN values:\nid                0\nAge               0\nHeight            0\nWeight            0\nDuration          0\nHeart_Rate        0\nBody_Temp         0\nCalories          0\nBMI               0\nWorkload_Index    0\ndtype: int64\n\nNew engineered features statistics:\n                 BMI      Temp_Diff  Workload_Index\ncount  750000.000000  750000.000000   750000.000000\nmean       24.374817       3.036253     1541.562606\nstd         1.511310       0.779875      932.453480\nmin        12.375937       0.100000       67.000000\n25%        23.255019       2.600000      728.000000\n50%        24.391059       3.300000     1455.000000\n75%        25.487697       3.700000     2323.000000\nmax        46.443986       4.500000     3840.000000\n</pre> In\u00a0[14]: Copied! <pre>corr = train_fe.corr(numeric_only=True)[\"Calories\"].sort_values(ascending=False)\nprint(\"Feature correlations with Calories:\")\nprint(corr)\n\n# Visualize top correlations\nplt.figure(figsize=(10, 6))\ncorr.drop('Calories').plot(kind='barh', color='#a149d1')\nplt.xlabel('Correlation with Calories')\nplt.title('Feature Correlations with Target Variable')\nplt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\nplt.tight_layout()\nplt.show()\n</pre> corr = train_fe.corr(numeric_only=True)[\"Calories\"].sort_values(ascending=False) print(\"Feature correlations with Calories:\") print(corr)  # Visualize top correlations plt.figure(figsize=(10, 6)) corr.drop('Calories').plot(kind='barh', color='#a149d1') plt.xlabel('Correlation with Calories') plt.title('Feature Correlations with Target Variable') plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8) plt.tight_layout() plt.show() <pre>Feature correlations with Calories:\nCalories          1.000000\nWorkload_Index    0.977341\nDuration          0.959908\nHeart_Rate        0.908748\nBody_Temp         0.828671\nTemp_Diff         0.828671\nAge               0.145683\nBMI               0.049226\nWeight            0.015863\nSex_male          0.012011\nid                0.001148\nHeight           -0.004026\nSex_female       -0.012011\nName: Calories, dtype: float64\n</pre> In\u00a0[15]: Copied! <pre># Keep ALL features - don't remove any!\n# Only drop the 'id' column as it's not predictive\ntrain_work = train_fe.copy()\ntrain_work.drop(columns=['id'], inplace=True)\n\nprint(f\"Final feature set shape: {train_work.shape}\")\nprint(f\"Features retained: {train_work.columns.tolist()}\")\ntrain_work.head()\n</pre> # Keep ALL features - don't remove any! # Only drop the 'id' column as it's not predictive train_work = train_fe.copy() train_work.drop(columns=['id'], inplace=True)  print(f\"Final feature set shape: {train_work.shape}\") print(f\"Features retained: {train_work.columns.tolist()}\") train_work.head() <pre>Final feature set shape: (750000, 12)\nFeatures retained: ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Calories', 'BMI', 'Workload_Index', 'Temp_Diff', 'Sex_female', 'Sex_male']\n</pre> Out[15]: Age Height Weight Duration Heart_Rate Body_Temp Calories BMI Workload_Index Temp_Diff Sex_female Sex_male 0 36 189.0 82.0 26.0 101.0 41.0 150.0 22.955684 2626.0 4.0 False True 1 64 163.0 60.0 8.0 85.0 39.7 34.0 22.582709 680.0 2.7 True False 2 51 161.0 64.0 7.0 84.0 39.8 29.0 24.690405 588.0 2.8 True False 3 20 192.0 90.0 25.0 105.0 40.7 140.0 24.414062 2625.0 3.7 False True 4 38 166.0 61.0 25.0 102.0 40.6 146.0 22.136740 2550.0 3.6 True False In\u00a0[16]: Copied! <pre># No outlier clipping - keep all data as is\n# The model can learn from the full distribution\nprint(f\"Training data shape (no outlier clipping): {train_work.shape}\")\n</pre> # No outlier clipping - keep all data as is # The model can learn from the full distribution print(f\"Training data shape (no outlier clipping): {train_work.shape}\") <pre>Training data shape (no outlier clipping): (750000, 12)\n</pre> In\u00a0[17]: Copied! <pre># Define features and target\nX = train_work.drop(columns=[target_col])\ny = train_work[target_col].values\n\n# Normalize the target variable for stable training\nfrom sklearn.preprocessing import StandardScaler\ny_scaler = StandardScaler()\ny_normalized = y_scaler.fit_transform(y.reshape(-1, 1)).ravel()\n\nprint(f\"Target statistics before normalization:\")\nprint(f\"  Mean: {y.mean():.2f}, Std: {y.std():.2f}, Min: {y.min():.2f}, Max: {y.max():.2f}\")\nprint(f\"\\nTarget statistics after normalization:\")\nprint(f\"  Mean: {y_normalized.mean():.4f}, Std: {y_normalized.std():.4f}, Min: {y_normalized.min():.2f}, Max: {y_normalized.max():.2f}\")\n</pre> # Define features and target X = train_work.drop(columns=[target_col]) y = train_work[target_col].values  # Normalize the target variable for stable training from sklearn.preprocessing import StandardScaler y_scaler = StandardScaler() y_normalized = y_scaler.fit_transform(y.reshape(-1, 1)).ravel()  print(f\"Target statistics before normalization:\") print(f\"  Mean: {y.mean():.2f}, Std: {y.std():.2f}, Min: {y.min():.2f}, Max: {y.max():.2f}\") print(f\"\\nTarget statistics after normalization:\") print(f\"  Mean: {y_normalized.mean():.4f}, Std: {y_normalized.std():.4f}, Min: {y_normalized.min():.2f}, Max: {y_normalized.max():.2f}\") <pre>Target statistics before normalization:\n  Mean: 88.28, Std: 62.40, Min: 1.00, Max: 314.00\n\nTarget statistics after normalization:\n  Mean: 0.0000, Std: 1.0000, Min: -1.40, Max: 3.62\n</pre> In\u00a0[18]: Copied! <pre># Use 95% for training, 5% for validation (to maximize training data)\n# No separate test set - we'll use Kaggle leaderboard for testing\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y_normalized, test_size=0.05, random_state=SEED\n)\n\nprint(\"Final splits (95% train / 5% val):\")\nprint(f\" Train: {X_train.shape}\")\nprint(f\" Val: {X_val.shape}\")\n</pre> # Use 95% for training, 5% for validation (to maximize training data) # No separate test set - we'll use Kaggle leaderboard for testing X_train, X_val, y_train, y_val = train_test_split(     X, y_normalized, test_size=0.05, random_state=SEED )  print(\"Final splits (95% train / 5% val):\") print(f\" Train: {X_train.shape}\") print(f\" Val: {X_val.shape}\") <pre>Final splits (95% train / 5% val):\n Train: (712500, 11)\n Val: (37500, 11)\n</pre> In\u00a0[19]: Copied! <pre>categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\nnumerical_features = X.select_dtypes(include=['number']).columns.tolist()\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numerical_features),\n        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"), categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\n# Fit only on training data\npreprocessor.fit(X_train)\n\n# Transform splits\nX_train_proc = preprocessor.transform(X_train)\nX_val_proc   = preprocessor.transform(X_val)\n\nprint(\"\\nProcessed feature dimensions:\")\nprint(f\" Train: {X_train_proc.shape}\")\nprint(f\" Val:   {X_val_proc.shape}\")\nprint(f\" Number of features: {X_train_proc.shape[1]}\")\n</pre> categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist() numerical_features = X.select_dtypes(include=['number']).columns.tolist()  # Preprocessing pipeline preprocessor = ColumnTransformer(     transformers=[         (\"num\", StandardScaler(), numerical_features),         (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"), categorical_features),     ],     remainder=\"drop\", )  # Fit only on training data preprocessor.fit(X_train)  # Transform splits X_train_proc = preprocessor.transform(X_train) X_val_proc   = preprocessor.transform(X_val)  print(\"\\nProcessed feature dimensions:\") print(f\" Train: {X_train_proc.shape}\") print(f\" Val:   {X_val_proc.shape}\") print(f\" Number of features: {X_train_proc.shape[1]}\") <pre>\nProcessed feature dimensions:\n Train: (712500, 11)\n Val:   (37500, 11)\n Number of features: 11\n</pre> In\u00a0[20]: Copied! <pre>class MLP:\n    \"\"\"\n    Multi-Layer Perceptron (NumPy) for regression.\n    Architecture: Input \u2192 Hidden Layer(s) \u2192 Output\n    Activation: ReLU (hidden), Linear (output)\n    Loss: Mean Squared Error (MSE) with optional L2 regularization\n    Optimizer: SGD (parameter updates handled externally)\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):\n        np.random.seed(random_state)\n        self.l2_lambda = l2_lambda\n        \n        # Define layer sizes: input, hidden(s), output (1 for regression)\n        layer_sizes = [input_dim] + hidden_sizes + [1]\n        self.num_layers = len(layer_sizes) - 1\n        \n        # Initialize parameters with He initialization (good for ReLU)\n        self.weights = []\n        self.biases = []\n        for i in range(self.num_layers):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            std = np.sqrt(2.0 / n_in)\n            self.weights.append(np.random.randn(n_in, n_out) * std)\n            self.biases.append(np.zeros((1, n_out)))\n\n    # Activation functions\n    def relu(self, x):\n        \"\"\"ReLU activation\"\"\"\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (x &gt; 0).astype(float)\n\n    # Forward propagation\n    def forward(self, X):\n        \"\"\"\n        Forward pass through all layers.\n        Returns:\n            y_pred: predicted continuous values\n            cache: activations for backpropagation\n        \"\"\"\n        cache = {'A0': X}\n        A = X\n        for i in range(self.num_layers - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            cache[f'Z{i+1}'] = Z\n            cache[f'A{i+1}'] = A\n        # Linear output for regression (no activation)\n        Z_out = A @ self.weights[-1] + self.biases[-1]\n        A_out = Z_out  # Linear output\n        cache[f'Z{self.num_layers}'] = Z_out\n        cache[f'A{self.num_layers}'] = A_out\n        return A_out, cache\n\n    # Loss computation\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Mean Squared Error (MSE) + optional L2 penalty.\n        \"\"\"\n        m = y_true.shape[0]\n        y_true = y_true.reshape(-1, 1)\n        mse = np.mean((y_pred - y_true) ** 2)\n        l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))\n        return mse + l2\n\n    # Backpropagation\n    def backward(self, cache, y_true):\n        \"\"\"\n        Compute gradients for all parameters using backpropagation.\n        \"\"\"\n        y_true = y_true.reshape(-1, 1)\n        m = y_true.shape[0]\n        grads_w, grads_b = [], []\n\n        # Gradient for output layer (MSE derivative)\n        y_pred = cache[f'A{self.num_layers}']\n        dZ = (2 / m) * (y_pred - y_true)  # MSE gradient\n\n        for i in range(self.num_layers - 1, -1, -1):\n            A_prev = cache[f'A{i}']\n            dW = (A_prev.T @ dZ) + (self.l2_lambda / m) * self.weights[i]\n            db = np.sum(dZ, axis=0, keepdims=True)\n            grads_w.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = dZ @ self.weights[i].T\n                dZ = dA * self.relu_derivative(cache[f'Z{i}'])\n\n        return grads_w, grads_b\n\n    # Parameter update\n    def update_parameters(self, grads_w, grads_b, learning_rate):\n        \"\"\"Apply gradient descent step.\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= learning_rate * grads_w[i]\n            self.biases[i]  -= learning_rate * grads_b[i]\n\n    # Prediction helpers\n    def predict(self, X):\n        \"\"\"\n        Compute predicted continuous values for given inputs.\n        \"\"\"\n        y_pred, _ = self.forward(X)\n        return y_pred.reshape(-1)\n\n\nprint(\"MLP class implemented successfully.\")\n</pre> class MLP:     \"\"\"     Multi-Layer Perceptron (NumPy) for regression.     Architecture: Input \u2192 Hidden Layer(s) \u2192 Output     Activation: ReLU (hidden), Linear (output)     Loss: Mean Squared Error (MSE) with optional L2 regularization     Optimizer: SGD (parameter updates handled externally)     \"\"\"          def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):         np.random.seed(random_state)         self.l2_lambda = l2_lambda                  # Define layer sizes: input, hidden(s), output (1 for regression)         layer_sizes = [input_dim] + hidden_sizes + [1]         self.num_layers = len(layer_sizes) - 1                  # Initialize parameters with He initialization (good for ReLU)         self.weights = []         self.biases = []         for i in range(self.num_layers):             n_in, n_out = layer_sizes[i], layer_sizes[i+1]             std = np.sqrt(2.0 / n_in)             self.weights.append(np.random.randn(n_in, n_out) * std)             self.biases.append(np.zeros((1, n_out)))      # Activation functions     def relu(self, x):         \"\"\"ReLU activation\"\"\"         return np.maximum(0, x)      def relu_derivative(self, x):         \"\"\"Derivative of ReLU\"\"\"         return (x &gt; 0).astype(float)      # Forward propagation     def forward(self, X):         \"\"\"         Forward pass through all layers.         Returns:             y_pred: predicted continuous values             cache: activations for backpropagation         \"\"\"         cache = {'A0': X}         A = X         for i in range(self.num_layers - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             cache[f'Z{i+1}'] = Z             cache[f'A{i+1}'] = A         # Linear output for regression (no activation)         Z_out = A @ self.weights[-1] + self.biases[-1]         A_out = Z_out  # Linear output         cache[f'Z{self.num_layers}'] = Z_out         cache[f'A{self.num_layers}'] = A_out         return A_out, cache      # Loss computation     def compute_loss(self, y_true, y_pred):         \"\"\"         Mean Squared Error (MSE) + optional L2 penalty.         \"\"\"         m = y_true.shape[0]         y_true = y_true.reshape(-1, 1)         mse = np.mean((y_pred - y_true) ** 2)         l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))         return mse + l2      # Backpropagation     def backward(self, cache, y_true):         \"\"\"         Compute gradients for all parameters using backpropagation.         \"\"\"         y_true = y_true.reshape(-1, 1)         m = y_true.shape[0]         grads_w, grads_b = [], []          # Gradient for output layer (MSE derivative)         y_pred = cache[f'A{self.num_layers}']         dZ = (2 / m) * (y_pred - y_true)  # MSE gradient          for i in range(self.num_layers - 1, -1, -1):             A_prev = cache[f'A{i}']             dW = (A_prev.T @ dZ) + (self.l2_lambda / m) * self.weights[i]             db = np.sum(dZ, axis=0, keepdims=True)             grads_w.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = dZ @ self.weights[i].T                 dZ = dA * self.relu_derivative(cache[f'Z{i}'])          return grads_w, grads_b      # Parameter update     def update_parameters(self, grads_w, grads_b, learning_rate):         \"\"\"Apply gradient descent step.\"\"\"         for i in range(self.num_layers):             self.weights[i] -= learning_rate * grads_w[i]             self.biases[i]  -= learning_rate * grads_b[i]      # Prediction helpers     def predict(self, X):         \"\"\"         Compute predicted continuous values for given inputs.         \"\"\"         y_pred, _ = self.forward(X)         return y_pred.reshape(-1)   print(\"MLP class implemented successfully.\") <pre>MLP class implemented successfully.\n</pre> In\u00a0[21]: Copied! <pre>def train_mlp(model, X_train, y_train, X_val, y_val,\n              epochs=50, batch_size=512, learning_rate=1e-2,\n              early_stopping=5, verbose=True):\n    \"\"\"\n    Mini-batch SGD training loop for regression.\n    \"\"\"\n    # Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed\n    if isinstance(X_train, pd.DataFrame):\n        X_train = X_train.values\n    if isinstance(X_val, pd.DataFrame):\n        X_val = X_val.values\n\n    if hasattr(X_train, \"toarray\"):\n        X_train = X_train.toarray()\n    if hasattr(X_val, \"toarray\"):\n        X_val = X_val.toarray()\n\n    y_train = np.asarray(y_train).reshape(-1)\n    y_val   = np.asarray(y_val).reshape(-1)\n\n    n = X_train.shape[0]\n    history = {'train_loss': [], 'val_loss': [], 'val_mae': [], 'val_r2': []}\n    best_val_loss = np.inf\n    patience = 0\n    best_weights = None\n\n    for epoch in range(1, epochs + 1):\n        # Shuffle training data\n        idx = np.random.permutation(n)\n        X_shuf = X_train[idx]\n        y_shuf = y_train[idx]\n\n        epoch_loss = 0.0\n\n        # Mini-batch training\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            Xb = X_shuf[start:end]\n            yb = y_shuf[start:end].reshape(-1, 1)\n\n            y_pred, cache = model.forward(Xb)\n            loss = model.compute_loss(yb, y_pred)\n            epoch_loss += loss * (end - start)\n\n            grads_w, grads_b = model.backward(cache, yb)\n            model.update_parameters(grads_w, grads_b, learning_rate)\n\n        epoch_loss /= n\n        history['train_loss'].append(epoch_loss)\n\n        # Validation metrics\n        y_val_pred, _ = model.forward(X_val)\n        y_val_reshaped = y_val.reshape(-1, 1)\n        val_loss = model.compute_loss(y_val_reshaped, y_val_pred)\n        \n        # Flatten predictions for sklearn metrics and ensure they're clean\n        y_val_pred_flat = y_val_pred.ravel()\n        \n        # Check for NaN or inf values\n        if np.isnan(y_val_pred_flat).any() or np.isinf(y_val_pred_flat).any():\n            print(f\"Warning: NaN or Inf detected in predictions at epoch {epoch}\")\n            break\n        \n        # Compute regression metrics\n        val_mae = mean_absolute_error(y_val, y_val_pred_flat)\n        val_r2 = r2_score(y_val, y_val_pred_flat)\n\n        history['val_loss'].append(val_loss)\n        history['val_mae'].append(val_mae)\n        history['val_r2'].append(val_r2)\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"\n                  f\"| val_mae={val_mae:.4f} | val_r2={val_r2:.4f}\")\n\n        # Early stopping\n        if val_loss &lt; best_val_loss - 1e-6:\n            best_val_loss = val_loss\n            best_weights = ([W.copy() for W in model.weights],\n                            [b.copy() for b in model.biases])\n            patience = 0\n        else:\n            patience += 1\n        if patience &gt;= early_stopping:\n            if verbose: print(\"Early stopping triggered.\")\n            break\n\n    if best_weights is not None:\n        model.weights, model.biases = best_weights\n\n    return history\n</pre> def train_mlp(model, X_train, y_train, X_val, y_val,               epochs=50, batch_size=512, learning_rate=1e-2,               early_stopping=5, verbose=True):     \"\"\"     Mini-batch SGD training loop for regression.     \"\"\"     # Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed     if isinstance(X_train, pd.DataFrame):         X_train = X_train.values     if isinstance(X_val, pd.DataFrame):         X_val = X_val.values      if hasattr(X_train, \"toarray\"):         X_train = X_train.toarray()     if hasattr(X_val, \"toarray\"):         X_val = X_val.toarray()      y_train = np.asarray(y_train).reshape(-1)     y_val   = np.asarray(y_val).reshape(-1)      n = X_train.shape[0]     history = {'train_loss': [], 'val_loss': [], 'val_mae': [], 'val_r2': []}     best_val_loss = np.inf     patience = 0     best_weights = None      for epoch in range(1, epochs + 1):         # Shuffle training data         idx = np.random.permutation(n)         X_shuf = X_train[idx]         y_shuf = y_train[idx]          epoch_loss = 0.0          # Mini-batch training         for start in range(0, n, batch_size):             end = min(start + batch_size, n)             Xb = X_shuf[start:end]             yb = y_shuf[start:end].reshape(-1, 1)              y_pred, cache = model.forward(Xb)             loss = model.compute_loss(yb, y_pred)             epoch_loss += loss * (end - start)              grads_w, grads_b = model.backward(cache, yb)             model.update_parameters(grads_w, grads_b, learning_rate)          epoch_loss /= n         history['train_loss'].append(epoch_loss)          # Validation metrics         y_val_pred, _ = model.forward(X_val)         y_val_reshaped = y_val.reshape(-1, 1)         val_loss = model.compute_loss(y_val_reshaped, y_val_pred)                  # Flatten predictions for sklearn metrics and ensure they're clean         y_val_pred_flat = y_val_pred.ravel()                  # Check for NaN or inf values         if np.isnan(y_val_pred_flat).any() or np.isinf(y_val_pred_flat).any():             print(f\"Warning: NaN or Inf detected in predictions at epoch {epoch}\")             break                  # Compute regression metrics         val_mae = mean_absolute_error(y_val, y_val_pred_flat)         val_r2 = r2_score(y_val, y_val_pred_flat)          history['val_loss'].append(val_loss)         history['val_mae'].append(val_mae)         history['val_r2'].append(val_r2)          if verbose:             print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"                   f\"| val_mae={val_mae:.4f} | val_r2={val_r2:.4f}\")          # Early stopping         if val_loss &lt; best_val_loss - 1e-6:             best_val_loss = val_loss             best_weights = ([W.copy() for W in model.weights],                             [b.copy() for b in model.biases])             patience = 0         else:             patience += 1         if patience &gt;= early_stopping:             if verbose: print(\"Early stopping triggered.\")             break      if best_weights is not None:         model.weights, model.biases = best_weights      return history In\u00a0[22]: Copied! <pre># Initialize and train MLP with improved architecture\nmlp_model = MLP(\n    input_dim=X_train_proc.shape[1], \n    hidden_sizes=[256, 128, 64, 32],  # Deeper network with more capacity\n    l2_lambda=1e-5\n)\n\nhistory = train_mlp(\n    mlp_model, \n    X_train_proc, y_train, \n    X_val_proc, y_val,\n    epochs=150,  # More epochs\n    batch_size=512, \n    learning_rate=0.001,\n    early_stopping=15,  # More patience\n    verbose=True\n)\n</pre> # Initialize and train MLP with improved architecture mlp_model = MLP(     input_dim=X_train_proc.shape[1],      hidden_sizes=[256, 128, 64, 32],  # Deeper network with more capacity     l2_lambda=1e-5 )  history = train_mlp(     mlp_model,      X_train_proc, y_train,      X_val_proc, y_val,     epochs=150,  # More epochs     batch_size=512,      learning_rate=0.001,     early_stopping=15,  # More patience     verbose=True ) <pre>Epoch 001 | train_loss=0.04514 | val_loss=0.02111 | val_mae=0.1100 | val_r2=0.9789\nEpoch 002 | train_loss=0.01721 | val_loss=0.01489 | val_mae=0.0908 | val_r2=0.9851\nEpoch 003 | train_loss=0.01335 | val_loss=0.01240 | val_mae=0.0823 | val_r2=0.9876\nEpoch 004 | train_loss=0.01150 | val_loss=0.01097 | val_mae=0.0770 | val_r2=0.9890\nEpoch 005 | train_loss=0.01035 | val_loss=0.00999 | val_mae=0.0730 | val_r2=0.9900\nEpoch 006 | train_loss=0.00954 | val_loss=0.00929 | val_mae=0.0700 | val_r2=0.9907\nEpoch 007 | train_loss=0.00894 | val_loss=0.00876 | val_mae=0.0677 | val_r2=0.9912\nEpoch 008 | train_loss=0.00847 | val_loss=0.00832 | val_mae=0.0657 | val_r2=0.9917\nEpoch 009 | train_loss=0.00809 | val_loss=0.00798 | val_mae=0.0641 | val_r2=0.9920\nEpoch 010 | train_loss=0.00777 | val_loss=0.00769 | val_mae=0.0626 | val_r2=0.9923\nEpoch 011 | train_loss=0.00750 | val_loss=0.00745 | val_mae=0.0614 | val_r2=0.9926\nEpoch 012 | train_loss=0.00727 | val_loss=0.00723 | val_mae=0.0603 | val_r2=0.9928\nEpoch 013 | train_loss=0.00707 | val_loss=0.00705 | val_mae=0.0594 | val_r2=0.9930\nEpoch 014 | train_loss=0.00690 | val_loss=0.00688 | val_mae=0.0585 | val_r2=0.9931\nEpoch 015 | train_loss=0.00674 | val_loss=0.00674 | val_mae=0.0577 | val_r2=0.9933\nEpoch 016 | train_loss=0.00660 | val_loss=0.00661 | val_mae=0.0570 | val_r2=0.9934\nEpoch 017 | train_loss=0.00647 | val_loss=0.00649 | val_mae=0.0564 | val_r2=0.9935\nEpoch 018 | train_loss=0.00636 | val_loss=0.00639 | val_mae=0.0558 | val_r2=0.9936\nEpoch 019 | train_loss=0.00626 | val_loss=0.00629 | val_mae=0.0552 | val_r2=0.9937\nEpoch 020 | train_loss=0.00616 | val_loss=0.00620 | val_mae=0.0547 | val_r2=0.9938\nEpoch 021 | train_loss=0.00607 | val_loss=0.00612 | val_mae=0.0542 | val_r2=0.9939\nEpoch 022 | train_loss=0.00599 | val_loss=0.00605 | val_mae=0.0538 | val_r2=0.9940\nEpoch 023 | train_loss=0.00592 | val_loss=0.00597 | val_mae=0.0534 | val_r2=0.9940\nEpoch 024 | train_loss=0.00585 | val_loss=0.00593 | val_mae=0.0531 | val_r2=0.9941\nEpoch 025 | train_loss=0.00578 | val_loss=0.00585 | val_mae=0.0526 | val_r2=0.9941\nEpoch 026 | train_loss=0.00572 | val_loss=0.00579 | val_mae=0.0523 | val_r2=0.9942\nEpoch 027 | train_loss=0.00566 | val_loss=0.00574 | val_mae=0.0520 | val_r2=0.9943\nEpoch 028 | train_loss=0.00561 | val_loss=0.00569 | val_mae=0.0517 | val_r2=0.9943\nEpoch 029 | train_loss=0.00556 | val_loss=0.00564 | val_mae=0.0514 | val_r2=0.9944\nEpoch 030 | train_loss=0.00551 | val_loss=0.00560 | val_mae=0.0511 | val_r2=0.9944\nEpoch 031 | train_loss=0.00546 | val_loss=0.00556 | val_mae=0.0508 | val_r2=0.9944\nEpoch 032 | train_loss=0.00542 | val_loss=0.00551 | val_mae=0.0506 | val_r2=0.9945\nEpoch 033 | train_loss=0.00538 | val_loss=0.00548 | val_mae=0.0503 | val_r2=0.9945\nEpoch 034 | train_loss=0.00534 | val_loss=0.00544 | val_mae=0.0501 | val_r2=0.9946\nEpoch 035 | train_loss=0.00530 | val_loss=0.00540 | val_mae=0.0499 | val_r2=0.9946\nEpoch 036 | train_loss=0.00526 | val_loss=0.00537 | val_mae=0.0496 | val_r2=0.9946\nEpoch 037 | train_loss=0.00523 | val_loss=0.00533 | val_mae=0.0494 | val_r2=0.9947\nEpoch 038 | train_loss=0.00519 | val_loss=0.00530 | val_mae=0.0492 | val_r2=0.9947\nEpoch 039 | train_loss=0.00516 | val_loss=0.00527 | val_mae=0.0490 | val_r2=0.9947\nEpoch 040 | train_loss=0.00513 | val_loss=0.00524 | val_mae=0.0488 | val_r2=0.9948\nEpoch 041 | train_loss=0.00510 | val_loss=0.00523 | val_mae=0.0487 | val_r2=0.9948\nEpoch 042 | train_loss=0.00507 | val_loss=0.00519 | val_mae=0.0485 | val_r2=0.9948\nEpoch 043 | train_loss=0.00505 | val_loss=0.00516 | val_mae=0.0483 | val_r2=0.9948\nEpoch 044 | train_loss=0.00502 | val_loss=0.00513 | val_mae=0.0481 | val_r2=0.9949\nEpoch 045 | train_loss=0.00500 | val_loss=0.00511 | val_mae=0.0480 | val_r2=0.9949\nEpoch 046 | train_loss=0.00497 | val_loss=0.00509 | val_mae=0.0478 | val_r2=0.9949\nEpoch 047 | train_loss=0.00495 | val_loss=0.00506 | val_mae=0.0477 | val_r2=0.9949\nEpoch 048 | train_loss=0.00493 | val_loss=0.00504 | val_mae=0.0475 | val_r2=0.9950\nEpoch 049 | train_loss=0.00491 | val_loss=0.00502 | val_mae=0.0474 | val_r2=0.9950\nEpoch 050 | train_loss=0.00488 | val_loss=0.00500 | val_mae=0.0472 | val_r2=0.9950\nEpoch 051 | train_loss=0.00486 | val_loss=0.00498 | val_mae=0.0471 | val_r2=0.9950\nEpoch 052 | train_loss=0.00484 | val_loss=0.00496 | val_mae=0.0470 | val_r2=0.9950\nEpoch 053 | train_loss=0.00483 | val_loss=0.00494 | val_mae=0.0468 | val_r2=0.9951\nEpoch 054 | train_loss=0.00481 | val_loss=0.00493 | val_mae=0.0467 | val_r2=0.9951\nEpoch 055 | train_loss=0.00479 | val_loss=0.00491 | val_mae=0.0466 | val_r2=0.9951\nEpoch 056 | train_loss=0.00477 | val_loss=0.00489 | val_mae=0.0465 | val_r2=0.9951\nEpoch 057 | train_loss=0.00476 | val_loss=0.00489 | val_mae=0.0464 | val_r2=0.9951\nEpoch 058 | train_loss=0.00474 | val_loss=0.00486 | val_mae=0.0462 | val_r2=0.9951\nEpoch 059 | train_loss=0.00472 | val_loss=0.00484 | val_mae=0.0461 | val_r2=0.9952\nEpoch 060 | train_loss=0.00471 | val_loss=0.00483 | val_mae=0.0460 | val_r2=0.9952\nEpoch 061 | train_loss=0.00469 | val_loss=0.00481 | val_mae=0.0459 | val_r2=0.9952\nEpoch 062 | train_loss=0.00468 | val_loss=0.00480 | val_mae=0.0458 | val_r2=0.9952\nEpoch 063 | train_loss=0.00466 | val_loss=0.00479 | val_mae=0.0458 | val_r2=0.9952\nEpoch 064 | train_loss=0.00465 | val_loss=0.00477 | val_mae=0.0456 | val_r2=0.9952\nEpoch 065 | train_loss=0.00464 | val_loss=0.00476 | val_mae=0.0455 | val_r2=0.9952\nEpoch 066 | train_loss=0.00462 | val_loss=0.00475 | val_mae=0.0454 | val_r2=0.9953\nEpoch 067 | train_loss=0.00461 | val_loss=0.00473 | val_mae=0.0454 | val_r2=0.9953\nEpoch 068 | train_loss=0.00460 | val_loss=0.00472 | val_mae=0.0453 | val_r2=0.9953\nEpoch 069 | train_loss=0.00459 | val_loss=0.00471 | val_mae=0.0452 | val_r2=0.9953\nEpoch 070 | train_loss=0.00457 | val_loss=0.00470 | val_mae=0.0451 | val_r2=0.9953\nEpoch 071 | train_loss=0.00456 | val_loss=0.00469 | val_mae=0.0450 | val_r2=0.9953\nEpoch 072 | train_loss=0.00455 | val_loss=0.00469 | val_mae=0.0451 | val_r2=0.9953\nEpoch 073 | train_loss=0.00454 | val_loss=0.00467 | val_mae=0.0449 | val_r2=0.9953\nEpoch 074 | train_loss=0.00453 | val_loss=0.00466 | val_mae=0.0448 | val_r2=0.9953\nEpoch 075 | train_loss=0.00452 | val_loss=0.00465 | val_mae=0.0447 | val_r2=0.9954\nEpoch 076 | train_loss=0.00451 | val_loss=0.00464 | val_mae=0.0447 | val_r2=0.9954\nEpoch 077 | train_loss=0.00450 | val_loss=0.00463 | val_mae=0.0446 | val_r2=0.9954\nEpoch 078 | train_loss=0.00449 | val_loss=0.00462 | val_mae=0.0445 | val_r2=0.9954\nEpoch 079 | train_loss=0.00448 | val_loss=0.00461 | val_mae=0.0444 | val_r2=0.9954\nEpoch 080 | train_loss=0.00447 | val_loss=0.00460 | val_mae=0.0444 | val_r2=0.9954\nEpoch 081 | train_loss=0.00446 | val_loss=0.00459 | val_mae=0.0443 | val_r2=0.9954\nEpoch 082 | train_loss=0.00445 | val_loss=0.00458 | val_mae=0.0442 | val_r2=0.9954\nEpoch 083 | train_loss=0.00444 | val_loss=0.00457 | val_mae=0.0442 | val_r2=0.9954\nEpoch 084 | train_loss=0.00443 | val_loss=0.00456 | val_mae=0.0441 | val_r2=0.9954\nEpoch 085 | train_loss=0.00442 | val_loss=0.00456 | val_mae=0.0440 | val_r2=0.9954\nEpoch 086 | train_loss=0.00441 | val_loss=0.00455 | val_mae=0.0440 | val_r2=0.9955\nEpoch 087 | train_loss=0.00441 | val_loss=0.00454 | val_mae=0.0439 | val_r2=0.9955\nEpoch 088 | train_loss=0.00440 | val_loss=0.00453 | val_mae=0.0439 | val_r2=0.9955\nEpoch 089 | train_loss=0.00439 | val_loss=0.00453 | val_mae=0.0438 | val_r2=0.9955\nEpoch 090 | train_loss=0.00438 | val_loss=0.00452 | val_mae=0.0437 | val_r2=0.9955\nEpoch 091 | train_loss=0.00437 | val_loss=0.00451 | val_mae=0.0437 | val_r2=0.9955\nEpoch 092 | train_loss=0.00437 | val_loss=0.00451 | val_mae=0.0437 | val_r2=0.9955\nEpoch 093 | train_loss=0.00436 | val_loss=0.00450 | val_mae=0.0436 | val_r2=0.9955\nEpoch 094 | train_loss=0.00435 | val_loss=0.00449 | val_mae=0.0435 | val_r2=0.9955\nEpoch 095 | train_loss=0.00434 | val_loss=0.00448 | val_mae=0.0435 | val_r2=0.9955\nEpoch 096 | train_loss=0.00434 | val_loss=0.00448 | val_mae=0.0435 | val_r2=0.9955\nEpoch 097 | train_loss=0.00433 | val_loss=0.00447 | val_mae=0.0434 | val_r2=0.9955\nEpoch 098 | train_loss=0.00432 | val_loss=0.00446 | val_mae=0.0433 | val_r2=0.9955\nEpoch 099 | train_loss=0.00432 | val_loss=0.00446 | val_mae=0.0433 | val_r2=0.9955\nEpoch 100 | train_loss=0.00431 | val_loss=0.00445 | val_mae=0.0432 | val_r2=0.9955\nEpoch 101 | train_loss=0.00430 | val_loss=0.00444 | val_mae=0.0432 | val_r2=0.9956\nEpoch 102 | train_loss=0.00430 | val_loss=0.00444 | val_mae=0.0431 | val_r2=0.9956\nEpoch 103 | train_loss=0.00429 | val_loss=0.00443 | val_mae=0.0431 | val_r2=0.9956\nEpoch 104 | train_loss=0.00428 | val_loss=0.00443 | val_mae=0.0430 | val_r2=0.9956\nEpoch 105 | train_loss=0.00428 | val_loss=0.00442 | val_mae=0.0430 | val_r2=0.9956\nEpoch 106 | train_loss=0.00427 | val_loss=0.00442 | val_mae=0.0430 | val_r2=0.9956\nEpoch 107 | train_loss=0.00427 | val_loss=0.00441 | val_mae=0.0429 | val_r2=0.9956\nEpoch 108 | train_loss=0.00426 | val_loss=0.00440 | val_mae=0.0429 | val_r2=0.9956\nEpoch 109 | train_loss=0.00426 | val_loss=0.00440 | val_mae=0.0428 | val_r2=0.9956\nEpoch 110 | train_loss=0.00425 | val_loss=0.00440 | val_mae=0.0428 | val_r2=0.9956\nEpoch 111 | train_loss=0.00424 | val_loss=0.00439 | val_mae=0.0427 | val_r2=0.9956\nEpoch 112 | train_loss=0.00424 | val_loss=0.00438 | val_mae=0.0427 | val_r2=0.9956\nEpoch 113 | train_loss=0.00423 | val_loss=0.00438 | val_mae=0.0427 | val_r2=0.9956\nEpoch 114 | train_loss=0.00423 | val_loss=0.00437 | val_mae=0.0426 | val_r2=0.9956\nEpoch 115 | train_loss=0.00422 | val_loss=0.00437 | val_mae=0.0426 | val_r2=0.9956\nEpoch 116 | train_loss=0.00422 | val_loss=0.00436 | val_mae=0.0425 | val_r2=0.9956\nEpoch 117 | train_loss=0.00421 | val_loss=0.00436 | val_mae=0.0425 | val_r2=0.9956\nEpoch 118 | train_loss=0.00421 | val_loss=0.00435 | val_mae=0.0425 | val_r2=0.9956\nEpoch 119 | train_loss=0.00420 | val_loss=0.00435 | val_mae=0.0424 | val_r2=0.9957\nEpoch 120 | train_loss=0.00420 | val_loss=0.00435 | val_mae=0.0424 | val_r2=0.9957\nEpoch 121 | train_loss=0.00419 | val_loss=0.00434 | val_mae=0.0423 | val_r2=0.9957\nEpoch 122 | train_loss=0.00419 | val_loss=0.00433 | val_mae=0.0423 | val_r2=0.9957\nEpoch 123 | train_loss=0.00418 | val_loss=0.00433 | val_mae=0.0423 | val_r2=0.9957\nEpoch 124 | train_loss=0.00418 | val_loss=0.00433 | val_mae=0.0422 | val_r2=0.9957\nEpoch 125 | train_loss=0.00417 | val_loss=0.00432 | val_mae=0.0422 | val_r2=0.9957\nEpoch 126 | train_loss=0.00417 | val_loss=0.00432 | val_mae=0.0422 | val_r2=0.9957\nEpoch 127 | train_loss=0.00416 | val_loss=0.00431 | val_mae=0.0421 | val_r2=0.9957\nEpoch 128 | train_loss=0.00416 | val_loss=0.00431 | val_mae=0.0421 | val_r2=0.9957\nEpoch 129 | train_loss=0.00416 | val_loss=0.00431 | val_mae=0.0421 | val_r2=0.9957\nEpoch 130 | train_loss=0.00415 | val_loss=0.00430 | val_mae=0.0420 | val_r2=0.9957\nEpoch 131 | train_loss=0.00415 | val_loss=0.00430 | val_mae=0.0420 | val_r2=0.9957\nEpoch 132 | train_loss=0.00414 | val_loss=0.00429 | val_mae=0.0419 | val_r2=0.9957\nEpoch 133 | train_loss=0.00414 | val_loss=0.00429 | val_mae=0.0419 | val_r2=0.9957\nEpoch 134 | train_loss=0.00413 | val_loss=0.00429 | val_mae=0.0419 | val_r2=0.9957\nEpoch 135 | train_loss=0.00413 | val_loss=0.00428 | val_mae=0.0419 | val_r2=0.9957\nEpoch 136 | train_loss=0.00413 | val_loss=0.00428 | val_mae=0.0418 | val_r2=0.9957\nEpoch 137 | train_loss=0.00412 | val_loss=0.00427 | val_mae=0.0418 | val_r2=0.9957\nEpoch 138 | train_loss=0.00412 | val_loss=0.00427 | val_mae=0.0418 | val_r2=0.9957\nEpoch 139 | train_loss=0.00411 | val_loss=0.00427 | val_mae=0.0417 | val_r2=0.9957\nEpoch 140 | train_loss=0.00411 | val_loss=0.00426 | val_mae=0.0417 | val_r2=0.9957\nEpoch 141 | train_loss=0.00411 | val_loss=0.00426 | val_mae=0.0417 | val_r2=0.9957\nEpoch 142 | train_loss=0.00410 | val_loss=0.00426 | val_mae=0.0416 | val_r2=0.9957\nEpoch 143 | train_loss=0.00410 | val_loss=0.00426 | val_mae=0.0416 | val_r2=0.9957\nEpoch 144 | train_loss=0.00410 | val_loss=0.00425 | val_mae=0.0416 | val_r2=0.9957\nEpoch 145 | train_loss=0.00409 | val_loss=0.00425 | val_mae=0.0416 | val_r2=0.9957\nEpoch 146 | train_loss=0.00409 | val_loss=0.00424 | val_mae=0.0415 | val_r2=0.9958\nEpoch 147 | train_loss=0.00409 | val_loss=0.00424 | val_mae=0.0415 | val_r2=0.9958\nEpoch 148 | train_loss=0.00408 | val_loss=0.00424 | val_mae=0.0415 | val_r2=0.9958\nEpoch 149 | train_loss=0.00408 | val_loss=0.00423 | val_mae=0.0415 | val_r2=0.9958\nEpoch 150 | train_loss=0.00408 | val_loss=0.00423 | val_mae=0.0414 | val_r2=0.9958\n</pre> In\u00a0[23]: Copied! <pre># Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Loss\naxes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\naxes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('MSE Loss', fontsize=12)\naxes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# MAE\naxes[1].plot(history['val_mae'], label='Val MAE', color='green', linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Mean Absolute Error', fontsize=12)\naxes[1].set_title('Validation MAE', fontsize=14, fontweight='bold')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# R\u00b2\naxes[2].plot(history['val_r2'], label='Val R\u00b2', color='red', linewidth=2)\naxes[2].set_xlabel('Epoch', fontsize=12)\naxes[2].set_ylabel('R\u00b2 Score', fontsize=12)\naxes[2].set_title('Validation R\u00b2 Score', fontsize=14, fontweight='bold')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest Validation Metrics:\")\nprint(f\"  MSE: {min(history['val_loss']):.4f}\")\nprint(f\"  MAE: {min(history['val_mae']):.4f}\")\nprint(f\"  R\u00b2: {max(history['val_r2']):.4f}\")\n</pre> # Plot training history fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Loss axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2) axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2) axes[0].set_xlabel('Epoch', fontsize=12) axes[0].set_ylabel('MSE Loss', fontsize=12) axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold') axes[0].legend() axes[0].grid(alpha=0.3)  # MAE axes[1].plot(history['val_mae'], label='Val MAE', color='green', linewidth=2) axes[1].set_xlabel('Epoch', fontsize=12) axes[1].set_ylabel('Mean Absolute Error', fontsize=12) axes[1].set_title('Validation MAE', fontsize=14, fontweight='bold') axes[1].legend() axes[1].grid(alpha=0.3)  # R\u00b2 axes[2].plot(history['val_r2'], label='Val R\u00b2', color='red', linewidth=2) axes[2].set_xlabel('Epoch', fontsize=12) axes[2].set_ylabel('R\u00b2 Score', fontsize=12) axes[2].set_title('Validation R\u00b2 Score', fontsize=14, fontweight='bold') axes[2].legend() axes[2].grid(alpha=0.3)  plt.tight_layout() plt.show()  print(f\"\\nBest Validation Metrics:\") print(f\"  MSE: {min(history['val_loss']):.4f}\") print(f\"  MAE: {min(history['val_mae']):.4f}\") print(f\"  R\u00b2: {max(history['val_r2']):.4f}\") <pre>\nBest Validation Metrics:\n  MSE: 0.0042\n  MAE: 0.0414\n  R\u00b2: 0.9958\n</pre> <p>Analysis of Training Curves</p> <p>Both training and validation loss curves decrease steadily over epochs, indicating effective learning without overfitting. The gap between training and validation loss remains small, suggesting good generalization.</p> <p>Validation metrics (MAE and R\u00b2) improve consistently, confirming that the model is learning meaningful patterns rather than memorizing the training data.</p> In\u00a0[24]: Copied! <pre># NOTE: This cell will fail if you haven't re-run the training after preprocessing changes!\n# Make sure to re-run cells starting from \"Feature Engineering\" section\n\n# Check feature dimensions match\nprint(f\"Model was trained with {mlp_model.weights[0].shape[0]} features\")\nprint(f\"Trying to predict with {X_val_proc.shape[1]} features\")\n\nif mlp_model.weights[0].shape[0] != X_val_proc.shape[1]:\n    print(\"\\n\u26a0\ufe0f ERROR: Feature dimension mismatch!\")\n    print(\"You need to re-run the training cell (e785d89e) after the preprocessing changes.\")\n    print(\"The model in memory was trained with the old preprocessing.\")\nelse:\n    # Predict on validation set\n    y_val_pred_norm = mlp_model.predict(X_val_proc)\n    y_val_pred = y_scaler.inverse_transform(y_val_pred_norm.reshape(-1, 1)).ravel()\n\n    # Check for negative predictions\n    num_negative_preds = np.sum(y_val_pred &lt; 0)\n    if num_negative_preds &gt; 0:\n        print(f\"Warning: {num_negative_preds} negative predictions found. Setting them to 1.0\")\n        y_val_pred = np.maximum(1.0, y_val_pred)\n\n    # Metrics on validation set\n    y_val_true = y_scaler.inverse_transform(y_val.reshape(-1, 1)).ravel()\n    val_mse = mean_squared_error(y_val_true, y_val_pred)\n    val_mae = mean_absolute_error(y_val_true, y_val_pred)\n    val_r2 = r2_score(y_val_true, y_val_pred)\n\n    print(f\"\\nValidation Set Metrics:\")\n    print(f\"  MSE: {val_mse:.4f}\")\n    print(f\"  MAE: {val_mae:.4f}\")\n    print(f\"  R\u00b2: {val_r2:.4f}\")\n</pre> # NOTE: This cell will fail if you haven't re-run the training after preprocessing changes! # Make sure to re-run cells starting from \"Feature Engineering\" section  # Check feature dimensions match print(f\"Model was trained with {mlp_model.weights[0].shape[0]} features\") print(f\"Trying to predict with {X_val_proc.shape[1]} features\")  if mlp_model.weights[0].shape[0] != X_val_proc.shape[1]:     print(\"\\n\u26a0\ufe0f ERROR: Feature dimension mismatch!\")     print(\"You need to re-run the training cell (e785d89e) after the preprocessing changes.\")     print(\"The model in memory was trained with the old preprocessing.\") else:     # Predict on validation set     y_val_pred_norm = mlp_model.predict(X_val_proc)     y_val_pred = y_scaler.inverse_transform(y_val_pred_norm.reshape(-1, 1)).ravel()      # Check for negative predictions     num_negative_preds = np.sum(y_val_pred &lt; 0)     if num_negative_preds &gt; 0:         print(f\"Warning: {num_negative_preds} negative predictions found. Setting them to 1.0\")         y_val_pred = np.maximum(1.0, y_val_pred)      # Metrics on validation set     y_val_true = y_scaler.inverse_transform(y_val.reshape(-1, 1)).ravel()     val_mse = mean_squared_error(y_val_true, y_val_pred)     val_mae = mean_absolute_error(y_val_true, y_val_pred)     val_r2 = r2_score(y_val_true, y_val_pred)      print(f\"\\nValidation Set Metrics:\")     print(f\"  MSE: {val_mse:.4f}\")     print(f\"  MAE: {val_mae:.4f}\")     print(f\"  R\u00b2: {val_r2:.4f}\") <pre>Model was trained with 11 features\nTrying to predict with 11 features\nWarning: 91 negative predictions found. Setting them to 1.0\n\nValidation Set Metrics:\n  MSE: 16.4178\n  MAE: 2.5767\n  R\u00b2: 0.9958\n</pre> <p>On the test set, the MLP achieved 48.1 MSE, 4.7 MAE and an R\u00b2 score of 0.988. These results indicate strong predictive performance, with low average error and high explained variance. Overall, the MLP effectively captured the complex relationships in the data, demonstrating its suitability for regression tasks on this dataset.</p> <p>We need to apply the exact same preprocessing pipeline to the Kaggle test set.</p> In\u00a0[25]: Copied! <pre># Step 1: Apply same feature engineering as training data\ntest_fe = test.copy()\n\n# BMI: kg/m^2\ntest_fe[\"BMI\"] = test_fe[\"Weight\"] / (test_fe[\"Height\"] / 100) ** 2\n\n# Workload index: combination of duration and heart rate\ntest_fe[\"Workload_Index\"] = test_fe[\"Duration\"] * test_fe[\"Heart_Rate\"]\n\n# Temperature deviation from normal body temp (37\u00b0C)\ntest_fe[\"Temp_Diff\"] = test_fe[\"Body_Temp\"] - 37.0\n\n# One-hot encode Sex (keep both columns)\ntest_fe = pd.get_dummies(test_fe, columns=[\"Sex\"], drop_first=False)\n\nprint(f\"After feature engineering: {test_fe.shape}\")\nprint(f\"Columns: {test_fe.columns.tolist()}\")\n</pre> # Step 1: Apply same feature engineering as training data test_fe = test.copy()  # BMI: kg/m^2 test_fe[\"BMI\"] = test_fe[\"Weight\"] / (test_fe[\"Height\"] / 100) ** 2  # Workload index: combination of duration and heart rate test_fe[\"Workload_Index\"] = test_fe[\"Duration\"] * test_fe[\"Heart_Rate\"]  # Temperature deviation from normal body temp (37\u00b0C) test_fe[\"Temp_Diff\"] = test_fe[\"Body_Temp\"] - 37.0  # One-hot encode Sex (keep both columns) test_fe = pd.get_dummies(test_fe, columns=[\"Sex\"], drop_first=False)  print(f\"After feature engineering: {test_fe.shape}\") print(f\"Columns: {test_fe.columns.tolist()}\") <pre>After feature engineering: (250000, 12)\nColumns: ['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI', 'Workload_Index', 'Temp_Diff', 'Sex_female', 'Sex_male']\n</pre> In\u00a0[26]: Copied! <pre># Step 2: Keep ALL features, only remove 'id'\n# Save the ID column before dropping it\ntest_ids = test_fe['id'].values\n\n# Only drop 'id' column - keep everything else!\ntest_fe.drop(columns=['id'], inplace=True)\n\nprint(f\"After removing id: {test_fe.shape}\")\nprint(f\"Remaining columns: {test_fe.columns.tolist()}\")\n</pre> # Step 2: Keep ALL features, only remove 'id' # Save the ID column before dropping it test_ids = test_fe['id'].values  # Only drop 'id' column - keep everything else! test_fe.drop(columns=['id'], inplace=True)  print(f\"After removing id: {test_fe.shape}\") print(f\"Remaining columns: {test_fe.columns.tolist()}\") <pre>After removing id: (250000, 11)\nRemaining columns: ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI', 'Workload_Index', 'Temp_Diff', 'Sex_female', 'Sex_male']\n</pre> In\u00a0[27]: Copied! <pre># Step 3: No outlier clipping - use data as is\ntest_work = test_fe.copy()\n\nprint(f\"Test data ready for preprocessing: {test_work.shape}\")\n</pre> # Step 3: No outlier clipping - use data as is test_work = test_fe.copy()  print(f\"Test data ready for preprocessing: {test_work.shape}\") <pre>Test data ready for preprocessing: (250000, 11)\n</pre> In\u00a0[28]: Copied! <pre># Step 4: Apply the fitted preprocessor (standardization)\n# Use the SAME preprocessor that was fitted on training data\nX_test_kaggle = test_work\nX_test_kaggle_proc = preprocessor.transform(X_test_kaggle)\n\nprint(f\"Final processed Kaggle test data shape: {X_test_kaggle_proc.shape}\")\nprint(f\"Expected shape: (250000, {X_train_proc.shape[1]})\")\n</pre> # Step 4: Apply the fitted preprocessor (standardization) # Use the SAME preprocessor that was fitted on training data X_test_kaggle = test_work X_test_kaggle_proc = preprocessor.transform(X_test_kaggle)  print(f\"Final processed Kaggle test data shape: {X_test_kaggle_proc.shape}\") print(f\"Expected shape: (250000, {X_train_proc.shape[1]})\") <pre>Final processed Kaggle test data shape: (250000, 11)\nExpected shape: (250000, 11)\n</pre> In\u00a0[29]: Copied! <pre># Convert sparse matrix to dense array if needed\nif hasattr(X_test_kaggle_proc, \"toarray\"):\n    X_test_kaggle_array = X_test_kaggle_proc.toarray()\nelse:\n    X_test_kaggle_array = X_test_kaggle_proc\n\n# Generate predictions (normalized scale)\ny_pred_kaggle_normalized = mlp_model.predict(X_test_kaggle_array)\n\n# Denormalize predictions back to original calorie scale\ny_pred_kaggle = y_scaler.inverse_transform(y_pred_kaggle_normalized.reshape(-1, 1)).ravel()\n\n# Check for negative predictions\nnegative_count = (y_pred_kaggle &lt; 0).sum()\nif negative_count &gt; 0:\n    print(f\"WARNING: Found {negative_count} negative predictions!\")\n    print(f\"  Min prediction before clipping: {y_pred_kaggle.min():.2f}\")\n    # Clip negative values to a small positive number (e.g., 1.0 calorie minimum)\n    y_pred_kaggle = np.maximum(y_pred_kaggle, 1.0)\n    print(f\"  Clipped negative predictions to minimum of 1.0 calories\")\n\nprint(f\"\\nPredictions generated for {len(y_pred_kaggle)} samples\")\nprint(f\"\\nPrediction statistics:\")\nprint(f\"  Mean: {y_pred_kaggle.mean():.2f}\")\nprint(f\"  Std: {y_pred_kaggle.std():.2f}\")\nprint(f\"  Min: {y_pred_kaggle.min():.2f}\")\nprint(f\"  Max: {y_pred_kaggle.max():.2f}\")\nprint(f\"\\nFirst 10 predictions: {y_pred_kaggle[:10]}\")\n</pre> # Convert sparse matrix to dense array if needed if hasattr(X_test_kaggle_proc, \"toarray\"):     X_test_kaggle_array = X_test_kaggle_proc.toarray() else:     X_test_kaggle_array = X_test_kaggle_proc  # Generate predictions (normalized scale) y_pred_kaggle_normalized = mlp_model.predict(X_test_kaggle_array)  # Denormalize predictions back to original calorie scale y_pred_kaggle = y_scaler.inverse_transform(y_pred_kaggle_normalized.reshape(-1, 1)).ravel()  # Check for negative predictions negative_count = (y_pred_kaggle &lt; 0).sum() if negative_count &gt; 0:     print(f\"WARNING: Found {negative_count} negative predictions!\")     print(f\"  Min prediction before clipping: {y_pred_kaggle.min():.2f}\")     # Clip negative values to a small positive number (e.g., 1.0 calorie minimum)     y_pred_kaggle = np.maximum(y_pred_kaggle, 1.0)     print(f\"  Clipped negative predictions to minimum of 1.0 calories\")  print(f\"\\nPredictions generated for {len(y_pred_kaggle)} samples\") print(f\"\\nPrediction statistics:\") print(f\"  Mean: {y_pred_kaggle.mean():.2f}\") print(f\"  Std: {y_pred_kaggle.std():.2f}\") print(f\"  Min: {y_pred_kaggle.min():.2f}\") print(f\"  Max: {y_pred_kaggle.max():.2f}\") print(f\"\\nFirst 10 predictions: {y_pred_kaggle[:10]}\") <pre>WARNING: Found 516 negative predictions!\n  Min prediction before clipping: -10.83\n  Clipped negative predictions to minimum of 1.0 calories\n\nPredictions generated for 250000 samples\n\nPrediction statistics:\n  Mean: 88.26\n  Std: 62.26\n  Min: 1.00\n  Max: 312.96\n\nFirst 10 predictions: [ 28.65376412 105.58138375  88.67509576 123.56889641  76.6958294\n  21.67882621  49.23047032   6.79581067  12.03656229 204.28665691]\n</pre> In\u00a0[30]: Copied! <pre># Create submission dataframe\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Calories': y_pred_kaggle\n})\n\n# Save to CSV\nsubmission_path = \"mlp_submission.csv\"\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"Submission file saved to: {submission_path}\")\nprint(f\"\\nSubmission shape: {submission.shape}\")\nprint(f\"\\nFirst 10 rows:\")\ndisplay(submission.head(10))\n\nprint(f\"\\nPrediction statistics:\")\nprint(f\"  Mean: {y_pred_kaggle.mean():.2f}\")\nprint(f\"  Std: {y_pred_kaggle.std():.2f}\")\nprint(f\"  Min: {y_pred_kaggle.min():.2f}\")\nprint(f\"  Max: {y_pred_kaggle.max():.2f}\")\nprint(f\"  Negatives clipped: {negative_count if negative_count &gt; 0 else 0}\")\n</pre> # Create submission dataframe submission = pd.DataFrame({     'id': test_ids,     'Calories': y_pred_kaggle })  # Save to CSV submission_path = \"mlp_submission.csv\" submission.to_csv(submission_path, index=False)  print(f\"Submission file saved to: {submission_path}\") print(f\"\\nSubmission shape: {submission.shape}\") print(f\"\\nFirst 10 rows:\") display(submission.head(10))  print(f\"\\nPrediction statistics:\") print(f\"  Mean: {y_pred_kaggle.mean():.2f}\") print(f\"  Std: {y_pred_kaggle.std():.2f}\") print(f\"  Min: {y_pred_kaggle.min():.2f}\") print(f\"  Max: {y_pred_kaggle.max():.2f}\") print(f\"  Negatives clipped: {negative_count if negative_count &gt; 0 else 0}\") <pre>Submission file saved to: mlp_submission.csv\n\nSubmission shape: (250000, 2)\n\nFirst 10 rows:\n</pre> id Calories 0 750000 28.653764 1 750001 105.581384 2 750002 88.675096 3 750003 123.568896 4 750004 76.695829 5 750005 21.678826 6 750006 49.230470 7 750007 6.795811 8 750008 12.036562 9 750009 204.286657 <pre>\nPrediction statistics:\n  Mean: 88.26\n  Std: 62.26\n  Min: 1.00\n  Max: 312.96\n  Negatives clipped: 516\n</pre> In\u00a0[32]: Copied! <pre># Submitting to Kaggle\n\n#!kaggle competitions submit -c playground-series-s5e5 -f mlp_submission.csv -m \"MLP model submission\"\n</pre> # Submitting to Kaggle  #!kaggle competitions submit -c playground-series-s5e5 -f mlp_submission.csv -m \"MLP model submission\" <p>With our current score, we would be ranked 3858th (out of 4318 participants) on the Kaggle leaderboard. The percentage is about top 89%.</p>"},{"location":"Projetos/Projeto2/notebook/#multi-layer-perceptron-mlp-regression","title":"Multi-Layer Perceptron (MLP) Regression\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#project-overview","title":"Project Overview\u00b6","text":"<p>This notebook implements a Multi-Layer Perceptron (MLP) neural network from scratch for regression on a Kaggle competition dataset.</p> <p>Authors: Jo\u00e3o Pedro Rodrigues, Matheus Castelucci, Rodrigo Medeiros</p>"},{"location":"Projetos/Projeto2/notebook/#1-dataset-selection","title":"1. Dataset Selection\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#dataset-information","title":"Dataset Information\u00b6","text":"<p>Name: Predict Calorie Expenditure</p> <p>Source: Kaggle Playground Series S5E5</p> <p>Original Dataset: Calories Burnt Prediction</p> <p>Size:</p> <ul> <li><p>Training set: 750,000 rows x 9 columns</p> </li> <li><p>Test set: 250,000 rows x 8 columns</p> </li> <li><p>Features: 7 features (6 numerical, 1 categorical)</p> </li> <li><p>Target: Continuous (calories burned)</p> </li> </ul> <p>Why this dataset?</p> <ul> <li><p>Realistic regression problem with a continuous target variable.</p> </li> <li><p>Large enough dataset to train a neural network.</p> </li> <li><p>Good opportunity to practice feature engineering and preprocessing.</p> </li> </ul>"},{"location":"Projetos/Projeto2/notebook/#2-dataset-explanation","title":"2. Dataset Explanation\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Numerical Features(6):</p> <ul> <li><code>Age</code>: Age of the individual (years)</li> <li><code>Height</code>: Height of the individual (cm)</li> <li><code>Weight</code>: Weight of the individual (kg)</li> <li><code>Duration</code>: Duration of the activity (minutes)</li> <li><code>Heart_rate</code>: Average heart rate during the activity (bpm)</li> <li><code>Body_temp</code>: Body temperature during the activity (\u00b0C)</li> </ul> <p>Categorical Feature(1):</p> <ul> <li><code>Sex</code>: Biological sex of the individual (Male or Female)</li> </ul> <p>Target Variable:</p> <ul> <li><code>Calories</code>: Total calories burned during the activity (continuous)</li> </ul>"},{"location":"Projetos/Projeto2/notebook/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#target-variable-distribution","title":"Target Variable Distribution\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#potential-issues-identified","title":"Potential Issues Identified\u00b6","text":"<ul> <li><p>Outliers: Z-score analysis (|z| &gt; 3) flagged &lt; 0.5 % of samples as potential outliers \u2014 primarily in <code>Body_Temp</code> and <code>Heart_Rate</code>. These are rare (&lt; 0.5 %) and will be retained, as they reflect legitimate high-intensity sessions rather than data errors.</p> </li> <li><p>Feature Correlation: The correlation matrix shows strong positive relationships between:</p> <ul> <li><p><code>Height</code> and <code>Weight</code> (0.96) \u2014 expected anthropometric link.</p> </li> <li><p><code>Duration</code>, <code>Heart_Rate</code>, and <code>Body_Temp</code> (0.8\u20130.9) \u2014 plausible physiological connections.</p> </li> <li><p>All three correlate strongly with <code>Calories</code> (0.83\u20130.96), confirming them as the main predictors.</p> </li> </ul> </li> <li><p>Collinearity: The high correlation between Height and Weight could introduce multicollinearity; normalization and careful regularization during training will help mitigate this.</p> </li> </ul>"},{"location":"Projetos/Projeto2/notebook/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#feature-engineering","title":"Feature Engineering\u00b6","text":"<p>Since some features have strong correlations between them, we'll create new meaningful features to help the MLP learn non-linear relationships:</p> New Feature Formula Motivation BMI (Body Mass Index) <code>BMI = Weight / (Height/100)^2</code> Captures body composition; <code>Weight</code> and <code>Height</code> are highly correlated and redundant alone. BMI can provide a more interpretable ratio. Effort Index <code>Effort_Index = Heart_Rate * Duration</code> Represents total exertion time \u00d7 intensity, directly related to calorie burn. Temp_Deviation <code>Temp_Deviation = Body_Temp - Body_Temp.mean()</code> Indicates thermoregulation response \u2014 people with higher deviation might burn more calories. Age_Squared <code>Age_Squared = Age ** 2</code> Introduces a nonlinear age effect \u2014 metabolism might change nonlinearly with age. Log_Duration <code>Log_Duration = log(Duration + 1)</code> Reduces potential scale imbalance; smooths duration\u2019s long tail."},{"location":"Projetos/Projeto2/notebook/#preprocessing-justification","title":"Preprocessing Justification\u00b6","text":"<ul> <li><p>Feature Engineering: Created 3 meaningful features (BMI, Workload_Index, Temp_Diff) while keeping ALL original features</p> </li> <li><p>No Feature Removal: Unlike the initial approach, we keep ALL features including Height, Weight, Sex. Every feature provides useful information.</p> </li> <li><p>No Outlier Clipping: The model learns from the full data distribution without artificial bounds.</p> </li> <li><p>Target Normalization: The target variable (Calories) is standardized using StandardScaler to have mean=0 and std=1. This is critical for:</p> <ul> <li>Preventing numerical instability (NaN/Inf) during training</li> <li>Ensuring gradients are in a reasonable range</li> <li>Faster convergence with lower learning rates</li> <li>The predictions are denormalized back to the original scale for evaluation</li> </ul> </li> <li><p>Data Split: Using 95%/5% train/val split (instead of 70/15/15) to maximize training data and rely on Kaggle leaderboard for final testing.</p> </li> </ul>"},{"location":"Projetos/Projeto2/notebook/#4-mlp-implementation","title":"4. MLP Implementation\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#hyperparameters","title":"Hyperparameters\u00b6","text":"<ul> <li><p>Input Layer: matches the number of preprocessed features.</p> </li> <li><p>Hidden Layers: three fully connected layers with 128, 64, and 32 neurons, each using ReLU activation to introduce non-linearity and mitigate vanishing gradients.</p> </li> <li><p>Output Layer: a single neuron with linear (identity) activation to output continuous values for regression.</p> </li> <li><p>Initialization: weights are initialized with He initialization (N(0, sqrt(2/n_in))) to maintain stable activation variance across layers.</p> </li> <li><p>Loss Function: Mean Squared Error (MSE) with optional L2 regularization, penalizing large weights to improve generalization.</p> </li> <li><p>Optimizer: standard Stochastic Gradient Descent (SGD) is used for parameter updates; gradient descent steps are handled in the training loop.</p> </li> </ul>"},{"location":"Projetos/Projeto2/notebook/#5-model-training","title":"5. Model Training\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#training-procedure","title":"Training Procedure\u00b6","text":"<p>The MLP was trained using mini-batch Stochastic Gradient Descent (SGD) implemented from scratch. Each epoch consists of the following steps:</p> <ol> <li><p>Data Shuffling and Mini-Batches \u2014 At the start of every epoch, the training data are randomly shuffled and divided into batches to improve gradient estimation and generalization.</p> </li> <li><p>Forward Propagation \u2014 For each batch, the model performs matrix multiplications and ReLU activations across layers to compute predicted continuous values.</p> </li> <li><p>Loss Computation \u2014 The Mean Squared Error (MSE) loss is computed between predictions and true labels, with an additional L2 penalty on the weights to discourage overfitting.</p> </li> <li><p>Backpropagation \u2014 Gradients of the MSE + L2 loss with respect to every weight and bias are obtained via the chain rule.</p> <ul> <li>The ReLU derivative (<code>1 if z &gt; 0 else 0</code>) prevents vanishing gradients common in sigmoid/tanh activations.</li> <li>Intermediate activations are stored in a <code>cache</code> dictionary for reuse during gradient computation.</li> </ul> </li> <li><p>Parameter Update \u2014 Each layer's weights and biases are updated by</p> <p>$$ W \\leftarrow W - \\eta\\,\\nabla_W L,\\qquad b \\leftarrow b - \\eta\\,\\nabla_b L $$</p> <p>where the learning rate controls the step size.</p> </li> <li><p>Validation Evaluation \u2014 After every epoch, the model runs a forward pass on the validation set to track MSE loss, MAE (Mean Absolute Error), and R\u00b2 score.</p> </li> <li><p>Early Stopping \u2014 If the validation loss fails to improve for a number of epochs, training stops and the best weights (lowest validation loss) are restored.</p> </li> </ol>"},{"location":"Projetos/Projeto2/notebook/#training-challenges-and-solutions","title":"Training Challenges and Solutions\u00b6","text":"<ul> <li>Vanishing Gradients: Addressed by using ReLU activations and He initialization, which preserve gradient scale across layers.</li> <li>Overfitting: Controlled with L2 regularization and early stopping; validation loss, MAE, and R\u00b2 were monitored each epoch.</li> <li>Instability in Loss: Occasional fluctuations caused by mini-batch noise; mitigated by averaging losses over all batches per epoch.</li> <li>Scale of Target Variable: Calorie values range widely; the model outputs continuous predictions without constraints, allowing it to learn the full range.</li> </ul>"},{"location":"Projetos/Projeto2/notebook/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy\u00b6","text":"<p>The dataset is split 95% train / 5% validation to maximize training data while still monitoring for overfitting.</p>"},{"location":"Projetos/Projeto2/notebook/#validation-role-in-hyperparameter-tuning","title":"Validation role in hyperparameter tuning\u00b6","text":"<ul> <li>The validation set is crucial for evaluating alternative model configurations such as learning rate, hidden layer size, L2 regularization strength, and batch size.</li> <li>The main selection metric is validation R\u00b2 score (<code>val_r2</code>), which indicates how well the model explains variance in the target. MAE (Mean Absolute Error) serves as a secondary indicator of prediction accuracy in the original scale.</li> <li>Typical tuning workflow:<ol> <li>Train each candidate model on the training set only.</li> <li>Evaluate on the validation set after every epoch to track learning progress.</li> <li>Select the configuration that yields the highest validation R\u00b2 or lowest validation MAE.</li> <li>Retrain the best model on the combined training + validation data before the final test evaluation.</li> </ol> </li> <li>Early stopping monitors validation loss and halts training when no improvement is observed for a fixed number of epochs (patience = 7), avoiding unnecessary computation and overfitting.</li> </ul>"},{"location":"Projetos/Projeto2/notebook/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#8-evaluation-metrics","title":"8. Evaluation Metrics\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#kaggle-submission","title":"Kaggle Submission\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#submission","title":"Submission\u00b6","text":""},{"location":"Projetos/Projeto2/notebook/#conclusion","title":"Conclusion\u00b6","text":"<p>The Multi-Layer Perceptron (MLP) implementation successfully demonstrated the fundamentals of neural network regression on the calorie expenditure prediction task. While achieving reasonable performance with an R\u00b2 score of 0.988 on validation data, this project highlights both the capabilities and limitations of basic fully-connected architectures.</p> <p>Key Achievements:</p> <ul> <li>Implemented a complete MLP from scratch using only NumPy</li> <li>Successfully handled feature engineering and preprocessing pipelines</li> <li>Achieved stable training with proper regularization and early stopping</li> <li>Generated valid predictions for Kaggle submission</li> </ul> <p>Limitations of MLP Architecture:</p> <ul> <li>Limited Feature Interaction Learning: MLPs struggle to capture complex, non-linear relationships between features compared to more sophisticated architectures</li> <li>Scalability Issues: Performance may plateau with increasing dataset size or feature complexity</li> <li>Sequential Dependencies: Cannot model temporal or sequential patterns that might exist in exercise data</li> <li>Feature Engineering Dependence: Requires manual creation of meaningful features (BMI, Workload_Index) rather than learning them automatically</li> </ul> <p>Alternative Neural Network Architectures for Better Performance:</p> <ul> <li>Convolutional Neural Networks (CNNs): Could better capture local patterns in multi-dimensional health data</li> <li>Recurrent Neural Networks (RNNs/LSTMs): Ideal for modeling temporal exercise patterns or heart rate sequences</li> <li>Transformer Networks: Excel at learning complex feature interactions through attention mechanisms</li> <li>Ensemble Methods: Combining multiple neural architectures often yields superior predictive performance</li> </ul> <p>While the MLP provides an acceptable baseline and serves as an excellent educational foundation, production systems would likely benefit from more advanced architectures that can automatically learn feature representations and capture complex data relationships without extensive manual feature engineering.</p>"},{"location":"Projetos/Projeto3/notebook/","title":"Project 3","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers import StableDiffusionImg2ImgPipeline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom typing import List, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Verificar se CUDA est\u00e1 dispon\u00edvel\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Usando dispositivo: {device}\")\nprint(f\"GPU dispon\u00edvel?: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n</pre> import torch from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler from diffusers import StableDiffusionImg2ImgPipeline import matplotlib.pyplot as plt from PIL import Image import numpy as np from typing import List, Optional import warnings warnings.filterwarnings('ignore')  # Verificar se CUDA est\u00e1 dispon\u00edvel device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Usando dispositivo: {device}\") print(f\"GPU dispon\u00edvel?: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\") In\u00a0[\u00a0]: Copied! <pre># Carregando o modelo Stable Diffusion\n# Usando o modelo v1.5 que \u00e9 gratuito e open-source\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\n\n# Configura\u00e7\u00f5es para otimizar mem\u00f3ria\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    safety_checker=None,  # Desabilitar para economizar mem\u00f3ria\n    requires_safety_checker=False\n)\n\n# Mover para GPU se dispon\u00edvel\npipe = pipe.to(device)\n\n# Habilitar otimiza\u00e7\u00f5es de mem\u00f3ria\nif torch.cuda.is_available():\n    pipe.enable_attention_slicing()\n    # pipe.enable_xformers_memory_efficient_attention()  # Descomente se xformers estiver instalado\n\nprint(\"Modelo carregado com sucesso!\")\n</pre> # Carregando o modelo Stable Diffusion # Usando o modelo v1.5 que \u00e9 gratuito e open-source model_id = \"runwayml/stable-diffusion-v1-5\"  # Configura\u00e7\u00f5es para otimizar mem\u00f3ria pipe = StableDiffusionPipeline.from_pretrained(     model_id,     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,     safety_checker=None,  # Desabilitar para economizar mem\u00f3ria     requires_safety_checker=False )  # Mover para GPU se dispon\u00edvel pipe = pipe.to(device)  # Habilitar otimiza\u00e7\u00f5es de mem\u00f3ria if torch.cuda.is_available():     pipe.enable_attention_slicing()     # pipe.enable_xformers_memory_efficient_attention()  # Descomente se xformers estiver instalado  print(\"Modelo carregado com sucesso!\") In\u00a0[\u00a0]: Copied! <pre># Visualiza\u00e7\u00e3o dos componentes do pipeline\ndef explain_pipeline_architecture():\n    \"\"\"\n    Explica a arquitetura do Stable Diffusion Pipeline\n    \"\"\"\n    print(\"=\"*80)\n    print(\"ARQUITETURA DO STABLE DIFFUSION PIPELINE\")\n    print(\"=\"*80)\n\n    components = {\n        \"1. Text Encoder (CLIP)\": {\n            \"Modelo\": pipe.text_encoder.__class__.__name__,\n            \"Fun\u00e7\u00e3o\": \"Converte texto em embeddings de 768 dimens\u00f5es\",\n            \"Par\u00e2metros\": sum(p.numel() for p in pipe.text_encoder.parameters())\n        },\n        \"2. Tokenizer\": {\n            \"Modelo\": pipe.tokenizer.__class__.__name__,\n            \"Fun\u00e7\u00e3o\": \"Tokeniza o texto de entrada (m\u00e1x 77 tokens)\",\n            \"Vocab Size\": pipe.tokenizer.vocab_size\n        },\n        \"3. U-Net\": {\n            \"Modelo\": pipe.unet.__class__.__name__,\n            \"Fun\u00e7\u00e3o\": \"Modelo de difus\u00e3o que remove ru\u00eddo iterativamente\",\n            \"Par\u00e2metros\": sum(p.numel() for p in pipe.unet.parameters()),\n            \"Input Channels\": pipe.unet.config.in_channels,\n            \"Output Channels\": pipe.unet.config.out_channels\n        },\n        \"4. VAE (Variational Autoencoder)\": {\n            \"Modelo\": pipe.vae.__class__.__name__,\n            \"Fun\u00e7\u00e3o\": \"Codifica/decodifica entre espa\u00e7o latente e imagem\",\n            \"Latent Channels\": pipe.vae.config.latent_channels,\n            \"Par\u00e2metros\": sum(p.numel() for p in pipe.vae.parameters())\n        },\n        \"5. Scheduler\": {\n            \"Modelo\": pipe.scheduler.__class__.__name__,\n            \"Fun\u00e7\u00e3o\": \"Controla o processo de denoising\",\n            \"Num Steps\": pipe.scheduler.config.num_train_timesteps\n        }\n    }\n\n    for component, details in components.items():\n        print(f\"\\n{component}\")\n        print(\"-\" * 40)\n        for key, value in details.items():\n            print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"FLUXO DO PROCESSO:\")\n    print(\"=\"*80)\n    print(\"1. Texto \u2192 Tokenizer \u2192 Tokens\")\n    print(\"2. Tokens \u2192 Text Encoder (CLIP) \u2192 Text Embeddings\")\n    print(\"3. Random Noise + Text Embeddings \u2192 U-Net (iterativo)\")\n    print(\"4. U-Net realiza denoising em m\u00faltiplos steps\")\n    print(\"5. Latent Image \u2192 VAE Decoder \u2192 Imagem Final (512x512)\")\n\nexplain_pipeline_architecture()\n</pre> # Visualiza\u00e7\u00e3o dos componentes do pipeline def explain_pipeline_architecture():     \"\"\"     Explica a arquitetura do Stable Diffusion Pipeline     \"\"\"     print(\"=\"*80)     print(\"ARQUITETURA DO STABLE DIFFUSION PIPELINE\")     print(\"=\"*80)      components = {         \"1. Text Encoder (CLIP)\": {             \"Modelo\": pipe.text_encoder.__class__.__name__,             \"Fun\u00e7\u00e3o\": \"Converte texto em embeddings de 768 dimens\u00f5es\",             \"Par\u00e2metros\": sum(p.numel() for p in pipe.text_encoder.parameters())         },         \"2. Tokenizer\": {             \"Modelo\": pipe.tokenizer.__class__.__name__,             \"Fun\u00e7\u00e3o\": \"Tokeniza o texto de entrada (m\u00e1x 77 tokens)\",             \"Vocab Size\": pipe.tokenizer.vocab_size         },         \"3. U-Net\": {             \"Modelo\": pipe.unet.__class__.__name__,             \"Fun\u00e7\u00e3o\": \"Modelo de difus\u00e3o que remove ru\u00eddo iterativamente\",             \"Par\u00e2metros\": sum(p.numel() for p in pipe.unet.parameters()),             \"Input Channels\": pipe.unet.config.in_channels,             \"Output Channels\": pipe.unet.config.out_channels         },         \"4. VAE (Variational Autoencoder)\": {             \"Modelo\": pipe.vae.__class__.__name__,             \"Fun\u00e7\u00e3o\": \"Codifica/decodifica entre espa\u00e7o latente e imagem\",             \"Latent Channels\": pipe.vae.config.latent_channels,             \"Par\u00e2metros\": sum(p.numel() for p in pipe.vae.parameters())         },         \"5. Scheduler\": {             \"Modelo\": pipe.scheduler.__class__.__name__,             \"Fun\u00e7\u00e3o\": \"Controla o processo de denoising\",             \"Num Steps\": pipe.scheduler.config.num_train_timesteps         }     }      for component, details in components.items():         print(f\"\\n{component}\")         print(\"-\" * 40)         for key, value in details.items():             print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")      print(\"\\n\" + \"=\"*80)     print(\"FLUXO DO PROCESSO:\")     print(\"=\"*80)     print(\"1. Texto \u2192 Tokenizer \u2192 Tokens\")     print(\"2. Tokens \u2192 Text Encoder (CLIP) \u2192 Text Embeddings\")     print(\"3. Random Noise + Text Embeddings \u2192 U-Net (iterativo)\")     print(\"4. U-Net realiza denoising em m\u00faltiplos steps\")     print(\"5. Latent Image \u2192 VAE Decoder \u2192 Imagem Final (512x512)\")  explain_pipeline_architecture() In\u00a0[\u00a0]: Copied! <pre>def generate_images(\n    prompt: str,\n    negative_prompt: Optional[str] = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 7.5,\n    height: int = 512,\n    width: int = 512,\n    seed: Optional[int] = None,\n    num_images: int = 1\n) -&gt; List[Image.Image]:\n    \"\"\"\n    Gera imagens usando Stable Diffusion\n\n    Par\u00e2metros:\n    -----------\n    prompt: Descri\u00e7\u00e3o textual da imagem desejada\n    negative_prompt: O que evitar na gera\u00e7\u00e3o\n    num_inference_steps: N\u00famero de passos de denoising (20-100)\n    guidance_scale: For\u00e7a de ader\u00eancia ao prompt (1-20)\n    height/width: Dimens\u00f5es da imagem (m\u00faltiplos de 8)\n    seed: Semente para reprodutibilidade\n    num_images: N\u00famero de imagens a gerar\n    \"\"\"\n\n    # Configurar seed se fornecido\n    if seed is not None:\n        generator = torch.Generator(device=device).manual_seed(seed)\n    else:\n        generator = None\n\n    # Gerar imagens\n    images = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        height=height,\n        width=width,\n        generator=generator,\n        num_images_per_prompt=num_images\n    ).images\n\n    return images\n\n# Fun\u00e7\u00e3o auxiliar para visualizar resultados\ndef plot_images(images: List[Image.Image], prompt: str, params: dict = None):\n    \"\"\"Visualiza as imagens geradas com seus par\u00e2metros\"\"\"\n    n_images = len(images)\n    fig, axes = plt.subplots(1, n_images, figsize=(6*n_images, 6))\n\n    if n_images == 1:\n        axes = [axes]\n\n    for idx, (ax, img) in enumerate(zip(axes, images)):\n        ax.imshow(img)\n        ax.axis('off')\n        if idx == 0 and params:\n            title = f\"Prompt: {prompt[:50]}...\\n\"\n            title += f\"Steps: {params.get('steps', 'N/A')}, \"\n            title += f\"Guidance: {params.get('guidance', 'N/A')}, \"\n            title += f\"Seed: {params.get('seed', 'Random')}\"\n            ax.set_title(title, fontsize=10, pad=10)\n\n    plt.tight_layout()\n    plt.show()\n</pre> def generate_images(     prompt: str,     negative_prompt: Optional[str] = None,     num_inference_steps: int = 50,     guidance_scale: float = 7.5,     height: int = 512,     width: int = 512,     seed: Optional[int] = None,     num_images: int = 1 ) -&gt; List[Image.Image]:     \"\"\"     Gera imagens usando Stable Diffusion      Par\u00e2metros:     -----------     prompt: Descri\u00e7\u00e3o textual da imagem desejada     negative_prompt: O que evitar na gera\u00e7\u00e3o     num_inference_steps: N\u00famero de passos de denoising (20-100)     guidance_scale: For\u00e7a de ader\u00eancia ao prompt (1-20)     height/width: Dimens\u00f5es da imagem (m\u00faltiplos de 8)     seed: Semente para reprodutibilidade     num_images: N\u00famero de imagens a gerar     \"\"\"      # Configurar seed se fornecido     if seed is not None:         generator = torch.Generator(device=device).manual_seed(seed)     else:         generator = None      # Gerar imagens     images = pipe(         prompt=prompt,         negative_prompt=negative_prompt,         num_inference_steps=num_inference_steps,         guidance_scale=guidance_scale,         height=height,         width=width,         generator=generator,         num_images_per_prompt=num_images     ).images      return images  # Fun\u00e7\u00e3o auxiliar para visualizar resultados def plot_images(images: List[Image.Image], prompt: str, params: dict = None):     \"\"\"Visualiza as imagens geradas com seus par\u00e2metros\"\"\"     n_images = len(images)     fig, axes = plt.subplots(1, n_images, figsize=(6*n_images, 6))      if n_images == 1:         axes = [axes]      for idx, (ax, img) in enumerate(zip(axes, images)):         ax.imshow(img)         ax.axis('off')         if idx == 0 and params:             title = f\"Prompt: {prompt[:50]}...\\n\"             title += f\"Steps: {params.get('steps', 'N/A')}, \"             title += f\"Guidance: {params.get('guidance', 'N/A')}, \"             title += f\"Seed: {params.get('seed', 'Random')}\"             ax.set_title(title, fontsize=10, pad=10)      plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre># Exemplo 1: Variando Guidance Scale\nprint(\"EXEMPLO 1: Efeito do Guidance Scale na Gera\u00e7\u00e3o\")\nprint(\"-\" * 50)\n\nprompt = \"A majestic lion wearing a crown, digital art, highly detailed\"\nnegative_prompt = \"blurry, low quality, distorted\"\n\nguidance_scales = [2.0, 5.0, 7.5, 10.0, 15.0]\nimages_guidance = []\n\nfor guidance in guidance_scales:\n    print(f\"Gerando com guidance_scale={guidance}...\")\n    img = generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        guidance_scale=guidance,\n        num_inference_steps=30,\n        seed=42  # Mesma seed para compara\u00e7\u00e3o\n    )[0]\n    images_guidance.append(img)\n\n# Plotar resultados\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, (img, guidance) in enumerate(zip(images_guidance, guidance_scales)):\n    axes[idx].imshow(img)\n    axes[idx].set_title(f\"Guidance: {guidance}\")\n    axes[idx].axis('off')\nplt.suptitle(\"Impacto do Guidance Scale (maior = mais fiel ao prompt)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Exemplo 1: Variando Guidance Scale print(\"EXEMPLO 1: Efeito do Guidance Scale na Gera\u00e7\u00e3o\") print(\"-\" * 50)  prompt = \"A majestic lion wearing a crown, digital art, highly detailed\" negative_prompt = \"blurry, low quality, distorted\"  guidance_scales = [2.0, 5.0, 7.5, 10.0, 15.0] images_guidance = []  for guidance in guidance_scales:     print(f\"Gerando com guidance_scale={guidance}...\")     img = generate_images(         prompt=prompt,         negative_prompt=negative_prompt,         guidance_scale=guidance,         num_inference_steps=30,         seed=42  # Mesma seed para compara\u00e7\u00e3o     )[0]     images_guidance.append(img)  # Plotar resultados fig, axes = plt.subplots(1, 5, figsize=(20, 4)) for idx, (img, guidance) in enumerate(zip(images_guidance, guidance_scales)):     axes[idx].imshow(img)     axes[idx].set_title(f\"Guidance: {guidance}\")     axes[idx].axis('off') plt.suptitle(\"Impacto do Guidance Scale (maior = mais fiel ao prompt)\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Exemplo 2: Variando n\u00famero de inference steps\nprint(\"EXEMPLO 2: Efeito do N\u00famero de Steps de Denoising\")\nprint(\"-\" * 50)\n\nprompt = \"A cyberpunk city at night with neon lights, rainy weather, reflections\"\nsteps_list = [10, 20, 30, 50, 75]\nimages_steps = []\n\nfor steps in steps_list:\n    print(f\"Gerando com {steps} steps...\")\n    img = generate_images(\n        prompt=prompt,\n        num_inference_steps=steps,\n        guidance_scale=7.5,\n        seed=123  # Mesma seed\n    )[0]\n    images_steps.append(img)\n\n# Plotar\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, (img, steps) in enumerate(zip(images_steps, steps_list)):\n    axes[idx].imshow(img)\n    axes[idx].set_title(f\"Steps: {steps}\")\n    axes[idx].axis('off')\nplt.suptitle(\"Impacto do N\u00famero de Steps (mais steps = mais refinamento)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Exemplo 2: Variando n\u00famero de inference steps print(\"EXEMPLO 2: Efeito do N\u00famero de Steps de Denoising\") print(\"-\" * 50)  prompt = \"A cyberpunk city at night with neon lights, rainy weather, reflections\" steps_list = [10, 20, 30, 50, 75] images_steps = []  for steps in steps_list:     print(f\"Gerando com {steps} steps...\")     img = generate_images(         prompt=prompt,         num_inference_steps=steps,         guidance_scale=7.5,         seed=123  # Mesma seed     )[0]     images_steps.append(img)  # Plotar fig, axes = plt.subplots(1, 5, figsize=(20, 4)) for idx, (img, steps) in enumerate(zip(images_steps, steps_list)):     axes[idx].imshow(img)     axes[idx].set_title(f\"Steps: {steps}\")     axes[idx].axis('off') plt.suptitle(\"Impacto do N\u00famero de Steps (mais steps = mais refinamento)\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Exemplo 3: Diferentes estilos art\u00edsticos\nprint(\"EXEMPLO 3: Gerando Diferentes Estilos Art\u00edsticos\")\nprint(\"-\" * 50)\n\nbase_subject = \"a beautiful mountain landscape with a lake\"\nstyles = [\n    \"photorealistic, 8k photography\",\n    \"oil painting in the style of Van Gogh\",\n    \"japanese anime style, studio ghibli\",\n    \"pencil sketch, detailed drawing\",\n    \"watercolor painting, soft colors\"\n]\n\nimages_styles = []\nfor style in styles:\n    full_prompt = f\"{base_subject}, {style}\"\n    print(f\"Gerando: {style[:30]}...\")\n    img = generate_images(\n        prompt=full_prompt,\n        num_inference_steps=40,\n        guidance_scale=8.0\n    )[0]\n    images_styles.append(img)\n\n# Plotar\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, (img, style) in enumerate(zip(images_styles, styles)):\n    axes[idx].imshow(img)\n    axes[idx].set_title(style[:30] + \"...\", fontsize=10)\n    axes[idx].axis('off')\nplt.suptitle(\"Mesmo Tema em Diferentes Estilos Art\u00edsticos\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Exemplo 3: Diferentes estilos art\u00edsticos print(\"EXEMPLO 3: Gerando Diferentes Estilos Art\u00edsticos\") print(\"-\" * 50)  base_subject = \"a beautiful mountain landscape with a lake\" styles = [     \"photorealistic, 8k photography\",     \"oil painting in the style of Van Gogh\",     \"japanese anime style, studio ghibli\",     \"pencil sketch, detailed drawing\",     \"watercolor painting, soft colors\" ]  images_styles = [] for style in styles:     full_prompt = f\"{base_subject}, {style}\"     print(f\"Gerando: {style[:30]}...\")     img = generate_images(         prompt=full_prompt,         num_inference_steps=40,         guidance_scale=8.0     )[0]     images_styles.append(img)  # Plotar fig, axes = plt.subplots(1, 5, figsize=(20, 4)) for idx, (img, style) in enumerate(zip(images_styles, styles)):     axes[idx].imshow(img)     axes[idx].set_title(style[:30] + \"...\", fontsize=10)     axes[idx].axis('off') plt.suptitle(\"Mesmo Tema em Diferentes Estilos Art\u00edsticos\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Exemplo 4: Impacto do Negative Prompt\nprint(\"EXEMPLO 4: Import\u00e2ncia do Negative Prompt\")\nprint(\"-\" * 50)\n\nprompt = \"A portrait of a wizard casting a spell, fantasy art\"\n\nnegative_prompts = [\n    \"\",  # Sem negative prompt\n    \"ugly, distorted\",\n    \"ugly, distorted, blurry, low quality\",\n    \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy\",\n    \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy, cartoon, anime\"\n]\n\nimages_negative = []\nfor neg_prompt in negative_prompts:\n    print(f\"Negative prompt: {neg_prompt[:30] if neg_prompt else 'None'}...\")\n    img = generate_images(\n        prompt=prompt,\n        negative_prompt=neg_prompt if neg_prompt else None,\n        num_inference_steps=40,\n        guidance_scale=7.5,\n        seed=999\n    )[0]\n    images_negative.append(img)\n\n# Plotar\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, (img, neg) in enumerate(zip(images_negative, negative_prompts)):\n    axes[idx].imshow(img)\n    title = neg[:20] + \"...\" if neg else \"Sem negative\"\n    axes[idx].set_title(title, fontsize=9)\n    axes[idx].axis('off')\nplt.suptitle(\"Efeito do Negative Prompt na Qualidade\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Exemplo 4: Impacto do Negative Prompt print(\"EXEMPLO 4: Import\u00e2ncia do Negative Prompt\") print(\"-\" * 50)  prompt = \"A portrait of a wizard casting a spell, fantasy art\"  negative_prompts = [     \"\",  # Sem negative prompt     \"ugly, distorted\",     \"ugly, distorted, blurry, low quality\",     \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy\",     \"ugly, distorted, blurry, low quality, extra limbs, bad anatomy, cartoon, anime\" ]  images_negative = [] for neg_prompt in negative_prompts:     print(f\"Negative prompt: {neg_prompt[:30] if neg_prompt else 'None'}...\")     img = generate_images(         prompt=prompt,         negative_prompt=neg_prompt if neg_prompt else None,         num_inference_steps=40,         guidance_scale=7.5,         seed=999     )[0]     images_negative.append(img)  # Plotar fig, axes = plt.subplots(1, 5, figsize=(20, 4)) for idx, (img, neg) in enumerate(zip(images_negative, negative_prompts)):     axes[idx].imshow(img)     title = neg[:20] + \"...\" if neg else \"Sem negative\"     axes[idx].set_title(title, fontsize=9)     axes[idx].axis('off') plt.suptitle(\"Efeito do Negative Prompt na Qualidade\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Exemplo 5: Varia\u00e7\u00e3o com diferentes seeds\nprint(\"EXEMPLO 5: Varia\u00e7\u00e3o com Diferentes Seeds\")\nprint(\"-\" * 50)\n\nprompt = \"A futuristic robot in a garden, detailed, artistic\"\nseeds = [42, 123, 456, 789, 2024]\nimages_seeds = []\n\nfor seed in seeds:\n    print(f\"Gerando com seed={seed}...\")\n    img = generate_images(\n        prompt=prompt,\n        num_inference_steps=35,\n        guidance_scale=7.5,\n        seed=seed\n    )[0]\n    images_seeds.append(img)\n\n# Plotar\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, (img, seed) in enumerate(zip(images_seeds, seeds)):\n    axes[idx].imshow(img)\n    axes[idx].set_title(f\"Seed: {seed}\")\n    axes[idx].axis('off')\nplt.suptitle(\"Varia\u00e7\u00f5es do Mesmo Prompt com Seeds Diferentes\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Exemplo 5: Varia\u00e7\u00e3o com diferentes seeds print(\"EXEMPLO 5: Varia\u00e7\u00e3o com Diferentes Seeds\") print(\"-\" * 50)  prompt = \"A futuristic robot in a garden, detailed, artistic\" seeds = [42, 123, 456, 789, 2024] images_seeds = []  for seed in seeds:     print(f\"Gerando com seed={seed}...\")     img = generate_images(         prompt=prompt,         num_inference_steps=35,         guidance_scale=7.5,         seed=seed     )[0]     images_seeds.append(img)  # Plotar fig, axes = plt.subplots(1, 5, figsize=(20, 4)) for idx, (img, seed) in enumerate(zip(images_seeds, seeds)):     axes[idx].imshow(img)     axes[idx].set_title(f\"Seed: {seed}\")     axes[idx].axis('off') plt.suptitle(\"Varia\u00e7\u00f5es do Mesmo Prompt com Seeds Diferentes\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>import time\n\ndef benchmark_generation(steps_list=[20, 30, 50]):\n    \"\"\"Analisa tempo de gera\u00e7\u00e3o vs qualidade\"\"\"\n    print(\"AN\u00c1LISE DE PERFORMANCE\")\n    print(\"-\" * 50)\n\n    prompt = \"A detailed portrait of a astronaut, professional photography\"\n    results = []\n\n    for steps in steps_list:\n        start_time = time.time()\n\n        img = generate_images(\n            prompt=prompt,\n            num_inference_steps=steps,\n            seed=42\n        )[0]\n\n        gen_time = time.time() - start_time\n\n        results.append({\n            'steps': steps,\n            'time': gen_time,\n            'time_per_step': gen_time / steps,\n            'image': img\n        })\n\n        print(f\"Steps: {steps:3d} | Tempo: {gen_time:.2f}s | Por step: {gen_time/steps:.3f}s\")\n\n    # Plotar resultados\n    fig, axes = plt.subplots(1, len(results), figsize=(15, 5))\n    for idx, res in enumerate(results):\n        axes[idx].imshow(res['image'])\n        axes[idx].set_title(\n            f\"Steps: {res['steps']}\\n\"\n            f\"Tempo: {res['time']:.1f}s\\n\"\n            f\"ms/step: {res['time_per_step']*1000:.1f}\",\n            fontsize=10\n        )\n        axes[idx].axis('off')\n\n    plt.suptitle(\"Trade-off: Qualidade vs Tempo de Gera\u00e7\u00e3o\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    return results\n\n# Executar benchmark\nresults = benchmark_generation()\n</pre> import time  def benchmark_generation(steps_list=[20, 30, 50]):     \"\"\"Analisa tempo de gera\u00e7\u00e3o vs qualidade\"\"\"     print(\"AN\u00c1LISE DE PERFORMANCE\")     print(\"-\" * 50)      prompt = \"A detailed portrait of a astronaut, professional photography\"     results = []      for steps in steps_list:         start_time = time.time()          img = generate_images(             prompt=prompt,             num_inference_steps=steps,             seed=42         )[0]          gen_time = time.time() - start_time          results.append({             'steps': steps,             'time': gen_time,             'time_per_step': gen_time / steps,             'image': img         })          print(f\"Steps: {steps:3d} | Tempo: {gen_time:.2f}s | Por step: {gen_time/steps:.3f}s\")      # Plotar resultados     fig, axes = plt.subplots(1, len(results), figsize=(15, 5))     for idx, res in enumerate(results):         axes[idx].imshow(res['image'])         axes[idx].set_title(             f\"Steps: {res['steps']}\\n\"             f\"Tempo: {res['time']:.1f}s\\n\"             f\"ms/step: {res['time_per_step']*1000:.1f}\",             fontsize=10         )         axes[idx].axis('off')      plt.suptitle(\"Trade-off: Qualidade vs Tempo de Gera\u00e7\u00e3o\", fontsize=14)     plt.tight_layout()     plt.show()      return results  # Executar benchmark results = benchmark_generation() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom datetime import datetime\n\ndef save_generation_batch(prompts_dict, output_dir=\"generated_images\"):\n    \"\"\"\n    Salva um batch de gera\u00e7\u00f5es com metadados\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    metadata = []\n\n    for name, config in prompts_dict.items():\n        print(f\"Gerando: {name}...\")\n\n        img = generate_images(**config)[0]\n\n        filename = f\"{timestamp}_{name}.png\"\n        filepath = os.path.join(output_dir, filename)\n        img.save(filepath)\n\n        metadata.append({\n            'name': name,\n            'file': filename,\n            'config': config\n        })\n\n        print(f\"  Salvo em: {filepath}\")\n\n    # Salvar metadados\n    import json\n    metadata_file = os.path.join(output_dir, f\"{timestamp}_metadata.json\")\n    with open(metadata_file, 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n    print(f\"\\nMetadados salvos em: {metadata_file}\")\n    return metadata\n\n# Exemplo de uso\nprompts_para_salvar = {\n    \"landscape\": {\n        \"prompt\": \"Beautiful mountain landscape at sunset, photorealistic\",\n        \"num_inference_steps\": 40,\n        \"guidance_scale\": 7.5,\n        \"seed\": 42\n    },\n    \"portrait\": {\n        \"prompt\": \"Professional portrait of a scientist in laboratory\",\n        \"num_inference_steps\": 50,\n        \"guidance_scale\": 8.0,\n        \"seed\": 123\n    },\n    \"abstract\": {\n        \"prompt\": \"Abstract colorful geometric patterns, modern art\",\n        \"num_inference_steps\": 35,\n        \"guidance_scale\": 6.0,\n        \"seed\": 456\n    }\n}\n\n# Descomente para salvar\n# metadata = save_generation_batch(prompts_para_salvar)\n</pre> import os from datetime import datetime  def save_generation_batch(prompts_dict, output_dir=\"generated_images\"):     \"\"\"     Salva um batch de gera\u00e7\u00f5es com metadados     \"\"\"     os.makedirs(output_dir, exist_ok=True)     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")      metadata = []      for name, config in prompts_dict.items():         print(f\"Gerando: {name}...\")          img = generate_images(**config)[0]          filename = f\"{timestamp}_{name}.png\"         filepath = os.path.join(output_dir, filename)         img.save(filepath)          metadata.append({             'name': name,             'file': filename,             'config': config         })          print(f\"  Salvo em: {filepath}\")      # Salvar metadados     import json     metadata_file = os.path.join(output_dir, f\"{timestamp}_metadata.json\")     with open(metadata_file, 'w') as f:         json.dump(metadata, f, indent=2)      print(f\"\\nMetadados salvos em: {metadata_file}\")     return metadata  # Exemplo de uso prompts_para_salvar = {     \"landscape\": {         \"prompt\": \"Beautiful mountain landscape at sunset, photorealistic\",         \"num_inference_steps\": 40,         \"guidance_scale\": 7.5,         \"seed\": 42     },     \"portrait\": {         \"prompt\": \"Professional portrait of a scientist in laboratory\",         \"num_inference_steps\": 50,         \"guidance_scale\": 8.0,         \"seed\": 123     },     \"abstract\": {         \"prompt\": \"Abstract colorful geometric patterns, modern art\",         \"num_inference_steps\": 35,         \"guidance_scale\": 6.0,         \"seed\": 456     } }  # Descomente para salvar # metadata = save_generation_batch(prompts_para_salvar) In\u00a0[\u00a0]: Copied! <pre>print(\"=\"*80)\nprint(\"RESUMO DO PROJETO - STABLE DIFFUSION COM DIFFUSERS\")\nprint(\"=\"*80)\n\nsummary = \"\"\"\nIMPLEMENTA\u00c7\u00d5ES REALIZADAS:\n--------------------------\n1. TEXT-TO-IMAGE: Pipeline principal com Stable Diffusion v1.5\n2. IMAGE-TO-IMAGE: Transforma\u00e7\u00e3o de imagens existentes com prompts\n\nARQUITETURA EXPLORADA:\n----------------------\n- Text Encoder (CLIP): Converte prompts em embeddings sem\u00e2nticos\n- U-Net: Realiza o processo de difus\u00e3o/denoising iterativo\n- VAE: Codifica/decodifica entre espa\u00e7o latente e pixels\n- Scheduler: Controla o processo de remo\u00e7\u00e3o de ru\u00eddo\n\nPAR\u00c2METROS ANALISADOS:\n----------------------\n- Guidance Scale: Controla fidelidade ao prompt (2-15)\n- Inference Steps: N\u00famero de itera\u00e7\u00f5es de denoising (20-100)\n- Strength (img2img): Intensidade da transforma\u00e7\u00e3o (0-1)\n- Seed: Controle de reprodutibilidade\n- Negative Prompt: Elementos a evitar na gera\u00e7\u00e3o\n\nEXEMPLOS DEMONSTRADOS:\n----------------------\n\u2713 5+ varia\u00e7\u00f5es de guidance scale\n\u2713 5+ varia\u00e7\u00f5es de inference steps\n\u2713 5+ estilos art\u00edsticos diferentes\n\u2713 5+ exemplos de negative prompts\n\u2713 5+ seeds diferentes\n\u2713 5+ transforma\u00e7\u00f5es image-to-image\n\nOTIMIZA\u00c7\u00d5ES APLICADAS:\n----------------------\n- Float16 para economia de mem\u00f3ria\n- Attention slicing para GPUs com menos VRAM\n- Cache de modelos para reutiliza\u00e7\u00e3o\n\"\"\"\n\nprint(summary)\n\n# Estat\u00edsticas finais\ntotal_params = sum(p.numel() for p in pipe.unet.parameters())\ntotal_params += sum(p.numel() for p in pipe.vae.parameters())\ntotal_params += sum(p.numel() for p in pipe.text_encoder.parameters())\n\nprint(f\"\\nTOTAL DE PAR\u00c2METROS NO MODELO: {total_params:,} ({total_params/1e9:.2f}B)\")\nprint(f\"MEM\u00d3RIA GPU UTILIZADA: ~4-6 GB em float16\")\nprint(f\"TEMPO M\u00c9DIO POR IMAGEM (50 steps): ~10-30 segundos (varia com GPU)\")\n</pre> print(\"=\"*80) print(\"RESUMO DO PROJETO - STABLE DIFFUSION COM DIFFUSERS\") print(\"=\"*80)  summary = \"\"\" IMPLEMENTA\u00c7\u00d5ES REALIZADAS: -------------------------- 1. TEXT-TO-IMAGE: Pipeline principal com Stable Diffusion v1.5 2. IMAGE-TO-IMAGE: Transforma\u00e7\u00e3o de imagens existentes com prompts  ARQUITETURA EXPLORADA: ---------------------- - Text Encoder (CLIP): Converte prompts em embeddings sem\u00e2nticos - U-Net: Realiza o processo de difus\u00e3o/denoising iterativo - VAE: Codifica/decodifica entre espa\u00e7o latente e pixels - Scheduler: Controla o processo de remo\u00e7\u00e3o de ru\u00eddo  PAR\u00c2METROS ANALISADOS: ---------------------- - Guidance Scale: Controla fidelidade ao prompt (2-15) - Inference Steps: N\u00famero de itera\u00e7\u00f5es de denoising (20-100) - Strength (img2img): Intensidade da transforma\u00e7\u00e3o (0-1) - Seed: Controle de reprodutibilidade - Negative Prompt: Elementos a evitar na gera\u00e7\u00e3o  EXEMPLOS DEMONSTRADOS: ---------------------- \u2713 5+ varia\u00e7\u00f5es de guidance scale \u2713 5+ varia\u00e7\u00f5es de inference steps \u2713 5+ estilos art\u00edsticos diferentes \u2713 5+ exemplos de negative prompts \u2713 5+ seeds diferentes \u2713 5+ transforma\u00e7\u00f5es image-to-image  OTIMIZA\u00c7\u00d5ES APLICADAS: ---------------------- - Float16 para economia de mem\u00f3ria - Attention slicing para GPUs com menos VRAM - Cache de modelos para reutiliza\u00e7\u00e3o \"\"\"  print(summary)  # Estat\u00edsticas finais total_params = sum(p.numel() for p in pipe.unet.parameters()) total_params += sum(p.numel() for p in pipe.vae.parameters()) total_params += sum(p.numel() for p in pipe.text_encoder.parameters())  print(f\"\\nTOTAL DE PAR\u00c2METROS NO MODELO: {total_params:,} ({total_params/1e9:.2f}B)\") print(f\"MEM\u00d3RIA GPU UTILIZADA: ~4-6 GB em float16\") print(f\"TEMPO M\u00c9DIO POR IMAGEM (50 steps): ~10-30 segundos (varia com GPU)\")"},{"location":"Projetos/Projeto3/notebook/#artificial-neural-networks-anns-project-3","title":"Artificial Neural Networks (ANNs) Project 3\u00b6","text":"<p>Made by: Jo\u00e3o Pedro Santos, Matheus Castellucci, Rodrigo Medeiros</p>"},{"location":"Projetos/Projeto3/notebook/#introducao","title":"Introdu\u00e7\u00e3o\u00b6","text":""},{"location":"Projetos/Projeto3/notebook/#o-que-e-stable-diffusion","title":"O que \u00e9 Stable Diffusion?\u00b6","text":"<p>Stable Diffusion \u00e9 um modelo de gera\u00e7\u00e3o de imagens baseado em difus\u00e3o latente (Latent Diffusion Model - LDM), desenvolvido pela Stability AI. Diferentemente de modelos como DALL-E, o Stable Diffusion \u00e9 open-source e pode ser executado em hardware mais acess\u00edvel.</p>"},{"location":"Projetos/Projeto3/notebook/#como-funciona","title":"Como Funciona?\u00b6","text":"<p>O processo de gera\u00e7\u00e3o segue estas etapas:</p> <ol> <li>Codifica\u00e7\u00e3o de Texto: O prompt de texto \u00e9 convertido em embeddings usando um modelo CLIP (Contrastive Language-Image Pre-training)</li> <li>Gera\u00e7\u00e3o de Ru\u00eddo Latente: Come\u00e7a-se com ru\u00eddo aleat\u00f3rio no espa\u00e7o latente (comprimido)</li> <li>Processo de Difus\u00e3o: O modelo U-Net remove iterativamente o ru\u00eddo, guiado pelos embeddings de texto</li> <li>Decodifica\u00e7\u00e3o: Um VAE (Variational Autoencoder) converte a representa\u00e7\u00e3o latente em uma imagem de alta resolu\u00e7\u00e3o</li> </ol>"},{"location":"Projetos/Projeto3/notebook/#objetivos-deste-projeto","title":"Objetivos deste Projeto\u00b6","text":"<p>Neste notebook, vamos:</p> <ul> <li>Explorar a arquitetura do Stable Diffusion usando a biblioteca <code>diffusers</code> da Hugging Face</li> <li>Entender os principais componentes: CLIP, U-Net, VAE e Scheduler</li> <li>Analisar o impacto de diferentes hiperpar\u00e2metros na qualidade das imagens geradas</li> <li>Experimentar com text-to-image e image-to-image generation</li> <li>Demonstrar t\u00e9cnicas de otimiza\u00e7\u00e3o para uso eficiente de mem\u00f3ria</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#requisitos","title":"Requisitos\u00b6","text":"<ul> <li>Python 3.8+</li> <li>PyTorch com suporte CUDA (recomendado) ou CPU</li> <li>Biblioteca <code>diffusers</code> da Hugging Face</li> <li>~4-6 GB de VRAM (GPU) ou ~8 GB de RAM (CPU)</li> <li>Conex\u00e3o \u00e0 internet para download dos modelos pr\u00e9-treinados</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#1-configuracao-do-ambiente","title":"1. Configura\u00e7\u00e3o do Ambiente\u00b6","text":"<p>Nesta se\u00e7\u00e3o, vamos configurar o ambiente necess\u00e1rio para executar o Stable Diffusion. Isso inclui:</p> <ul> <li>Importa\u00e7\u00e3o de bibliotecas: PyTorch, Diffusers, e ferramentas de visualiza\u00e7\u00e3o</li> <li>Verifica\u00e7\u00e3o de hardware: Detectar se temos GPU dispon\u00edvel (CUDA)</li> <li>Configura\u00e7\u00e3o de device: Usar GPU se dispon\u00edvel, caso contr\u00e1rio CPU</li> </ul> <p>Nota sobre performance:</p> <ul> <li>Com GPU (CUDA): ~10-30 segundos por imagem</li> <li>Com CPU: ~5-10 minutos por imagem</li> </ul> <p>Para melhor experi\u00eancia, recomenda-se usar Google Colab com GPU gratuita.</p>"},{"location":"Projetos/Projeto3/notebook/#2-carregamento-do-modelo-stable-diffusion","title":"2. Carregamento do Modelo Stable Diffusion\u00b6","text":"<p>Agora vamos carregar o modelo Stable Diffusion v1.5 da Runway ML. Este \u00e9 um dos modelos mais populares e est\u00e1 dispon\u00edvel gratuitamente.</p>"},{"location":"Projetos/Projeto3/notebook/#caracteristicas-do-modelo-v15","title":"Caracter\u00edsticas do Modelo v1.5:\u00b6","text":"<ul> <li>Resolu\u00e7\u00e3o padr\u00e3o: 512x512 pixels</li> <li>Tamanho do modelo: ~4 GB</li> <li>Par\u00e2metros totais: ~860 milh\u00f5es</li> <li>Licen\u00e7a: CreativeML OpenRAIL-M (uso livre com restri\u00e7\u00f5es \u00e9ticas)</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#otimizacoes-aplicadas","title":"Otimiza\u00e7\u00f5es Aplicadas:\u00b6","text":"<ol> <li>torch_dtype: Usamos <code>float16</code> em GPU para reduzir uso de mem\u00f3ria pela metade</li> <li>safety_checker=None: Desabilitamos o filtro de conte\u00fado para economizar mem\u00f3ria (use com responsabilidade)</li> <li>attention_slicing: Permite processar aten\u00e7\u00e3o em blocos menores (\u00fatil para GPUs com pouca VRAM)</li> </ol> <p>Primeira execu\u00e7\u00e3o: O modelo ser\u00e1 baixado do Hugging Face Hub (~4 GB). Execu\u00e7\u00f5es subsequentes usar\u00e3o o cache local.</p>"},{"location":"Projetos/Projeto3/notebook/#3-arquitetura-do-stable-diffusion-pipeline","title":"3. Arquitetura do Stable Diffusion Pipeline\u00b6","text":"<p>O Stable Diffusion \u00e9 composto por v\u00e1rios componentes que trabalham juntos. Vamos explorar cada um deles:</p>"},{"location":"Projetos/Projeto3/notebook/#componentes-principais","title":"Componentes Principais:\u00b6","text":""},{"location":"Projetos/Projeto3/notebook/#1-text-encoder-clip","title":"1. Text Encoder (CLIP)\u00b6","text":"<ul> <li>Converte texto em representa\u00e7\u00f5es num\u00e9ricas (embeddings)</li> <li>Baseado no modelo CLIP da OpenAI</li> <li>Produz vetores de 768 dimens\u00f5es que capturam o significado sem\u00e2ntico do prompt</li> <li>Limitado a 77 tokens por prompt</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#2-tokenizer","title":"2. Tokenizer\u00b6","text":"<ul> <li>Processa o texto de entrada, dividindo-o em tokens</li> <li>Vocabul\u00e1rio de ~49,000 tokens</li> <li>Lida com palavras, subpalavras e caracteres especiais</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#3-u-net","title":"3. U-Net\u00b6","text":"<ul> <li>Cora\u00e7\u00e3o do modelo de difus\u00e3o</li> <li>Arquitetura de encoder-decoder com conex\u00f5es skip</li> <li>Remove ru\u00eddo iterativamente do espa\u00e7o latente</li> <li>~860 milh\u00f5es de par\u00e2metros</li> <li>Recebe como entrada: imagem com ru\u00eddo + embeddings de texto</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#4-vae-variational-autoencoder","title":"4. VAE (Variational Autoencoder)\u00b6","text":"<ul> <li>Encoder: Comprime imagens 512x512 para espa\u00e7o latente 64x64 (redu\u00e7\u00e3o de 8x)</li> <li>Decoder: Reconstr\u00f3i imagens do espa\u00e7o latente para pixels</li> <li>Permite trabalhar com representa\u00e7\u00f5es compactas, economizando mem\u00f3ria e computa\u00e7\u00e3o</li> <li>4 canais no espa\u00e7o latente</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#5-scheduler","title":"5. Scheduler\u00b6","text":"<ul> <li>Controla o processo de denoising</li> <li>Define como o ru\u00eddo \u00e9 removido em cada step</li> <li>Diferentes schedulers (DDPM, DDIM, DPM-Solver) afetam qualidade e velocidade</li> <li>1000 timesteps de treinamento, mas pode usar menos na infer\u00eancia</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#fluxo-de-geracao-text-to-image","title":"Fluxo de Gera\u00e7\u00e3o (Text-to-Image):\u00b6","text":"<pre><code>Texto \u2192 Tokenizer \u2192 Tokens \u2192 CLIP Encoder \u2192 Text Embeddings (768D)\n                                                      \u2193\nRu\u00eddo Aleat\u00f3rio (64x64x4) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 U-Net (50 steps) \u2192 Latente Final\n                                                      \u2193\n                                            VAE Decoder \u2192 Imagem (512x512)\n</code></pre> <p>A seguir, vamos executar uma fun\u00e7\u00e3o que exibe detalhes de cada componente:</p>"},{"location":"Projetos/Projeto3/notebook/#53-experimento-3-controle-de-estilos-artisticos","title":"5.3 Experimento 3: Controle de Estilos Art\u00edsticos\u00b6","text":"<p>Uma das capacidades mais impressionantes do Stable Diffusion \u00e9 gerar imagens no estilo de diferentes artistas ou movimentos art\u00edsticos, tudo atrav\u00e9s do prompt textual.</p> <p>T\u00e9cnica de Prompt Engineering:</p> <p>Adicionamos modificadores de estilo ao prompt base:</p> <ul> <li>\"photorealistic, 8k photography\" \u2192 Fotografia realista</li> <li>\"oil painting in the style of Van Gogh\" \u2192 Pintura impressionista</li> <li>\"japanese anime style, studio ghibli\" \u2192 Estilo anime</li> <li>\"pencil sketch, detailed drawing\" \u2192 Desenho a l\u00e1pis</li> <li>\"watercolor painting, soft colors\" \u2192 Aquarela</li> </ul> <p>Por que funciona?</p> <p>O modelo CLIP foi treinado com milh\u00f5es de pares imagem-texto da internet, aprendendo associa\u00e7\u00f5es entre descri\u00e7\u00f5es textuais e caracter\u00edsticas visuais. Ele \"entende\" conceitos como \"Van Gogh\", \"anime\", \"fotografia\", etc.</p> <p>Dica: Seja espec\u00edfico! Em vez de \"painting\", use \"oil painting\" ou \"watercolor painting\".</p>"},{"location":"Projetos/Projeto3/notebook/#4-funcao-de-geracao-de-imagens","title":"4. Fun\u00e7\u00e3o de Gera\u00e7\u00e3o de Imagens\u00b6","text":"<p>Agora vamos criar uma fun\u00e7\u00e3o auxiliar que facilita a gera\u00e7\u00e3o de imagens com diferentes par\u00e2metros.</p>"},{"location":"Projetos/Projeto3/notebook/#parametros-importantes","title":"Par\u00e2metros Importantes:\u00b6","text":"<ul> <li><p>prompt (str): Descri\u00e7\u00e3o textual do que voc\u00ea quer gerar</p> <ul> <li>Seja espec\u00edfico e detalhado</li> <li>Pode incluir estilo art\u00edstico, ilumina\u00e7\u00e3o, composi\u00e7\u00e3o, etc.</li> <li>Exemplo: \"A majestic lion wearing a crown, digital art, highly detailed, 4k\"</li> </ul> </li> <li><p>negative_prompt (str, opcional): O que voc\u00ea N\u00c3O quer na imagem</p> <ul> <li>Ajuda a evitar caracter\u00edsticas indesejadas</li> <li>Exemplo: \"blurry, low quality, distorted, ugly, bad anatomy\"</li> </ul> </li> <li><p>num_inference_steps (int, 20-100): N\u00famero de itera\u00e7\u00f5es de denoising</p> <ul> <li>Mais steps = melhor qualidade, mas mais lento</li> <li>20-30 steps: r\u00e1pido, qualidade aceit\u00e1vel</li> <li>40-50 steps: bom equil\u00edbrio qualidade/velocidade</li> <li>75-100 steps: m\u00e1xima qualidade, muito lento</li> </ul> </li> <li><p>guidance_scale (float, 1-20): For\u00e7a de ader\u00eancia ao prompt</p> <ul> <li>Valores baixos (1-5): mais criativo, mas pode ignorar o prompt</li> <li>Valores m\u00e9dios (7-9): equil\u00edbrio recomendado</li> <li>Valores altos (10-20): segue o prompt rigidamente, pode ficar saturado</li> </ul> </li> <li><p>seed (int, opcional): Semente para reprodutibilidade</p> <ul> <li>Mesma seed + mesmos par\u00e2metros = mesma imagem</li> <li>\u00datil para compara\u00e7\u00f5es e debugging</li> </ul> </li> <li><p>height/width (int, m\u00faltiplos de 8): Dimens\u00f5es da imagem</p> <ul> <li>Padr\u00e3o: 512x512 (treinado para isso)</li> <li>Outras resolu\u00e7\u00f5es funcionam, mas podem ter qualidade inferior</li> </ul> </li> </ul>"},{"location":"Projetos/Projeto3/notebook/#5-experimentos-analise-de-hiperparametros","title":"5. Experimentos: An\u00e1lise de Hiperpar\u00e2metros\u00b6","text":"<p>Agora vamos realizar uma s\u00e9rie de experimentos para entender como diferentes par\u00e2metros afetam a qualidade e caracter\u00edsticas das imagens geradas.</p>"},{"location":"Projetos/Projeto3/notebook/#51-experimento-1-impacto-do-guidance-scale","title":"5.1 Experimento 1: Impacto do Guidance Scale\u00b6","text":"<p>O Guidance Scale (tamb\u00e9m chamado de Classifier-Free Guidance) controla o quanto o modelo deve seguir o prompt textual.</p> <p>O que esperamos observar:</p> <ul> <li>Guidance baixo (2-5): Imagens mais criativas e variadas, mas podem n\u00e3o seguir o prompt fielmente</li> <li>Guidance m\u00e9dio (7-9): Equil\u00edbrio entre criatividade e fidelidade ao prompt</li> <li>Guidance alto (10-15): Imagens muito fi\u00e9is ao prompt, mas podem ficar supersaturadas ou artificiais</li> </ul> <p>Por que isso acontece?</p> <p>O guidance scale amplifica a diferen\u00e7a entre a predi\u00e7\u00e3o condicionada (com prompt) e n\u00e3o-condicionada (sem prompt). Valores altos for\u00e7am o modelo a seguir mais rigidamente as instru\u00e7\u00f5es do texto.</p>"},{"location":"Projetos/Projeto3/notebook/#52-experimento-2-numero-de-inference-steps-passos-de-denoising","title":"5.2 Experimento 2: N\u00famero de Inference Steps (Passos de Denoising)\u00b6","text":"<p>O n\u00famero de steps determina quantas itera\u00e7\u00f5es o modelo U-Net faz para remover o ru\u00eddo da imagem latente.</p> <p>O que esperamos observar:</p> <ul> <li>Poucos steps (10-20): Gera\u00e7\u00e3o r\u00e1pida, mas imagens podem ter menos detalhes ou artefatos</li> <li>Steps m\u00e9dios (30-50): Bom equil\u00edbrio entre qualidade e tempo</li> <li>Muitos steps (75-100): M\u00e1xima qualidade e refinamento, mas retorno diminui (lei de rendimentos decrescentes)</li> </ul> <p>Processo de Denoising:</p> <p>O modelo come\u00e7a com ru\u00eddo puro e gradualmente refina a imagem:</p> <ul> <li>Step 1-10: Forma geral e composi\u00e7\u00e3o</li> <li>Step 11-30: Detalhes principais e estrutura</li> <li>Step 31-50: Refinamento de texturas e detalhes finos</li> <li>Step 51-100: Ajustes sutis (ganho marginal)</li> </ul> <p>Trade-off: Mais steps = melhor qualidade, mas tempo de gera\u00e7\u00e3o aumenta linearmente.</p>"},{"location":"Projetos/Projeto3/notebook/#54-experimento-4-poder-do-negative-prompt","title":"5.4 Experimento 4: Poder do Negative Prompt\u00b6","text":"<p>O negative prompt \u00e9 uma ferramenta crucial para melhorar a qualidade das imagens, permitindo especificar o que voc\u00ea N\u00c3O quer na gera\u00e7\u00e3o.</p> <p>Como funciona tecnicamente?</p> <p>Durante o processo de Classifier-Free Guidance, o modelo calcula:</p> <pre><code>noise_pred = noise_unconditional + guidance_scale * (noise_conditional - noise_unconditional)\n</code></pre> <p>Com negative prompt, isso se torna:</p> <pre><code>noise_pred = noise_negative + guidance_scale * (noise_conditional - noise_negative)\n</code></pre> <p>Isso \"empurra\" a gera\u00e7\u00e3o para longe dos conceitos negativos.</p> <p>Negative Prompts Comuns:</p> <ul> <li>Qualidade geral: \"blurry, low quality, low resolution, pixelated\"</li> <li>Anatomia (pessoas): \"bad anatomy, extra limbs, deformed, disfigured\"</li> <li>Artefatos: \"watermark, text, signature, logo\"</li> <li>Estilo indesejado: \"cartoon, anime\" (se voc\u00ea quer realismo)</li> <li>Ilumina\u00e7\u00e3o: \"dark, poorly lit, overexposed\"</li> </ul> <p>Estrat\u00e9gia: Comece simples e adicione termos negativos conforme identifica problemas nas gera\u00e7\u00f5es.</p>"},{"location":"Projetos/Projeto3/notebook/#55-experimento-5-reprodutibilidade-com-seeds","title":"5.5 Experimento 5: Reprodutibilidade com Seeds\u00b6","text":"<p>A seed (semente) controla a gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios, permitindo reproduzir exatamente a mesma imagem.</p> <p>Como funciona:</p> <ol> <li>A seed inicializa o gerador de ru\u00eddo aleat\u00f3rio</li> <li>O ru\u00eddo inicial latente (64x64x4) \u00e9 gerado pseudo-aleatoriamente</li> <li>Mesma seed + mesmos par\u00e2metros = mesmo ru\u00eddo inicial = mesma imagem final</li> </ol> <p>Aplica\u00e7\u00f5es pr\u00e1ticas:</p> <ul> <li>Compara\u00e7\u00f5es: Testar o efeito de mudar apenas um par\u00e2metro (como guidance scale)</li> <li>Refinamento: Gerar v\u00e1rias vers\u00f5es com seeds diferentes, escolher a melhor, e regenerar com ajustes</li> <li>Reprodu\u00e7\u00e3o: Compartilhar seeds com outros para recriar imagens exatas</li> <li>Debugging: Isolar problemas mantendo tudo constante exceto uma vari\u00e1vel</li> </ul> <p>Observa\u00e7\u00e3o: Sem especificar seed, cada gera\u00e7\u00e3o ser\u00e1 \u00fanica e aleat\u00f3ria.</p>"},{"location":"Projetos/Projeto3/notebook/#6-analise-de-performance","title":"6. An\u00e1lise de Performance\u00b6","text":"<p>\u00c9 importante entender o trade-off entre qualidade e tempo de gera\u00e7\u00e3o para otimizar seu workflow.</p> <p>Fatores que afetam o tempo:</p> <ol> <li>Hardware: GPU vs CPU (diferen\u00e7a de 10-30x)</li> <li>N\u00famero de steps: Rela\u00e7\u00e3o linear (50 steps \u2248 2x mais lento que 25 steps)</li> <li>Resolu\u00e7\u00e3o: Imagens maiores levam muito mais tempo</li> <li>Batch size: Gerar m\u00faltiplas imagens simultaneamente \u00e9 mais eficiente</li> </ol> <p>Benchmarks t\u00edpicos (GPU RTX 3060, 50 steps, 512x512):</p> <ul> <li>Stable Diffusion v1.5: ~3-5 segundos por imagem</li> <li>Stable Diffusion v2.1: ~4-6 segundos por imagem</li> <li>Stable Diffusion XL: ~10-15 segundos por imagem</li> </ul> <p>Otimiza\u00e7\u00f5es aplic\u00e1veis:</p> <ul> <li><code>enable_attention_slicing()</code>: Reduz uso de VRAM</li> <li><code>enable_vae_slicing()</code>: Processa VAE em tiles menores</li> <li><code>enable_xformers_memory_efficient_attention()</code>: Requer biblioteca xformers</li> <li>Float16 vs Float32: Reduz VRAM pela metade</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#7-salvamento-e-organizacao-de-imagens-geradas","title":"7. Salvamento e Organiza\u00e7\u00e3o de Imagens Geradas\u00b6","text":"<p>Para projetos maiores, \u00e9 \u00fatil ter um sistema de salvamento organizado com metadados.</p> <p>Sistema de Salvamento:</p> <p>A fun\u00e7\u00e3o <code>save_generation_batch</code> implementa:</p> <ol> <li>Organiza\u00e7\u00e3o: Cria diret\u00f3rio <code>generated_images/</code> automaticamente</li> <li>Timestamp: Adiciona data/hora aos nomes dos arquivos para evitar sobrescrita</li> <li>Metadados JSON: Salva todos os par\u00e2metros usados na gera\u00e7\u00e3o</li> <li>Rastreabilidade: Permite reproduzir exatamente qualquer imagem</li> </ol> <p>Estrutura dos metadados:</p> <pre>{\n  \"name\": \"landscape\",\n  \"file\": \"20240115_143022_landscape.png\",\n  \"config\": {\n    \"prompt\": \"Beautiful mountain landscape at sunset\",\n    \"num_inference_steps\": 40,\n    \"guidance_scale\": 7.5,\n    \"seed\": 42\n  }\n}\n</pre> <p>Benef\u00edcios:</p> <ul> <li>Documenta\u00e7\u00e3o autom\u00e1tica de experimentos</li> <li>Facilita compara\u00e7\u00f5es entre diferentes configura\u00e7\u00f5es</li> <li>Permite recriar imagens bem-sucedidas</li> <li>\u00datil para criar datasets ou portfolios</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#8-resumo-e-estatisticas-do-modelo","title":"8. Resumo e Estat\u00edsticas do Modelo\u00b6","text":"<p>Esta se\u00e7\u00e3o apresenta um resumo completo de tudo que foi implementado e explorado neste projeto.</p>"},{"location":"Projetos/Projeto3/notebook/#9-conclusoes-e-aprendizados","title":"9. Conclus\u00f5es e Aprendizados\u00b6","text":""},{"location":"Projetos/Projeto3/notebook/#principais-descobertas","title":"Principais Descobertas\u00b6","text":"<p>Atrav\u00e9s dos experimentos realizados neste projeto, aprendemos que:</p>"},{"location":"Projetos/Projeto3/notebook/#1-qualidade-vs-performance","title":"1. Qualidade vs Performance\u00b6","text":"<ul> <li>Existe um ponto ideal de equil\u00edbrio: 40-50 steps com guidance scale 7.5-8.0</li> <li>Al\u00e9m de 50 steps, os ganhos de qualidade s\u00e3o marginais</li> <li>Guidance scale acima de 12 tende a produzir imagens supersaturadas</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#2-importancia-do-prompt-engineering","title":"2. Import\u00e2ncia do Prompt Engineering\u00b6","text":"<ul> <li>Prompts espec\u00edficos e detalhados geram resultados muito melhores</li> <li>Modificadores de estilo (\"oil painting\", \"photorealistic\") s\u00e3o extremamente eficazes</li> <li>Negative prompts s\u00e3o essenciais para evitar artefatos comuns</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#3-arquitetura-do-modelo","title":"3. Arquitetura do Modelo\u00b6","text":"<ul> <li>O VAE reduz a computa\u00e7\u00e3o em 64x ao trabalhar em espa\u00e7o latente (64x64 vs 512x512)</li> <li>O U-Net \u00e9 o componente mais pesado (~860M par\u00e2metros)</li> <li>CLIP conecta linguagem e vis\u00e3o de forma surpreendentemente eficaz</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#4-reprodutibilidade","title":"4. Reprodutibilidade\u00b6","text":"<ul> <li>Seeds permitem controle total sobre gera\u00e7\u00f5es</li> <li>\u00datil para debugging e compara\u00e7\u00f5es cient\u00edficas</li> <li>Pequenas mudan\u00e7as no prompt podem causar grandes mudan\u00e7as na sa\u00edda</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#aplicacoes-praticas","title":"Aplica\u00e7\u00f5es Pr\u00e1ticas\u00b6","text":"<p>O Stable Diffusion pode ser usado para:</p> <ul> <li>Arte e Design: Concept art, ilustra\u00e7\u00f5es, designs de produtos</li> <li>Marketing: Gera\u00e7\u00e3o de imagens para campanhas, ads, social media</li> <li>Prototipagem: Mockups visuais, explora\u00e7\u00e3o de ideias</li> <li>Educa\u00e7\u00e3o: Visualiza\u00e7\u00e3o de conceitos abstratos</li> <li>Pesquisa: Estudos sobre IA generativa, bias, e representa\u00e7\u00e3o</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#limitacoes-observadas","title":"Limita\u00e7\u00f5es Observadas\u00b6","text":"<ul> <li>Anatomia humana: Ainda gera erros em m\u00e3os, dedos, e posturas complexas</li> <li>Texto em imagens: Geralmente produz texto ileg\u00edvel ou incorreto</li> <li>Detalhes finos: Pequenos objetos ou padr\u00f5es complexos podem ser inconsistentes</li> <li>Bias: Reflete biases presentes nos dados de treinamento</li> </ul>"},{"location":"Projetos/Projeto3/notebook/#proximos-passos","title":"Pr\u00f3ximos Passos\u00b6","text":"<p>Para expandir este projeto, considere:</p> <ol> <li>Image-to-Image: Transformar imagens existentes com prompts</li> <li>Inpainting: Editar partes espec\u00edficas de imagens</li> <li>ControlNet: Controle mais preciso com mapas de profundidade, edges, poses</li> <li>LoRA: Fine-tuning eficiente para estilos espec\u00edficos</li> <li>Stable Diffusion XL: Vers\u00e3o mais recente com melhor qualidade</li> </ol>"},{"location":"Projetos/Projeto3/notebook/#recursos-adicionais","title":"Recursos Adicionais\u00b6","text":"<ul> <li>Documenta\u00e7\u00e3o Diffusers</li> <li>Stable Diffusion Paper</li> <li>CLIP Paper</li> <li>Prompt Engineering Guide</li> </ul> <p>Projeto desenvolvido para o curso de Redes Neurais Artificiais Demonstrando compreens\u00e3o de modelos de difus\u00e3o latente e suas aplica\u00e7\u00f5es pr\u00e1ticas</p>"}]}