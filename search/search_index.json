{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ementa","text":"Teste GitHub Pages Teste com Lorem Ipsum <p>Esse \u00e9 um exemplo de p\u00e1gina simples para GitHub Pages.</p> Se\u00e7\u00e3o 1 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet felis in nunc fringilla ullamcorper. Proin non lacus vitae ligula pulvinar facilisis.</p> Se\u00e7\u00e3o 2 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vitae venenatis ligula. Ut malesuada augue nec mi tempor, eu malesuada libero hendrerit.</p> Se\u00e7\u00e3o 3 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tincidunt, ipsum at sagittis tincidunt, risus ipsum cursus lorem, non dictum ipsum sapien quis elit.</p> <p>Rodap\u00e9 - P\u00e1gina de Teste</p>"},{"location":"Exercicios/EX1/data/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class</p> In\u00a0[2]: Copied! <pre>np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.\n\n# M\u00e9dias e desvios para cada classe\nmeans = [(2, 3), (5, 6), (8, 1), (15, 4)]\nstds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]\n\ndata = []\nlabels = []\n\nfor i in range(len(means)):\n    mean = means[i]\n    std = stds[i]\n\n    x = np.random.normal(loc=mean[0], scale=std[0], size=100)\n    y = np.random.normal(loc=mean[1], scale=std[1], size=100)\n\n    points = []\n    for j in range(100):\n        points.append([x[j], y[j]])\n\n    data.extend(points)\n\n    for j in range(100):\n        labels.append(i)\n\ndata = np.array(data)\nlabels = np.array(labels)\n</pre> np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.  # M\u00e9dias e desvios para cada classe means = [(2, 3), (5, 6), (8, 1), (15, 4)] stds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]  data = [] labels = []  for i in range(len(means)):     mean = means[i]     std = stds[i]      x = np.random.normal(loc=mean[0], scale=std[0], size=100)     y = np.random.normal(loc=mean[1], scale=std[1], size=100)      points = []     for j in range(100):         points.append([x[j], y[j]])      data.extend(points)      for j in range(100):         labels.append(i)  data = np.array(data) labels = np.array(labels) <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nfor cls in range(4):\n    plt.scatter(data[labels == cls, 0],\n                data[labels == cls, 1],\n                label=f'Classe {cls}',\n                alpha=0.7)\nplt.legend()\nplt.xlabel('Eixo X')\nplt.ylabel('Eixo Y')\nplt.title('Dispers\u00e3o das 4 classes em 2D')\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) for cls in range(4):     plt.scatter(data[labels == cls, 0],                 data[labels == cls, 1],                 label=f'Classe {cls}',                 alpha=0.7) plt.legend() plt.xlabel('Eixo X') plt.ylabel('Eixo Y') plt.title('Dispers\u00e3o das 4 classes em 2D') plt.show() <p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p>Answers</p> <ol> <li>O scatter plot mostra que as quatro classes est\u00e3o distribuidas de forma relativamente clara, com alguma sobreposi\u00e7\u00e3o entre elas. As classes 0 e 1 est\u00e3o mais pr\u00f3ximas uma da outra, enquanto as classes 2 ainda encosta um pouco na classe 1, e longe de todas as outras temos a classe 3.</li> <li>N\u00e3o, uma fronteira linear simples n\u00e3o seria capaz de separar todas as classes de forma eficaz, especialmente devido \u00e0 sobreposi\u00e7\u00e3o entre as classes 0 e 1.</li> <li>Pode ser visto no gr\u00e1fico abaixo:</li> </ol> <p></p> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B.</p> In\u00a0[4]: Copied! <pre>mu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1.0]])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],\n                    [-0.7, 1.5,  0.4, 0.0, 0.0],\n                    [0.2,  0.4,  1.5, 0.6, 0.0],\n                    [0.0,  0.0,  0.6, 1.5, 0.3],\n                    [0.0,  0.0,  0.0, 0.3, 1.5]])\n\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)\n</pre> mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],                     [0.8, 1.0, 0.3, 0.0, 0.0],                     [0.1, 0.3, 1.0, 0.5, 0.0],                     [0.0, 0.0, 0.5, 1.0, 0.2],                     [0.0, 0.0, 0.0, 0.2, 1.0]])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],                     [-0.7, 1.5,  0.4, 0.0, 0.0],                     [0.2,  0.4,  1.5, 0.6, 0.0],                     [0.0,  0.0,  0.6, 1.5, 0.3],                     [0.0,  0.0,  0.0, 0.3, 1.5]])  XA = np.random.multivariate_normal(mu_A, Sigma_A, size=500) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack([XA, XB]) y = np.array([0]*500 + [1]*500) <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>PCA:</p> <ol> <li>Centralizar os dados (tirar a m\u00e9dia).</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Extrair autovalores e autovetores da matriz de covari\u00e2ncia.</li> <li>Ordenar autovetores pelos maiores autovalores.</li> <li>Projetar os dados nos autovetores escolhidos.</li> </ol> In\u00a0[5]: Copied! <pre>def my_pca(X, n_components=None):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered, rowvar=False)\n    autovalores, autovetores = np.linalg.eigh(cov_matrix)\n\n    sorted_idx = np.argsort(autovalores)[::-1]\n    autovalores = autovalores[sorted_idx]\n    autovetores = autovetores[:, sorted_idx]\n\n    total_var = np.sum(autovalores)\n\n    if n_components is not None:\n        autovetores = autovetores[:, :n_components]\n        autovalores = autovalores[:n_components]\n\n    X_pca = np.dot(X_centered, autovetores)\n\n    return X_pca, autovetores, autovalores, total_var\n</pre> def my_pca(X, n_components=None):     X_centered = X - np.mean(X, axis=0)     cov_matrix = np.cov(X_centered, rowvar=False)     autovalores, autovetores = np.linalg.eigh(cov_matrix)      sorted_idx = np.argsort(autovalores)[::-1]     autovalores = autovalores[sorted_idx]     autovetores = autovetores[:, sorted_idx]      total_var = np.sum(autovalores)      if n_components is not None:         autovetores = autovetores[:, :n_components]         autovalores = autovalores[:n_components]      X_pca = np.dot(X_centered, autovetores)      return X_pca, autovetores, autovalores, total_var In\u00a0[6]: Copied! <pre>Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\nplt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)')\nplt.legend(); plt.show()\n\nprint('Vari\u00e2ncia explicada:', autovalores / total_var)\nprint('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var))\n</pre> Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)  plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A') plt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B') plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)') plt.legend(); plt.show()  print('Vari\u00e2ncia explicada:', autovalores / total_var) print('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var)) <pre>Vari\u00e2ncia explicada: [0.52303265 0.15751841]\nVari\u00e2ncia Acumulada: 0.6805510605944183\n</pre> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> <p>Answers</p> <ol> <li>As duas classes est\u00e3o bem pr\u00f3ximas uma da outra, com uma quantidade significativa de sobreposi\u00e7\u00e3o. Caso n\u00e3o fossem duas classes diferentes, poder\u00edamos considerar que se tratam de uma \u00fanica classe.</li> <li>N\u00e3o \u00e9 possivel tra\u00e7ar uma linha que separa as duas classes de forma eficaz, sempre haver\u00e1 uma \u00e1rea de sobreposi\u00e7\u00e3o. Modelos lineares simples, como um Perceptron ou Regress\u00e3o Log\u00edstica, v\u00e3o ter dificuldade porque s\u00f3 conseguem aprender fronteiras lineares (hiperplanos). Eles errariam bastante nos pontos da regi\u00e3o central de overlap.</li> </ol> <p>2. Describe the Data:</p> <ol> <li>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</li> <li>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ol> <p>Answers:</p> <ol> <li><p>O dataset busca prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da Spaceship Titanic. Isso pode ser visto na coluna \"Transported\", que \u00e9 a coluna-alvo (sendo booleano).</p> </li> <li><p>Features: num\u00e9ricas vs categ\u00f3ricas</p> <p>Num\u00e9ricas</p> <ul> <li><p>Age \u2192 idade do passageiro</p> </li> <li><p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 valores gastos em diferentes servi\u00e7os</p> </li> </ul> <p>Categ\u00f3ricas</p> <ul> <li><p>HomePlanet \u2192 planeta de origem (ex: Earth, Europa, Mars)</p> </li> <li><p>CryoSleep \u2192 booleano (se o passageiro entrou em sono criog\u00eanico)</p> </li> <li><p>Cabin \u2192 cont\u00e9m m\u00faltiplas infos (deck/num/side). Pode ser decomposta em:</p> <ul> <li><p>Deck (categ\u00f3rica)</p> </li> <li><p>Num (num\u00e9rica)</p> </li> <li><p>Side (P/S \u2192 categ\u00f3rica bin\u00e1ria)</p> </li> </ul> </li> <li><p>Destination \u2192 destino da viagem (ex: TRAPPIST-1e, etc.)</p> </li> <li><p>VIP \u2192 booleano (pagou servi\u00e7o VIP)</p> </li> <li><p>Name \u2192 geralmente descartado (n\u00e3o tem valor preditivo direto)</p> </li> <li><p>PassengerId \u2192 identificador \u00fanico (n\u00e3o usado como feature)</p> </li> </ul> </li> <li><p>Colunas com valores faltantes:</p> </li> </ol> In\u00a0[7]: Copied! <pre>csv_path = \"train.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\nprint(\"Colunas:\", list(df.columns))\n</pre> csv_path = \"train.csv\" df = pd.read_csv(csv_path) print(\"Shape:\", df.shape) print(\"Colunas:\", list(df.columns)) <pre>Shape: (8693, 14)\nColunas: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n</pre> In\u00a0[8]: Copied! <pre>missing = df.isnull().sum()\nmissing_percent = 100 * missing / len(df)\nmissing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent})\nprint(missing_df[missing_df[\"Missing Values\"] &gt; 0])\n</pre> missing = df.isnull().sum() missing_percent = 100 * missing / len(df) missing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent}) print(missing_df[missing_df[\"Missing Values\"] &gt; 0]) <pre>              Missing Values   Percent\nHomePlanet               201  2.312205\nCryoSleep                217  2.496261\nCabin                    199  2.289198\nDestination              182  2.093639\nAge                      179  2.059128\nVIP                      203  2.335212\nRoomService              181  2.082135\nFoodCourt                183  2.105142\nShoppingMall             208  2.392730\nSpa                      183  2.105142\nVRDeck                   188  2.162660\nName                     200  2.300702\n</pre> <p>3. Preprocessing the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ol> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ol> In\u00a0[9]: Copied! <pre>df_processed = df.copy()\n\n\"\"\"Num\u00e9ricas: \nSolu\u00e7\u00e3o: Preencher com mediana, \nJustificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o.\n\"\"\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    median_val = df_processed[col].median()\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(median_val, inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")\n</pre> df_processed = df.copy()  \"\"\"Num\u00e9ricas:  Solu\u00e7\u00e3o: Preencher com mediana,  Justificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o. \"\"\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] for col in num_cols:     median_val = df_processed[col].median()     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(median_val, inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")  <pre>   - Age: 179 valores preenchidos com 27.00\n   - RoomService: 181 valores preenchidos com 0.00\n   - FoodCourt: 183 valores preenchidos com 0.00\n   - ShoppingMall: 208 valores preenchidos com 0.00\n   - Spa: 183 valores preenchidos com 0.00\n   - VRDeck: 188 valores preenchidos com 0.00\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\n</pre> In\u00a0[10]: Copied! <pre>\"\"\"Categ\u00f3ricas: \nSolu\u00e7\u00e3o: Preencher com \"Unknown\"\nJustificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios.\n\"\"\"\ncat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]\nfor col in cat_cols:\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(\"Unknown\", inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos\")\n</pre> \"\"\"Categ\u00f3ricas:  Solu\u00e7\u00e3o: Preencher com \"Unknown\" Justificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios. \"\"\" cat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"] for col in cat_cols:     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(\"Unknown\", inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos\")  <pre>   - HomePlanet: 201 valores preenchidos\n   - Destination: 182 valores preenchidos\n   - CryoSleep: 217 valores preenchidos\n   - VIP: 203 valores preenchidos\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\797796973.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(\"Unknown\", inplace=True)\n</pre> In\u00a0[11]: Copied! <pre>\"\"\"Cabin: \nSolu\u00e7\u00e3o: Separar em 3 colunas\nJustificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes.\n\"\"\"\ncabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True)\ndf_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\")\ndf_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")\n\n# Preencher CabinNum com mediana\ncabin_num_median = df_processed[\"CabinNum\"].median()\ndf_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> \"\"\"Cabin:  Solu\u00e7\u00e3o: Separar em 3 colunas Justificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes. \"\"\" cabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True) df_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\") df_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\") df_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")  # Preencher CabinNum com mediana cabin_num_median = df_processed[\"CabinNum\"].median() df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True) <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_16728\\2304158112.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> In\u00a0[12]: Copied! <pre>\"\"\"Name: \nSolu\u00e7\u00e3o: descartar\nJustificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido.\n\"\"\"\ndf_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)    \n\nprint(\"Ap\u00f3s tratamento:\", df_processed.shape)\n</pre> \"\"\"Name:  Solu\u00e7\u00e3o: descartar Justificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido. \"\"\" df_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)      print(\"Ap\u00f3s tratamento:\", df_processed.shape) <pre>Ap\u00f3s tratamento: (8693, 14)\n</pre> In\u00a0[13]: Copied! <pre>df_encoded = df_processed.copy()\n\n# Mapeamento booleano/tri-estado\nboolean_mappings = {\n    \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},\n    \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1}\n}\nfor col, mapping in boolean_mappings.items():\n    if col in df_encoded.columns:\n        df_encoded[col] = df_encoded[col].astype(str).map(mapping)\n        print(f\"   - {col}: {mapping}\")\n\n# One-hot para categ\u00f3ricas\ncategorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns]\nfor col in categorical_cols:\n    unique_values = df_encoded[col].astype(str).unique()\n    print(f\"   - {col}: {len(unique_values)} categorias\")\n\ndf_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False)\nprint(\"Ap\u00f3s encoding:\", df_encoded.shape)\n</pre> df_encoded = df_processed.copy()  # Mapeamento booleano/tri-estado boolean_mappings = {     \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},     \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1} } for col, mapping in boolean_mappings.items():     if col in df_encoded.columns:         df_encoded[col] = df_encoded[col].astype(str).map(mapping)         print(f\"   - {col}: {mapping}\")  # One-hot para categ\u00f3ricas categorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns] for col in categorical_cols:     unique_values = df_encoded[col].astype(str).unique()     print(f\"   - {col}: {len(unique_values)} categorias\")  df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False) print(\"Ap\u00f3s encoding:\", df_encoded.shape) <pre>   - CryoSleep: {'True': 1, 'False': 0, 'Unknown': -1}\n   - VIP: {'True': 1, 'False': 0, 'Unknown': -1}\n   - HomePlanet: 4 categorias\n   - Destination: 4 categorias\n   - Deck: 9 categorias\n   - Side: 3 categorias\nAp\u00f3s encoding: (8693, 30)\n</pre> In\u00a0[14]: Copied! <pre>print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\")\nprint(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")\n\ndef minmax_scale_to_neg1_pos1(series):\n    return 2 * ((series - series.min()) / (series.max() - series.min())) - 1\n\ndf_normalized = df_encoded.copy()\noriginal_data = {}\n\nscaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns]\nfor col in scaling_cols:\n    original_data[col] = df_normalized[col].copy()\n    original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"\n    df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])\n    normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"\n    print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")\n\nprint(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape)\n</pre> print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\") print(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")  def minmax_scale_to_neg1_pos1(series):     return 2 * ((series - series.min()) / (series.max() - series.min())) - 1  df_normalized = df_encoded.copy() original_data = {}  scaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns] for col in scaling_cols:     original_data[col] = df_normalized[col].copy()     original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"     df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])     normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"     print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")  print(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape) <pre>\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\nM\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\n   - Age: [0.0, 79.0] \u2192 [-1.000, 1.000]\n   - RoomService: [0.0, 14327.0] \u2192 [-1.000, 1.000]\n   - FoodCourt: [0.0, 29813.0] \u2192 [-1.000, 1.000]\n   - ShoppingMall: [0.0, 23492.0] \u2192 [-1.000, 1.000]\n   - Spa: [0.0, 22408.0] \u2192 [-1.000, 1.000]\n   - VRDeck: [0.0, 24133.0] \u2192 [-1.000, 1.000]\n   - CabinNum: [0.0, 1894.0] \u2192 [-1.000, 1.000]\nAp\u00f3s normaliza\u00e7\u00e3o: (8693, 30)\n</pre> In\u00a0[15]: Copied! <pre>target_col = \"Transported\"\nX = df_normalized.drop(columns=[target_col]).values\ny = df_normalized[target_col].map({True: 1, False: 0}).values\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n</pre> target_col = \"Transported\" X = df_normalized.drop(columns=[target_col]).values y = df_normalized[target_col].map({True: 1, False: 0}).values print(\"X shape:\", X.shape, \"| y shape:\", y.shape) <pre>X shape: (8693, 29) | y shape: (8693,)\n</pre> <p>4. Visualize the Data:</p> <ul> <li>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation</li> </ul> In\u00a0[16]: Copied! <pre>for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:\n    plt.figure(figsize=(12, 5))\n\n    # Antes\n    plt.subplot(1, 2, 1)\n    plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    # Depois\n    plt.subplot(1, 2, 2)\n    plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (normalizado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:     plt.figure(figsize=(12, 5))      # Antes     plt.subplot(1, 2, 1)     plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (original)\")     plt.ylabel(\"Frequ\u00eancia\")      # Depois     plt.subplot(1, 2, 2)     plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (normalizado)\")     plt.ylabel(\"Frequ\u00eancia\")      plt.tight_layout()     plt.show()"},{"location":"Exercicios/EX1/data/#1-data","title":"1. Data\u00b6","text":""},{"location":"Exercicios/EX1/data/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks\u00b6","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"Exercicios/EX1/data/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/EX1/data/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/EX1/data/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Preparing Real-World Data for a Neural Network This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"Exercicios/EX2/perceptron/","title":"2. Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [1.5, 1.5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [5, 5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]].</li> </ul> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> In\u00a0[2]: Copied! <pre>n_por_classe=1000\nnp.random.seed(42)\n\nmean0 = np.array([1.5, 1.5])\nmean1 = np.array([5, 5])\n\ncov = np.array([[0.5, 0], \n                [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia\n\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\nidx = np.random.permutation(len(X))\nX, y = X[idx], y[idx]\n</pre> n_por_classe=1000 np.random.seed(42)  mean0 = np.array([1.5, 1.5]) mean1 = np.array([5, 5])  cov = np.array([[0.5, 0],                  [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia  X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  idx = np.random.permutation(len(X)) X, y = X[idx], y[idx] In\u00a0[3]: Copied! <pre>plt.figure(figsize=(6, 6))\nplt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0')\nplt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Dados 2D - duas classes')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(6, 6)) plt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0') plt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1') plt.xlabel('x1') plt.ylabel('x2') plt.title('Dados 2D - duas classes') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li><p>Initialize weights (w) as a 2D vector (plus a bias term b).</p> </li> <li><p>Use the perceptron learning rule: For each misclassified sample <code>(x, y)</code>, update <code>w = w + \u03b7 * y * x</code> and <code>b = b + \u03b7 * y</code>, where <code>\u03b7</code> is the learning rate (start with 0.1).</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</p> </li> <li><p>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by <code>w * x + b = 0</code>) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> In\u00a0[4]: Copied! <pre>y_pm1 = np.where(y == 1, 1, -1).astype(int)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\nprint(\"w inicial:\", w, \"b inicial:\", b)\n\n# --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) ---\nscores = X @ w + b\npreds = np.where(scores &gt;= 0.0, 1, -1)\n\nprint(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5])\n</pre> y_pm1 = np.where(y == 1, 1, -1).astype(int) w = np.zeros(2, dtype=float) b = 0.0  print(\"w inicial:\", w, \"b inicial:\", b)  # --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) --- scores = X @ w + b preds = np.where(scores &gt;= 0.0, 1, -1)  print(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5]) <pre>w inicial: [0. 0.] b inicial: 0.0\nprimeiras predi\u00e7\u00f5es (sem treino): [1 1 1 1 1]\n</pre> In\u00a0[5]: Copied! <pre># --- Checar se um ponto foi mal classificado ---\nx0 = X[0]\ny0 = y_pm1[0]\nmisclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0\nprint(\"primeiro ponto est\u00e1 errado?\", misclassified)\n\n# --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) ---\neta = 0.1\nif misclassified:\n    w = w + eta * y0 * x0\n    b = b + eta * y0\n\nprint(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b)\n</pre> # --- Checar se um ponto foi mal classificado --- x0 = X[0] y0 = y_pm1[0] misclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0 print(\"primeiro ponto est\u00e1 errado?\", misclassified)  # --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) --- eta = 0.1 if misclassified:     w = w + eta * y0 * x0     b = b + eta * y0  print(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b) <pre>primeiro ponto est\u00e1 errado? True\nw ap\u00f3s poss\u00edvel update: [0.51299906 0.69042624] b: 0.1\n</pre> In\u00a0[6]: Copied! <pre># Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# Hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\nnp.random.seed(42)\n\n# Inicializa\u00e7\u00e3o\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:            # misclassified (ou na margem)\n            w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso\n            b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s\n            updates += 1\n\n    # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)\n    scores = X @ w + b\n    y_pred = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred == y_pm1).mean()\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    if updates == 0:\n        print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")\n        break\n\n# Resultados finais\nfinal_epoch = len(accuracies)\nfinal_acc = accuracies[-1]\nprint(\"\\n--------- Finais ---------\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas:\", final_epoch)\nprint(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\")\nprint(\"--------------------------\")\n</pre> # Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1 y_pm1 = np.where(y == 1, 1, -1).astype(int)  # Hiperpar\u00e2metros eta = 0.01 max_epochs = 100 np.random.seed(42)  # Inicializa\u00e7\u00e3o w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:            # misclassified (ou na margem)             w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso             b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s             updates += 1      # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)     scores = X @ w + b     y_pred = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred == y_pm1).mean()     accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      if updates == 0:         print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")         break  # Resultados finais final_epoch = len(accuracies) final_acc = accuracies[-1] print(\"\\n--------- Finais ---------\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas:\", final_epoch) print(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\") print(\"--------------------------\")  <pre>\u00c9poca   1 | updates:   60 | acc: 0.9905\n\u00c9poca   2 | updates:   24 | acc: 0.9995\n\u00c9poca   3 | updates:    8 | acc: 0.9950\n\u00c9poca   4 | updates:    6 | acc: 1.0000\n\u00c9poca   5 | updates:    0 | acc: 1.0000\nConverg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\n\n--------- Finais ---------\nw: [0.0643648  0.04329078]\nb: -0.36000000000000015\n\u00e9pocas: 5\nacur\u00e1cia final: 1.0000\n--------------------------\n</pre> In\u00a0[7]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# --- 3A) Acur\u00e1cia por \u00e9poca ---\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca')\naxes[0].grid(True, alpha=0.3)\n\n# --- 3B) Fronteira de decis\u00e3o sobre os dados + erros ---\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\n# fronteira\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\n# erros\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1],\n                    s=40, marker='x', linewidths=1.5,\n                    label=f'Erros ({mis_idx.size})')\n\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(\"w final:\", w, \"| b final:\", b)\n</pre> fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # --- 3A) Acur\u00e1cia por \u00e9poca --- axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca') axes[0].grid(True, alpha=0.3)  # --- 3B) Fronteira de decis\u00e3o sobre os dados + erros --- scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  # fronteira x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  # erros if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1],                     s=40, marker='x', linewidths=1.5,                     label=f'Erros ({mis_idx.size})')  axes[1].set_title('Perceptron \u2014 Dados e Fronteira') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show()  print(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\") print(\"w final:\", w, \"| b final:\", b)  <pre>Total de pontos: 2000 | Erros: 0 | Acur\u00e1cia final: 1.0000\nw final: [0.0643648  0.04329078] | b final: -0.36000000000000015\n</pre> <p>Answer: O perceptron funciona muito bem quando h\u00e1 separabilidade linear com boa margem, convergindo r\u00e1pido e com fronteira simples. Logo, os dados gerados s\u00e3o ideais para o perceptron, que consegue encontrar uma fronteira linear eficaz. A baixa vari\u00e2ncia e a dist\u00e2ncia entre as m\u00e9dias das classes minimizam sobreposi\u00e7\u00f5es, facilitando a classifica\u00e7\u00e3o correta. Assim, o perceptron atinge alta acur\u00e1cia rapidamente, demonstrando sua efic\u00e1cia em cen\u00e1rios de separabilidade linear clara.</p> <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [3, 3],</li> <li>Covariance = [[1.5, 0], [0, 1.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [4, 4],</li> <li>Covariance = [[1.5, 0], [0, 1.5]].</li> </ul> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> In\u00a0[8]: Copied! <pre># ---- Par\u00e2metros do Ex.2 ----\nnp.random.seed(42)\nn_por_classe = 1000\n\nmean0 = np.array([3.0, 3.0])\nmean1 = np.array([4.0, 4.0])\n\ncov = np.array([[1.5, 0.0],\n                [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap\n\n# ---- Amostragem (sem fun\u00e7\u00f5es) ----\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\n# Embaralha para uso posterior\nidx = np.random.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</pre> # ---- Par\u00e2metros do Ex.2 ---- np.random.seed(42) n_por_classe = 1000  mean0 = np.array([3.0, 3.0]) mean1 = np.array([4.0, 4.0])  cov = np.array([[1.5, 0.0],                 [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap  # ---- Amostragem (sem fun\u00e7\u00f5es) ---- X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  # Embaralha para uso posterior idx = np.random.permutation(len(X)) X = X[idx] y = y[idx]  In\u00a0[9]: Copied! <pre># ---- Visualiza\u00e7\u00e3o: overlap entre classes ----\nplt.figure(figsize=(6,6))\nplt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\nplt.xlabel('x1'); plt.ylabel('x2')\nplt.title('Ex.2 \u2014 Dados 2D (overlap parcial)')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # ---- Visualiza\u00e7\u00e3o: overlap entre classes ---- plt.figure(figsize=(6,6)) plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8) plt.xlabel('x1'); plt.ylabel('x2') plt.title('Ex.2 \u2014 Dados 2D (overlap parcial)') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li><p>Follow the same initialization, update rule, and training process.</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</p> </li> <li><p>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> In\u00a0[10]: Copied! <pre>np.random.seed(42)\n\n# r\u00f3tulos em {-1,+1}\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\n\n# inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    # embaralhar a ordem a cada \u00e9poca\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra (regra do perceptron)\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:           # misclassified\n            w = w + eta * yi * xi\n            b = b + eta * yi\n            updates += 1\n\n    # acur\u00e1cia na \u00e9poca (dataset completo)\n    scores = X @ w + b\n    y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred_pm1 == y_pm1).mean()\n\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)\n    if updates == 0:\n        print(\"Parada por aus\u00eancia de updates (raro com overlap).\")\n        break\n\n# m\u00e9tricas finais\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\nprint(\"\\n--- Finais ---\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas executadas:\", len(accuracies))\nprint(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(f\"erros: {mis_idx.size} de {len(X)}\")\n\n# ---------- Plots lado a lado ----------\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# (A) acur\u00e1cia por \u00e9poca\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)')\naxes[0].grid(True, alpha=0.3)\n\n# (B) fronteira de decis\u00e3o + erros\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',\n                    linewidths=1.5, label=f'Erros ({mis_idx.size})')\n\naxes[1].set_xlabel('x1'); axes[1].set_ylabel('x2')\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n</pre> np.random.seed(42)  # r\u00f3tulos em {-1,+1} y_pm1 = np.where(y == 1, 1, -1).astype(int)  # hiperpar\u00e2metros eta = 0.01 max_epochs = 100  # inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser) w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     # embaralhar a ordem a cada \u00e9poca     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra (regra do perceptron)     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:           # misclassified             w = w + eta * yi * xi             b = b + eta * yi             updates += 1      # acur\u00e1cia na \u00e9poca (dataset completo)     scores = X @ w + b     y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred_pm1 == y_pm1).mean()      accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)     if updates == 0:         print(\"Parada por aus\u00eancia de updates (raro com overlap).\")         break  # m\u00e9tricas finais scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  print(\"\\n--- Finais ---\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas executadas:\", len(accuracies)) print(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\") print(f\"erros: {mis_idx.size} de {len(X)}\")  # ---------- Plots lado a lado ---------- fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # (A) acur\u00e1cia por \u00e9poca axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)') axes[0].grid(True, alpha=0.3)  # (B) fronteira de decis\u00e3o + erros axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',                     linewidths=1.5, label=f'Erros ({mis_idx.size})')  axes[1].set_xlabel('x1'); axes[1].set_ylabel('x2') axes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show() <pre>\u00c9poca   1 | updates:  849 | acc: 0.6915\n\u00c9poca   2 | updates:  768 | acc: 0.6670\n\u00c9poca   3 | updates:  816 | acc: 0.6550\n\u00c9poca   4 | updates:  799 | acc: 0.5845\n\u00c9poca   5 | updates:  768 | acc: 0.6940\n\u00c9poca   6 | updates:  780 | acc: 0.6590\n\u00c9poca   7 | updates:  799 | acc: 0.6025\n\u00c9poca   8 | updates:  764 | acc: 0.5430\n\u00c9poca   9 | updates:  784 | acc: 0.6835\n\u00c9poca  10 | updates:  757 | acc: 0.5690\n\u00c9poca  11 | updates:  743 | acc: 0.5015\n</pre> <pre>\u00c9poca  12 | updates:  755 | acc: 0.6795\n\u00c9poca  13 | updates:  748 | acc: 0.6630\n\u00c9poca  14 | updates:  752 | acc: 0.5005\n\u00c9poca  15 | updates:  777 | acc: 0.5320\n\u00c9poca  16 | updates:  737 | acc: 0.6680\n\u00c9poca  17 | updates:  773 | acc: 0.5640\n\u00c9poca  18 | updates:  790 | acc: 0.5865\n\u00c9poca  19 | updates:  790 | acc: 0.6645\n\u00c9poca  20 | updates:  798 | acc: 0.6375\n\u00c9poca  21 | updates:  782 | acc: 0.7050\n\u00c9poca  22 | updates:  768 | acc: 0.6740\n\u00c9poca  23 | updates:  750 | acc: 0.5000\n\u00c9poca  24 | updates:  776 | acc: 0.5000\n\u00c9poca  25 | updates:  741 | acc: 0.5000\n\u00c9poca  26 | updates:  762 | acc: 0.6285\n\u00c9poca  27 | updates:  778 | acc: 0.6725\n\u00c9poca  28 | updates:  770 | acc: 0.6980\n\u00c9poca  29 | updates:  720 | acc: 0.6210\n\u00c9poca  30 | updates:  761 | acc: 0.5010\n\u00c9poca  31 | updates:  776 | acc: 0.6595\n\u00c9poca  32 | updates:  761 | acc: 0.5620\n\u00c9poca  33 | updates:  775 | acc: 0.5000\n\u00c9poca  34 | updates:  772 | acc: 0.5000\n\u00c9poca  35 | updates:  761 | acc: 0.6105\n\u00c9poca  36 | updates:  779 | acc: 0.6980\n</pre> <pre>\u00c9poca  37 | updates:  778 | acc: 0.5815\n\u00c9poca  38 | updates:  742 | acc: 0.6785\n\u00c9poca  39 | updates:  785 | acc: 0.6925\n\u00c9poca  40 | updates:  749 | acc: 0.6985\n\u00c9poca  41 | updates:  746 | acc: 0.6550\n\u00c9poca  42 | updates:  799 | acc: 0.5900\n\u00c9poca  43 | updates:  780 | acc: 0.5145\n\u00c9poca  44 | updates:  745 | acc: 0.5010\n\u00c9poca  45 | updates:  770 | acc: 0.6335\n\u00c9poca  46 | updates:  784 | acc: 0.5845\n\u00c9poca  47 | updates:  789 | acc: 0.6975\n\u00c9poca  48 | updates:  767 | acc: 0.5185\n\u00c9poca  49 | updates:  751 | acc: 0.5820\n\u00c9poca  50 | updates:  759 | acc: 0.5000\n</pre> <pre>\u00c9poca  51 | updates:  762 | acc: 0.5020\n\u00c9poca  52 | updates:  753 | acc: 0.5230\n\u00c9poca  53 | updates:  765 | acc: 0.5780\n\u00c9poca  54 | updates:  778 | acc: 0.5505\n\u00c9poca  55 | updates:  728 | acc: 0.6365\n\u00c9poca  56 | updates:  780 | acc: 0.6035\n\u00c9poca  57 | updates:  789 | acc: 0.7045\n\u00c9poca  58 | updates:  761 | acc: 0.6790\n\u00c9poca  59 | updates:  755 | acc: 0.6195\n\u00c9poca  60 | updates:  755 | acc: 0.5770\n\u00c9poca  61 | updates:  789 | acc: 0.6650\n\u00c9poca  62 | updates:  768 | acc: 0.6520\n\u00c9poca  63 | updates:  776 | acc: 0.6225\n\u00c9poca  64 | updates:  768 | acc: 0.6635\n\u00c9poca  65 | updates:  800 | acc: 0.6935\n\u00c9poca  66 | updates:  780 | acc: 0.6580\n\u00c9poca  67 | updates:  773 | acc: 0.6560\n\u00c9poca  68 | updates:  768 | acc: 0.5915\n\u00c9poca  69 | updates:  777 | acc: 0.6385\n\u00c9poca  70 | updates:  762 | acc: 0.5000\n\u00c9poca  71 | updates:  811 | acc: 0.6360\n\u00c9poca  72 | updates:  761 | acc: 0.5865\n</pre> <pre>\u00c9poca  73 | updates:  762 | acc: 0.6370\n\u00c9poca  74 | updates:  767 | acc: 0.6595\n\u00c9poca  75 | updates:  771 | acc: 0.7005\n\u00c9poca  76 | updates:  775 | acc: 0.7005\n\u00c9poca  77 | updates:  783 | acc: 0.5265\n\u00c9poca  78 | updates:  786 | acc: 0.6995\n\u00c9poca  79 | updates:  763 | acc: 0.5000\n\u00c9poca  80 | updates:  773 | acc: 0.6415\n\u00c9poca  81 | updates:  748 | acc: 0.5685\n\u00c9poca  82 | updates:  738 | acc: 0.6575\n\u00c9poca  83 | updates:  793 | acc: 0.5815\n\u00c9poca  84 | updates:  764 | acc: 0.5745\n\u00c9poca  85 | updates:  779 | acc: 0.5870\n\u00c9poca  86 | updates:  765 | acc: 0.5395\n\u00c9poca  87 | updates:  787 | acc: 0.6665\n</pre> <pre>\u00c9poca  88 | updates:  770 | acc: 0.5475\n\u00c9poca  89 | updates:  753 | acc: 0.5015\n\u00c9poca  90 | updates:  779 | acc: 0.5735\n\u00c9poca  91 | updates:  801 | acc: 0.5005\n\u00c9poca  92 | updates:  787 | acc: 0.6385\n\u00c9poca  93 | updates:  747 | acc: 0.7025\n\u00c9poca  94 | updates:  793 | acc: 0.5675\n\u00c9poca  95 | updates:  768 | acc: 0.6485\n\u00c9poca  96 | updates:  778 | acc: 0.6695\n\u00c9poca  97 | updates:  760 | acc: 0.6520\n\u00c9poca  98 | updates:  777 | acc: 0.7025\n\u00c9poca  99 | updates:  784 | acc: 0.6280\n\u00c9poca 100 | updates:  807 | acc: 0.5035\n\n--- Finais ---\nw: [0.02953414 0.05292094]\nb: -0.5200000000000002\n\u00e9pocas executadas: 100\nacur\u00e1cia final: 0.5035\nerros: 993 de 2000\n</pre> <p>Answer: O overlap introduzido no Ex.2 mostra uma limita\u00e7\u00e3o fundamental do perceptron cl\u00e1ssico: ele s\u00f3 converge se os dados forem linearmente separ\u00e1veis. Em cen\u00e1rios mais realistas, com ru\u00eddo ou vari\u00e2ncia maior, o perceptron n\u00e3o converge e a acur\u00e1cia fica limitada. Isso motiva a ado\u00e7\u00e3o de variantes como o Perceptron com Margem, regress\u00e3o log\u00edstica, SVM ou redes neurais multicamadas, que lidam melhor com dados n\u00e3o separ\u00e1veis.</p>"},{"location":"Exercicios/EX2/perceptron/#2-perceptron","title":"2. Perceptron\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#activity-understanding-perceptrons-and-their-limitations","title":"Activity: Understanding Perceptrons and Their Limitations\u00b6","text":"<p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"Exercicios/EX2/perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/","title":"3. MLP","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): L = 1/N * (y - \u0177)\u00b2, where \u0177 is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li><p>Input and output vectors:</p> <ul> <li>X: [0.5, -0.2]</li> <li>Y: 1.0</li> </ul> </li> <li><p>Hidden layer weights:</p> <ul> <li>W\u00b9 = [[0.3, -0.1], [0.2, 0.4]]  (2x2 matrix)</li> </ul> </li> <li><p>Hidden layer biases:</p> <ul> <li>b\u00b9 = [0.1, -0.2]  (1x2 vector)</li> </ul> </li> <li><p>Output layer weights:</p> <ul> <li>W\u00b2 = [0.5, -0.3]</li> </ul> </li> <li><p>Output layer bias:</p> <ul> <li>b\u00b2 = 0.2</li> </ul> </li> <li><p>Learning rate:</p> <ul> <li>\u03b7 = 0.3</li> </ul> </li> <li><p>Activation function: tanh</p> </li> </ul> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li><p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: Z\u00b9 = W\u00b9 * X + b\u00b9.</li> <li>Apply tanh to get hidden activations: a\u00b9 = tanh(Z\u00b9).</li> <li>Compute the output pre-activation: Z\u00b2 = W\u00b2 * a\u00b9 + b\u00b2.</li> <li>Compute the final output: \u0177 = tanh(Z\u00b2).</li> </ul> </li> <li><p>Loss Calculation:</p> <ul> <li>Compute the loss: L = 1/N * (Y - \u0177)\u00b2.</li> </ul> </li> <li><p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with delL/del\u0177 then compute:</p> <ul> <li>delL/delZ\u00b2 (using the tanh derivative: del/delZ tanh(Z) = 1 - tanh\u00b2(Z)).</li> <li>Gradients for output layer: delL/delW\u00b2, delL/delb\u00b2.</li> <li>Propagate to hidden layer: delL/delA\u00b9, delL/delZ\u00b9.</li> <li>Gradients for hidden layer: delL/delW\u00b9, delL/delb\u00b9.</li> <li>Show all intermediate steps and calculations.</li> </ul> </li> <li><p>Parameter Update: Using the learning rate \u03b7 = 0.1, update all weights and biases via gradient descent:</p> <ul> <li>W\u00b2 &lt;- W\u00b2 - \u03b7 * delL/delW\u00b2</li> <li>b\u00b2 &lt;- b\u00b2 - \u03b7 * delL/delb\u00b2</li> <li>W\u00b9 &lt;- W\u00b9 - \u03b7 * delL/delW\u00b9</li> <li>b\u00b9 &lt;- b\u00b9 - \u03b7 * delL/delb\u00b9</li> <li>Provide the numerical values for all updated parameters.</li> </ul> </li> </ol> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p> In\u00a0[2]: Copied! <pre># --- Helpers ---\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(z):\n    return 1.0 - np.tanh(z)**2\n\ndef fmt(x):\n    if isinstance(x, float):\n        return f\"{x:.6f}\"\n    arr = np.array(x, dtype=float)\n    return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},\n                           floatmode='maxprec', suppress_small=False)\n\ndef p(title, value):\n    print(f\"{title}: {fmt(value)}\")\n\n# --- Dados do exerc\u00edcio ---\nX = np.array([0.5, -0.2], dtype=float)\nY = 1.0 \n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)\n\nb1 = np.array([0.1, -0.2], dtype=float)\n\nW2 = np.array([0.5, -0.3], dtype=float)\nb2 = 0.2\n\neta_update = 0.1\n\nprint(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\")\np(\"X\", X); p(\"Y\", Y)\nprint(\"\\n--- Par\u00e2metros iniciais ---\")\np(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)\n</pre> # --- Helpers --- def tanh(x):     return np.tanh(x)  def dtanh(z):     return 1.0 - np.tanh(z)**2  def fmt(x):     if isinstance(x, float):         return f\"{x:.6f}\"     arr = np.array(x, dtype=float)     return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},                            floatmode='maxprec', suppress_small=False)  def p(title, value):     print(f\"{title}: {fmt(value)}\")  # --- Dados do exerc\u00edcio --- X = np.array([0.5, -0.2], dtype=float) Y = 1.0   W1 = np.array([[0.3, -0.1],                [0.2,  0.4]], dtype=float)  b1 = np.array([0.1, -0.2], dtype=float)  W2 = np.array([0.5, -0.3], dtype=float) b2 = 0.2  eta_update = 0.1  print(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\") p(\"X\", X); p(\"Y\", Y) print(\"\\n--- Par\u00e2metros iniciais ---\") p(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)  <pre>=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\n\nX: [0.500000 -0.200000]\nY: 1.000000\n\n--- Par\u00e2metros iniciais ---\nW1: [[0.300000 -0.100000]\n [0.200000 0.400000]]\nb1: [0.100000 -0.200000]\nW2: [0.500000 -0.300000]\nb2: 0.200000\n</pre> In\u00a0[3]: Copied! <pre># === 1) Forward pass ===\nZ1 = W1 @ X + b1\nA1 = tanh(Z1)\nZ2 = float(W2 @ A1 + b2)\nY_hat = float(tanh(Z2))\n\nprint(\"\\n--- Forward Pass ---\")\np(\"Z1 = W1 @ X + b1\", Z1)\np(\"A1 = tanh(Z1)\", A1)\np(\"Z2 = W2 \u00b7 A1 + b2\", Z2)\np(\"\u0177 = tanh(Z2)\", Y_hat)\n\n# === 2) Loss ===\nL = (Y - Y_hat)**2\nprint(\"\\n--- Loss (MSE) ---\")\np(\"L = (Y - \u0177)^2\", L)\n</pre> # === 1) Forward pass === Z1 = W1 @ X + b1 A1 = tanh(Z1) Z2 = float(W2 @ A1 + b2) Y_hat = float(tanh(Z2))  print(\"\\n--- Forward Pass ---\") p(\"Z1 = W1 @ X + b1\", Z1) p(\"A1 = tanh(Z1)\", A1) p(\"Z2 = W2 \u00b7 A1 + b2\", Z2) p(\"\u0177 = tanh(Z2)\", Y_hat)  # === 2) Loss === L = (Y - Y_hat)**2 print(\"\\n--- Loss (MSE) ---\") p(\"L = (Y - \u0177)^2\", L)  <pre>\n--- Forward Pass ---\nZ1 = W1 @ X + b1: [0.270000 -0.180000]\nA1 = tanh(Z1): [0.263625 -0.178081]\nZ2 = W2 \u00b7 A1 + b2: 0.385237\n\u0177 = tanh(Z2): 0.367247\n\n--- Loss (MSE) ---\nL = (Y - \u0177)^2: 0.400377\n</pre> In\u00a0[4]: Copied! <pre># === 3) Backpropagation ===\n\n# Sa\u00edda\ndL_dYhat = 2.0*(Y_hat - Y)\ndYhat_dZ2 = dtanh(Z2)\ndL_dZ2 = dL_dYhat * dYhat_dZ2\n\nprint(\"\\n-- Sa\u00edda --\")\np(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat)\np(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2)\np(\"dL/dZ2\", dL_dZ2)\n\n# Gradientes da camada de sa\u00edda\ndL_dW2 = dL_dZ2 * A1            # (2,)\ndL_db2 = dL_dZ2                 # escalar\n\nprint(\"\\n-- Gradientes camada de sa\u00edda --\")\np(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2)\np(\"dL/db2 = dL/dZ2\", dL_db2)\n\n# Propaga\u00e7\u00e3o p/ camada oculta\ndL_dA1 = dL_dZ2 * W2            # (2,)\ndA1_dZ1 = dtanh(Z1)             # (2,)\ndL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)\n\nprint(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\")\np(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1)\np(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1)\np(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)\n\n# Gradientes da camada oculta\ndL_dW1 = np.outer(dL_dZ1, X)    # (2,2)\ndL_db1 = dL_dZ1                 # (2,)\n\nprint(\"\\n-- Gradientes camada oculta --\")\np(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1)\np(\"dL/db1 = dL/dZ1\", dL_db1)\n</pre> # === 3) Backpropagation ===  # Sa\u00edda dL_dYhat = 2.0*(Y_hat - Y) dYhat_dZ2 = dtanh(Z2) dL_dZ2 = dL_dYhat * dYhat_dZ2  print(\"\\n-- Sa\u00edda --\") p(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat) p(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2) p(\"dL/dZ2\", dL_dZ2)  # Gradientes da camada de sa\u00edda dL_dW2 = dL_dZ2 * A1            # (2,) dL_db2 = dL_dZ2                 # escalar  print(\"\\n-- Gradientes camada de sa\u00edda --\") p(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2) p(\"dL/db2 = dL/dZ2\", dL_db2)  # Propaga\u00e7\u00e3o p/ camada oculta dL_dA1 = dL_dZ2 * W2            # (2,) dA1_dZ1 = dtanh(Z1)             # (2,) dL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)  print(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\") p(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1) p(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1) p(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)  # Gradientes da camada oculta dL_dW1 = np.outer(dL_dZ1, X)    # (2,2) dL_db1 = dL_dZ1                 # (2,)  print(\"\\n-- Gradientes camada oculta --\") p(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1) p(\"dL/db1 = dL/dZ1\", dL_db1) <pre>\n-- Sa\u00edda --\ndL/d\u0177 = 2*(\u0177 - Y): -1.265507\ndtanh(Z2) = 1 - tanh^2(Z2): 0.865130\ndL/dZ2: -1.094828\n\n-- Gradientes camada de sa\u00edda --\ndL/dW2 = dL/dZ2 * A1: [-0.288624 0.194968]\ndL/db2 = dL/dZ2: -1.094828\n\n-- Propaga\u00e7\u00e3o para a oculta --\ndL/dA1 = dL/dZ2 * W2: [-0.547414 0.328448]\ndtanh(Z1) = 1 - tanh^2(Z1): [0.930502 0.968287]\ndL/dZ1 = dL/dA1 \u2299 dtanh(Z1): [-0.509370 0.318032]\n\n-- Gradientes camada oculta --\ndL/dW1 = outer(dL/dZ1, X): [[-0.254685 0.101874]\n [0.159016 -0.063606]]\ndL/db1 = dL/dZ1: [-0.509370 0.318032]\n</pre> In\u00a0[5]: Copied! <pre># === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) ===\nW2_new = W2 - eta_update * dL_dW2\nb2_new = b2 - eta_update * dL_db2\nW1_new = W1 - eta_update * dL_dW1\nb1_new = b1 - eta_update * dL_db1\n\np(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new)\np(\"b2_new = b2 - \u03b7*dL/db2\", b2_new)\np(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new)\np(\"b1_new = b1 - \u03b7*dL/db1\", b1_new)\n</pre> # === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) === W2_new = W2 - eta_update * dL_dW2 b2_new = b2 - eta_update * dL_db2 W1_new = W1 - eta_update * dL_dW1 b1_new = b1 - eta_update * dL_db1  p(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new) p(\"b2_new = b2 - \u03b7*dL/db2\", b2_new) p(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new) p(\"b1_new = b1 - \u03b7*dL/db1\", b1_new) <pre>\nW2_new = W2 - \u03b7*dL/dW2: [0.528862 -0.319497]\nb2_new = b2 - \u03b7*dL/db2: 0.309483\nW1_new = W1 - \u03b7*dL/dW1: [[0.325468 -0.110187]\n [0.184098 0.406361]]\nb1_new = b1 - \u03b7*dL/db1: [0.150937 -0.231803]\n</pre> In\u00a0[6]: Copied! <pre># === Checagem opcional: forward com par\u00e2metros atualizados ===\nZ1_new = W1_new @ X + b1_new\nA1_new = tanh(Z1_new)\nZ2_new = float(W2_new @ A1_new + b2_new)\nY_hat_new = float(tanh(Z2_new))\nL_new = (Y - Y_hat_new)**2\n\np(\"\\n\u0177 (antes)\", Y_hat)\np(\"L (antes)\", L)\np(\"\u0177 (depois)\", Y_hat_new)\np(\"L (depois)\", L_new)\n</pre> # === Checagem opcional: forward com par\u00e2metros atualizados === Z1_new = W1_new @ X + b1_new A1_new = tanh(Z1_new) Z2_new = float(W2_new @ A1_new + b2_new) Y_hat_new = float(tanh(Z2_new)) L_new = (Y - Y_hat_new)**2  p(\"\\n\u0177 (antes)\", Y_hat) p(\"L (antes)\", L) p(\"\u0177 (depois)\", Y_hat_new) p(\"L (depois)\", L_new)  <pre>\n\u0177 (antes): 0.367247\nL (antes): 0.400377\n\u0177 (depois): 0.500620\nL (depois): 0.249380\n</pre> <p>Using the <code>make_classification</code> function from scikit-learn, generate a synthetic dataset with the following specifications:</p> <ul> <li><p>Number of samples: 1000</p> </li> <li><p>Number of classes: 2</p> </li> <li><p>Number of clusters per class: Use the n_clusters_per_class parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</p> </li> <li><p>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</p> </li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li><p>Number of hidden layers (at least 1)</p> </li> <li><p>Number of neurons per layer</p> </li> <li><p>Activation functions (e.g., sigmoid, ReLU, tanh)</p> </li> <li><p>Loss function (e.g., binary cross-entropy)</p> </li> <li><p>Optimizer (e.g., gradient descent, with a chosen learning rate)</p> </li> </ul> <p>Steps to follow:</p> <ol> <li><p>Generate and split the data into training (80%) and testing (20%) sets.</p> </li> <li><p>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</p> </li> <li><p>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</p> </li> <li><p>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</p> </li> <li><p>Submit your code and results, including any visualizations.</p> </li> </ol> In\u00a0[7]: Copied! <pre>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(42)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid(a):\n    # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)\n    return a * (1.0 - a)\n\ndef bce_loss(y_true, y_pred, eps=1e-12):\n    # y_true, y_pred com shape (m,1)\n    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n    return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))\n\ndef accuracy(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    return (y_hat == y_true).mean()\n\ndef confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())\n    tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())\n    fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())\n    fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())\n    return np.array([[tn, fp],\n                     [fn, tp]])\n</pre> from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  np.random.seed(42)  def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))  def dsigmoid(a):     # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)     return a * (1.0 - a)  def bce_loss(y_true, y_pred, eps=1e-12):     # y_true, y_pred com shape (m,1)     y_pred = np.clip(y_pred, eps, 1.0 - eps)     return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))  def accuracy(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     return (y_hat == y_true).mean()  def confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())     tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())     fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())     fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())     return np.array([[tn, fp],                      [fn, tp]]) In\u00a0[8]: Copied! <pre># Par\u00e2metros gerais\nN_total = 1000\nclass_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador\nflip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados)\nrandom_state = 42\n\n# 1) Classe 0 com 1 cluster\nX0, y0 = make_classification(\n    n_samples=N_total//2, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=1, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state\n)\n# Filtra apenas a classe 0\nX0 = X0[y0 == 0]\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1\nX1_full, y1_full = make_classification(\n    n_samples=N_total, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=2, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state + 1\n)\nX1 = X1_full[y1_full == 1]\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Balanceia o tamanho: escolhe min entre os dois lados\nn = min(len(X0), len(X1))\nX0 = X0[:n]\ny0 = y0[:n]\nX1 = X1[:n]\ny1 = y1[:n]\n\n# Combina\nX = np.vstack([X0, X1])\ny = np.concatenate([y0, y1])\n\n# Embaralha\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")\n\n# Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o)\nplt.figure()\nplt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8)\nplt.title(\"Dados sint\u00e9ticos (antes do split)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Par\u00e2metros gerais N_total = 1000 class_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador flip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados) random_state = 42  # 1) Classe 0 com 1 cluster X0, y0 = make_classification(     n_samples=N_total//2, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=1, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state ) # Filtra apenas a classe 0 X0 = X0[y0 == 0] y0 = np.zeros((X0.shape[0],), dtype=int)  # 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1 X1_full, y1_full = make_classification(     n_samples=N_total, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=2, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state + 1 ) X1 = X1_full[y1_full == 1] y1 = np.ones((X1.shape[0],), dtype=int)  # Balanceia o tamanho: escolhe min entre os dois lados n = min(len(X0), len(X1)) X0 = X0[:n] y0 = y0[:n] X1 = X1[:n] y1 = y1[:n]  # Combina X = np.vstack([X0, X1]) y = np.concatenate([y0, y1])  # Embaralha perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")  # Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o) plt.figure() plt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8) plt.title(\"Dados sint\u00e9ticos (antes do split)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>X shape: (498, 2), y shape: (498,), classe 0: 249, classe 1: 249\n</pre> In\u00a0[9]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Ajustar shapes de y p/ coluna\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  # Ajustar shapes de y p/ coluna y_train = y_train.reshape(-1, 1) y_test = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) <pre>(398, 2) (398, 1) (100, 2) (100, 1)\n</pre> In\u00a0[10]: Copied! <pre># Arquitetura\nn_in = 2\nn_hidden = 16\nn_out = 1\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef forward(Xb):\n    # Xb: (m, 2)\n    z1 = Xb @ W1 + b1          # (m, hidden)\n    a1 = tanh(z1)              # (m, hidden)\n    z2 = a1 @ W2 + b2          # (m, 1)\n    a2 = sigmoid(z2)           # (m, 1)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb):\n    # yb: (m,1)\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m\n    dz2 = (a2 - yb) / m                     # (m,1)\n    dW2 = a1.T @ dz2                        # (hidden,1)\n    db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)\n\n    da1 = dz2 @ W2.T                        # (m,hidden)\n    dz1 = da1 * dtanh(a1)                   # (m,hidden)\n    dW1 = Xb.T @ dz1                        # (2,hidden)\n    db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)\n\n    return dW1, db1, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = 2 n_hidden = 16 n_out = 1  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def forward(Xb):     # Xb: (m, 2)     z1 = Xb @ W1 + b1          # (m, hidden)     a1 = tanh(z1)              # (m, hidden)     z2 = a1 @ W2 + b2          # (m, 1)     a2 = sigmoid(z2)           # (m, 1)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb):     # yb: (m,1)     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m     dz2 = (a2 - yb) / m                     # (m,1)     dW2 = a1.T @ dz2                        # (hidden,1)     db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)      da1 = dz2 @ W2.T                        # (m,hidden)     dz1 = da1 * dtanh(a1)                   # (m,hidden)     dW1 = Xb.T @ dz1                        # (2,hidden)     db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)      return dW1, db1, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[11]: Copied! <pre># Hiperpar\u00e2metros de treino\nepochs = 300\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nindices = np.arange(m)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(indices)\n    X_train_shuf = X_train[indices]\n    y_train_shuf = y_train[indices]\n\n    # mini-batches\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = X_train_shuf[start:end]\n        yb = y_train_shuf[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = bce_loss(yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        # Acur\u00e1cia de treino r\u00e1pida\n        yhat_full, _ = forward(X_train)\n        acc_tr = accuracy(y_train, yhat_full)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\n# Curva de loss\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist, marker=None)\nplt.title(\"Loss (treino) por \u00e9poca\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Binary Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Hiperpar\u00e2metros de treino epochs = 300 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] indices = np.arange(m)  for ep in range(1, epochs+1):     np.random.shuffle(indices)     X_train_shuf = X_train[indices]     y_train_shuf = y_train[indices]      # mini-batches     ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = X_train_shuf[start:end]         yb = y_train_shuf[start:end]          yhat, cache = forward(Xb)         loss = bce_loss(yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         # Acur\u00e1cia de treino r\u00e1pida         yhat_full, _ = forward(X_train)         acc_tr = accuracy(y_train, yhat_full)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  # Curva de loss plt.figure() plt.plot(range(1, epochs+1), loss_hist, marker=None) plt.title(\"Loss (treino) por \u00e9poca\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Binary Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.5989 | acc_train=0.9271\nEpoch   25 | loss=0.1912 | acc_train=0.9372\nEpoch   50 | loss=0.1536 | acc_train=0.9422\nEpoch   75 | loss=0.1555 | acc_train=0.9372\n</pre> <pre>Epoch  100 | loss=0.1495 | acc_train=0.9397\nEpoch  125 | loss=0.1472 | acc_train=0.9397\nEpoch  150 | loss=0.1438 | acc_train=0.9422\nEpoch  175 | loss=0.1459 | acc_train=0.9422\nEpoch  200 | loss=0.1496 | acc_train=0.9397\nEpoch  225 | loss=0.1481 | acc_train=0.9422\nEpoch  250 | loss=0.1531 | acc_train=0.9422\nEpoch  275 | loss=0.1605 | acc_train=0.9422\nEpoch  300 | loss=0.1393 | acc_train=0.9397\n</pre> In\u00a0[12]: Copied! <pre># Predi\u00e7\u00f5es no teste\nyprob_test, _ = forward(X_test)\nacc_te = accuracy(y_test, yprob_test)\ncm = confusion_matrix_manual(y_test, yprob_test)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\")\nprint(cm)\n</pre> # Predi\u00e7\u00f5es no teste yprob_test, _ = forward(X_test) acc_te = accuracy(y_test, yprob_test) cm = confusion_matrix_manual(y_test, yprob_test)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\") print(cm) <pre>Acur\u00e1cia (teste): 0.9600\nMatriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\n[[47  3]\n [ 1 49]]\n</pre> In\u00a0[13]: Copied! <pre># Grade para visualizar a fronteira\nx_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0\ny_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                     np.linspace(y_min, y_max, 300))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Lembre: padronizamos com 'scaler'\ngrid_std = scaler.transform(grid)\nprobs, _ = forward(grid_std)\nZZ = probs.reshape(xx.shape)\n\nplt.figure()\nplt.contourf(xx, yy, ZZ, levels=50, alpha=0.6)\nplt.colorbar(label=\"p(y=1)\")\nplt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8)\nplt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Grade para visualizar a fronteira x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0 y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                      np.linspace(y_min, y_max, 300)) grid = np.c_[xx.ravel(), yy.ravel()]  # Lembre: padronizamos com 'scaler' grid_std = scaler.transform(grid) probs, _ = forward(grid_std) ZZ = probs.reshape(xx.shape)  plt.figure() plt.contourf(xx, yy, ZZ, levels=50, alpha=0.6) plt.colorbar(label=\"p(y=1)\") plt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8) plt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Synthetic Data and Reusable MLP</p> <p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol> In\u00a0[14]: Copied! <pre>from sklearn.decomposition import PCA\n\nnp.random.seed(42)\nplt.rcParams[\"figure.figsize\"] = (6, 5)\n# --- Ativa\u00e7\u00f5es ---\ndef softmax(z):\n    # z: (m, K)\n    z_shift = z - np.max(z, axis=1, keepdims=True)\n    e = np.exp(z_shift)\n    return e / np.sum(e, axis=1, keepdims=True)\n\n# --- Loss e m\u00e9tricas (multi-classe) ---\ndef cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):\n    # y_true_onehot, y_prob: (m, K)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))\n\ndef accuracy_multiclass(y_true, y_prob):\n    # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}\n    y_pred = np.argmax(y_prob, axis=1)\n    return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()\n\ndef confusion_matrix_k(y_true, y_prob, K):\n    y_pred = np.argmax(y_prob, axis=1)\n    cm = np.zeros((K, K), dtype=int)\n    for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):\n        cm[t, p] += 1\n    return cm\n</pre> from sklearn.decomposition import PCA  np.random.seed(42) plt.rcParams[\"figure.figsize\"] = (6, 5) # --- Ativa\u00e7\u00f5es --- def softmax(z):     # z: (m, K)     z_shift = z - np.max(z, axis=1, keepdims=True)     e = np.exp(z_shift)     return e / np.sum(e, axis=1, keepdims=True)  # --- Loss e m\u00e9tricas (multi-classe) --- def cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):     # y_true_onehot, y_prob: (m, K)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))  def accuracy_multiclass(y_true, y_prob):     # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}     y_pred = np.argmax(y_prob, axis=1)     return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()  def confusion_matrix_k(y_true, y_prob, K):     y_pred = np.argmax(y_prob, axis=1)     cm = np.zeros((K, K), dtype=int)     for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):         cm[t, p] += 1     return cm In\u00a0[15]: Copied! <pre>N_total = 1500\nK = 3\nn_features = 4\nclass_sep = 1.8\nflip_y = 0.00\n\n# Queremos tamanhos semelhantes por classe\ntarget_per_class = N_total // K  # 500\n\ndef gen_class_subset(target, n_clusters, rs):\n    # como vamos filtrar apenas a classe positiva, geramos o dobro\n    n_samples_tmp = target * 2\n    X_tmp, y_tmp = make_classification(\n        n_samples=n_samples_tmp,\n        n_features=n_features,\n        n_informative=n_features,\n        n_redundant=0,\n        n_classes=2,\n        n_clusters_per_class=n_clusters,\n        class_sep=class_sep,\n        flip_y=flip_y,\n        random_state=rs\n    )\n    X_pos = X_tmp[y_tmp == 1]\n    if len(X_pos) &lt; target:\n        # se por algum motivo veio menos, reamostrar com rs+1\n        X_more, y_more = make_classification(\n            n_samples=n_samples_tmp,\n            n_features=n_features,\n            n_informative=n_features,\n            n_redundant=0,\n            n_classes=2,\n            n_clusters_per_class=n_clusters,\n            class_sep=class_sep,\n            flip_y=flip_y,\n            random_state=rs+101\n        )\n        X_pos = np.vstack([X_pos, X_more[y_more == 1]])\n    return X_pos[:target]\n\n# Classe 0 -&gt; 2 clusters\nX0 = gen_class_subset(target_per_class, n_clusters=2, rs=42)\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# Classe 1 -&gt; 3 clusters\nX1 = gen_class_subset(target_per_class, n_clusters=3, rs=43)\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Classe 2 -&gt; 4 clusters\nX2 = gen_class_subset(target_per_class, n_clusters=4, rs=44)\ny2 = np.full((X2.shape[0],), 2, dtype=int)\n\n# Combine e embaralhe\nX = np.vstack([X0, X1, X2])\ny = np.concatenate([y0, y1, y2])\n\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])\n\n# Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar\npca = PCA(n_components=2, random_state=0)\nX_2d = pca.fit_transform(X)\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c)\nplt.title(\"Dados (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> N_total = 1500 K = 3 n_features = 4 class_sep = 1.8 flip_y = 0.00  # Queremos tamanhos semelhantes por classe target_per_class = N_total // K  # 500  def gen_class_subset(target, n_clusters, rs):     # como vamos filtrar apenas a classe positiva, geramos o dobro     n_samples_tmp = target * 2     X_tmp, y_tmp = make_classification(         n_samples=n_samples_tmp,         n_features=n_features,         n_informative=n_features,         n_redundant=0,         n_classes=2,         n_clusters_per_class=n_clusters,         class_sep=class_sep,         flip_y=flip_y,         random_state=rs     )     X_pos = X_tmp[y_tmp == 1]     if len(X_pos) &lt; target:         # se por algum motivo veio menos, reamostrar com rs+1         X_more, y_more = make_classification(             n_samples=n_samples_tmp,             n_features=n_features,             n_informative=n_features,             n_redundant=0,             n_classes=2,             n_clusters_per_class=n_clusters,             class_sep=class_sep,             flip_y=flip_y,             random_state=rs+101         )         X_pos = np.vstack([X_pos, X_more[y_more == 1]])     return X_pos[:target]  # Classe 0 -&gt; 2 clusters X0 = gen_class_subset(target_per_class, n_clusters=2, rs=42) y0 = np.zeros((X0.shape[0],), dtype=int)  # Classe 1 -&gt; 3 clusters X1 = gen_class_subset(target_per_class, n_clusters=3, rs=43) y1 = np.ones((X1.shape[0],), dtype=int)  # Classe 2 -&gt; 4 clusters X2 = gen_class_subset(target_per_class, n_clusters=4, rs=44) y2 = np.full((X2.shape[0],), 2, dtype=int)  # Combine e embaralhe X = np.vstack([X0, X1, X2]) y = np.concatenate([y0, y1, y2])  perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])  # Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar pca = PCA(n_components=2, random_state=0) X_2d = pca.fit_transform(X) plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c) plt.title(\"Dados (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>Shapes: (1500, 4) (1500,) | classes: [np.int64(500), np.int64(500), np.int64(500)]\n</pre> In\u00a0[16]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[17]: Copied! <pre># Arquitetura\nn_in = n_features\nn_hidden = 32\nn_out = K\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    # y: (m,1)  -&gt; (m,K)\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    z1 = Xb @ W1 + b1         # (m, hidden)\n    a1 = tanh(z1)             # (m, hidden)\n    z2 = a1 @ W2 + b2         # (m, K)\n    a2 = softmax(z2)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m\n    dz2 = (a2 - yb_onehot) / m          # (m,K)\n    dW2 = a1.T @ dz2                    # (hidden,K)\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    da1 = dz2 @ W2.T                    # (m,hidden)\n    dz1 = da1 * dtanh(a1)               # (m,hidden)\n    dW1 = Xb.T @ dz1                    # (n_in,hidden)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)\n\n    return dW1, db1_, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = n_features n_hidden = 32 n_out = K  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def onehot(y, K):     # y: (m,1)  -&gt; (m,K)     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     z1 = Xb @ W1 + b1         # (m, hidden)     a1 = tanh(z1)             # (m, hidden)     z2 = a1 @ W2 + b2         # (m, K)     a2 = softmax(z2)          # (m, K)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m     dz2 = (a2 - yb_onehot) / m          # (m,K)     dW2 = a1.T @ dz2                    # (hidden,K)     db2 = np.sum(dz2, axis=0, keepdims=True)      da1 = dz2 @ W2.T                    # (m,hidden)     dz1 = da1 * dtanh(a1)               # (m,hidden)     dW1 = Xb.T @ dz1                    # (n_in,hidden)     db1_ = np.sum(dz1, axis=0, keepdims=True)      return dW1, db1_, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[18]: Copied! <pre>epochs = 350\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\n\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 350 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m)  Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9847 | acc_train=0.5717\nEpoch   25 | loss=0.7307 | acc_train=0.6967\nEpoch   50 | loss=0.5286 | acc_train=0.8008\n</pre> <pre>Epoch   75 | loss=0.4153 | acc_train=0.8483\nEpoch  100 | loss=0.3526 | acc_train=0.8850\nEpoch  125 | loss=0.3169 | acc_train=0.8983\nEpoch  150 | loss=0.2904 | acc_train=0.9025\n</pre> <pre>Epoch  175 | loss=0.2701 | acc_train=0.9025\nEpoch  200 | loss=0.2559 | acc_train=0.9117\nEpoch  225 | loss=0.2439 | acc_train=0.9167\nEpoch  250 | loss=0.2333 | acc_train=0.9150\n</pre> <pre>Epoch  275 | loss=0.2225 | acc_train=0.9192\nEpoch  300 | loss=0.2137 | acc_train=0.9208\nEpoch  325 | loss=0.2055 | acc_train=0.9283\nEpoch  350 | loss=0.1999 | acc_train=0.9267\n</pre> In\u00a0[19]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9200\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[93  7  0]\n [ 6 90  4]\n [ 6  1 93]]\n</pre> In\u00a0[20]: Copied! <pre># Projeta teste em 2D para visualizar predi\u00e7\u00f5es\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Projeta teste em 2D para visualizar predi\u00e7\u00f5es pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Deeper MLP Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p> In\u00a0[21]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[22]: Copied! <pre># Arquitetura (duas ocultas)\nK = 3                 # n\u00famero de classes\nn_in = X_train.shape[1]\nn_hidden1 = 64\nn_hidden2 = 32\nn_out = K\n\n# Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden1))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1))\nb1 = np.zeros((1, n_hidden1))\n\nlimit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2))\nb2 = np.zeros((1, n_hidden2))\n\nlimit3 = np.sqrt(6.0 / (n_hidden2 + n_out))\nW3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out))\nb3 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    # camada 1\n    z1 = Xb @ W1 + b1         # (m, h1)\n    a1 = tanh(z1)             # (m, h1)\n    # camada 2\n    z2 = a1 @ W2 + b2         # (m, h2)\n    a2 = tanh(z2)             # (m, h2)\n    # sa\u00edda\n    z3 = a2 @ W3 + b3         # (m, K)\n    a3 = softmax(z3)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2, z3, a3)\n    return a3, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2, z3, a3 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE\n    dz3 = (a3 - yb_onehot) / m               # (m,K)\n    dW3 = a2.T @ dz3                         # (h2,K)\n    db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)\n\n    da2 = dz3 @ W3.T                         # (m,h2)\n    dz2 = da2 * dtanh(a2)                    # (m,h2)\n    dW2 = a1.T @ dz2                         # (h1,h2)\n    db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)\n\n    da1 = dz2 @ W2.T                         # (m,h1)\n    dz1 = da1 * dtanh(a1)                    # (m,h1)\n    dW1 = Xb.T @ dz1                         # (n_in,h1)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)\n\n    return dW1, db1_, dW2, db2_, dW3, db3\n\ndef update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):\n    global W1, b1, W2, b2, W3, b3\n    W1 -= lr * dW1; b1 -= lr * db1_\n    W2 -= lr * dW2; b2 -= lr * db2_\n    W3 -= lr * dW3; b3 -= lr * db3_\n</pre> # Arquitetura (duas ocultas) K = 3                 # n\u00famero de classes n_in = X_train.shape[1] n_hidden1 = 64 n_hidden2 = 32 n_out = K  # Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden1)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1)) b1 = np.zeros((1, n_hidden1))  limit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2)) b2 = np.zeros((1, n_hidden2))  limit3 = np.sqrt(6.0 / (n_hidden2 + n_out)) W3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out)) b3 = np.zeros((1, n_out))  def onehot(y, K):     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     # camada 1     z1 = Xb @ W1 + b1         # (m, h1)     a1 = tanh(z1)             # (m, h1)     # camada 2     z2 = a1 @ W2 + b2         # (m, h2)     a2 = tanh(z2)             # (m, h2)     # sa\u00edda     z3 = a2 @ W3 + b3         # (m, K)     a3 = softmax(z3)          # (m, K)     cache = (Xb, z1, a1, z2, a2, z3, a3)     return a3, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2, z3, a3 = cache     m = Xb.shape[0]      # Softmax + CE     dz3 = (a3 - yb_onehot) / m               # (m,K)     dW3 = a2.T @ dz3                         # (h2,K)     db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)      da2 = dz3 @ W3.T                         # (m,h2)     dz2 = da2 * dtanh(a2)                    # (m,h2)     dW2 = a1.T @ dz2                         # (h1,h2)     db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)      da1 = dz2 @ W2.T                         # (m,h1)     dz1 = da1 * dtanh(a1)                    # (m,h1)     dW1 = Xb.T @ dz1                         # (n_in,h1)     db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)      return dW1, db1_, dW2, db2_, dW3, db3  def update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):     global W1, b1, W2, b2, W3, b3     W1 -= lr * dW1; b1 -= lr * db1_     W2 -= lr * dW2; b2 -= lr * db2_     W3 -= lr * dW3; b3 -= lr * db3_  In\u00a0[23]: Copied! <pre>epochs = 400\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 400 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m) Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9520 | acc_train=0.5642\nEpoch   25 | loss=0.5414 | acc_train=0.7717\n</pre> <pre>Epoch   50 | loss=0.3694 | acc_train=0.8708\nEpoch   75 | loss=0.3003 | acc_train=0.8975\n</pre> <pre>Epoch  100 | loss=0.2528 | acc_train=0.9058\nEpoch  125 | loss=0.2266 | acc_train=0.9233\n</pre> <pre>Epoch  150 | loss=0.2025 | acc_train=0.9308\nEpoch  175 | loss=0.1887 | acc_train=0.9367\n</pre> <pre>Epoch  200 | loss=0.1764 | acc_train=0.9250\nEpoch  225 | loss=0.1619 | acc_train=0.9408\n</pre> <pre>Epoch  250 | loss=0.1524 | acc_train=0.9467\nEpoch  275 | loss=0.1475 | acc_train=0.9308\n</pre> <pre>Epoch  300 | loss=0.1390 | acc_train=0.9467\nEpoch  325 | loss=0.1300 | acc_train=0.9408\n</pre> <pre>Epoch  350 | loss=0.1282 | acc_train=0.9467\nEpoch  375 | loss=0.1261 | acc_train=0.9483\n</pre> <pre>Epoch  400 | loss=0.1212 | acc_train=0.9533\n</pre> In\u00a0[24]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9267\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[96  4  0]\n [10 87  3]\n [ 5  0 95]]\n</pre> In\u00a0[25]: Copied! <pre># Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n</pre> # Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  In\u00a0[26]: Copied! <pre># === Avalia\u00e7\u00e3o treino vs teste ===\nyprob_tr, _ = forward(X_train)\nyprob_te, _ = forward(X_test)\n\nacc_tr = accuracy_multiclass(y_train, yprob_tr)\nacc_te = accuracy_multiclass(y_test, yprob_te)\n\nprint(f\"Acc TREINO: {acc_tr:.4f}\")\nprint(f\"Acc TESTE : {acc_te:.4f}\")\n\nloss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr)\nloss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te)\nprint(f\"Loss TREINO: {loss_tr:.4f}\")\nprint(f\"Loss TESTE : {loss_te:.4f}\")\n</pre> # === Avalia\u00e7\u00e3o treino vs teste === yprob_tr, _ = forward(X_train) yprob_te, _ = forward(X_test)  acc_tr = accuracy_multiclass(y_train, yprob_tr) acc_te = accuracy_multiclass(y_test, yprob_te)  print(f\"Acc TREINO: {acc_tr:.4f}\") print(f\"Acc TESTE : {acc_te:.4f}\")  loss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr) loss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te) print(f\"Loss TREINO: {loss_tr:.4f}\") print(f\"Loss TESTE : {loss_te:.4f}\") <pre>Acc TREINO: 0.9533\nAcc TESTE : 0.9267\nLoss TREINO: 0.1145\nLoss TESTE : 0.1905\n</pre> <p><code>Accuracy Train</code> foi maior que <code>Accuracy Test</code>, indicando que o modelo est\u00e1 sofrendo de overfitting.</p>"},{"location":"Exercicios/EX3/MLP/#3-mlp","title":"3. MLP\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#activity-understanding-multi-layer-perceptrons-mlps","title":"Activity: Understanding Multi-Layer Perceptrons (MLPs)\u00b6","text":"<p>This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs).</p>"},{"location":"Exercicios/EX3/MLP/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-4","title":"Exercise 4\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#verficacao-de-overfitting-do-ex4","title":"Verfica\u00e7\u00e3o de Overfitting do EX4\u00b6","text":""}]}