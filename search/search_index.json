{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ementa","text":"Teste GitHub Pages Teste com Lorem Ipsum <p>Esse \u00e9 um exemplo de p\u00e1gina simples para GitHub Pages.</p> Se\u00e7\u00e3o 1 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet felis in nunc fringilla ullamcorper. Proin non lacus vitae ligula pulvinar facilisis.</p> Se\u00e7\u00e3o 2 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vitae venenatis ligula. Ut malesuada augue nec mi tempor, eu malesuada libero hendrerit.</p> Se\u00e7\u00e3o 3 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tincidunt, ipsum at sagittis tincidunt, risus ipsum cursus lorem, non dictum ipsum sapien quis elit.</p> <p>Rodap\u00e9 - P\u00e1gina de Teste</p>"},{"location":"Exercicios/EX1/data/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class</p> In\u00a0[2]: Copied! <pre>np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.\n\n# M\u00e9dias e desvios para cada classe\nmeans = [(2, 3), (5, 6), (8, 1), (15, 4)]\nstds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]\n\ndata = []\nlabels = []\n\nfor i in range(len(means)):\n    mean = means[i]\n    std = stds[i]\n\n    x = np.random.normal(loc=mean[0], scale=std[0], size=100)\n    y = np.random.normal(loc=mean[1], scale=std[1], size=100)\n\n    points = []\n    for j in range(100):\n        points.append([x[j], y[j]])\n\n    data.extend(points)\n\n    for j in range(100):\n        labels.append(i)\n\ndata = np.array(data)\nlabels = np.array(labels)\n</pre> np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.  # M\u00e9dias e desvios para cada classe means = [(2, 3), (5, 6), (8, 1), (15, 4)] stds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]  data = [] labels = []  for i in range(len(means)):     mean = means[i]     std = stds[i]      x = np.random.normal(loc=mean[0], scale=std[0], size=100)     y = np.random.normal(loc=mean[1], scale=std[1], size=100)      points = []     for j in range(100):         points.append([x[j], y[j]])      data.extend(points)      for j in range(100):         labels.append(i)  data = np.array(data) labels = np.array(labels) <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nfor cls in range(4):\n    plt.scatter(data[labels == cls, 0],\n                data[labels == cls, 1],\n                label=f'Classe {cls}',\n                alpha=0.7)\nplt.legend()\nplt.xlabel('Eixo X')\nplt.ylabel('Eixo Y')\nplt.title('Dispers\u00e3o das 4 classes em 2D')\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) for cls in range(4):     plt.scatter(data[labels == cls, 0],                 data[labels == cls, 1],                 label=f'Classe {cls}',                 alpha=0.7) plt.legend() plt.xlabel('Eixo X') plt.ylabel('Eixo Y') plt.title('Dispers\u00e3o das 4 classes em 2D') plt.show() <p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p>Answers</p> <ol> <li>O scatter plot mostra que as quatro classes est\u00e3o distribuidas de forma relativamente clara, com alguma sobreposi\u00e7\u00e3o entre elas. As classes 0 e 1 est\u00e3o mais pr\u00f3ximas uma da outra, enquanto as classes 2 ainda encosta um pouco na classe 1, e longe de todas as outras temos a classe 3.</li> <li>N\u00e3o, uma fronteira linear simples n\u00e3o seria capaz de separar todas as classes de forma eficaz, especialmente devido \u00e0 sobreposi\u00e7\u00e3o entre as classes 0 e 1.</li> <li>Pode ser visto no gr\u00e1fico abaixo:</li> </ol> <p></p> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B.</p> In\u00a0[4]: Copied! <pre>mu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1.0]])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],\n                    [-0.7, 1.5,  0.4, 0.0, 0.0],\n                    [0.2,  0.4,  1.5, 0.6, 0.0],\n                    [0.0,  0.0,  0.6, 1.5, 0.3],\n                    [0.0,  0.0,  0.0, 0.3, 1.5]])\n\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)\n</pre> mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],                     [0.8, 1.0, 0.3, 0.0, 0.0],                     [0.1, 0.3, 1.0, 0.5, 0.0],                     [0.0, 0.0, 0.5, 1.0, 0.2],                     [0.0, 0.0, 0.0, 0.2, 1.0]])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],                     [-0.7, 1.5,  0.4, 0.0, 0.0],                     [0.2,  0.4,  1.5, 0.6, 0.0],                     [0.0,  0.0,  0.6, 1.5, 0.3],                     [0.0,  0.0,  0.0, 0.3, 1.5]])  XA = np.random.multivariate_normal(mu_A, Sigma_A, size=500) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack([XA, XB]) y = np.array([0]*500 + [1]*500) <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>PCA:</p> <ol> <li>Centralizar os dados (tirar a m\u00e9dia).</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Extrair autovalores e autovetores da matriz de covari\u00e2ncia.</li> <li>Ordenar autovetores pelos maiores autovalores.</li> <li>Projetar os dados nos autovetores escolhidos.</li> </ol> In\u00a0[5]: Copied! <pre>def my_pca(X, n_components=None):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered, rowvar=False)\n    autovalores, autovetores = np.linalg.eigh(cov_matrix)\n\n    sorted_idx = np.argsort(autovalores)[::-1]\n    autovalores = autovalores[sorted_idx]\n    autovetores = autovetores[:, sorted_idx]\n\n    total_var = np.sum(autovalores)\n\n    if n_components is not None:\n        autovetores = autovetores[:, :n_components]\n        autovalores = autovalores[:n_components]\n\n    X_pca = np.dot(X_centered, autovetores)\n\n    return X_pca, autovetores, autovalores, total_var\n</pre> def my_pca(X, n_components=None):     X_centered = X - np.mean(X, axis=0)     cov_matrix = np.cov(X_centered, rowvar=False)     autovalores, autovetores = np.linalg.eigh(cov_matrix)      sorted_idx = np.argsort(autovalores)[::-1]     autovalores = autovalores[sorted_idx]     autovetores = autovetores[:, sorted_idx]      total_var = np.sum(autovalores)      if n_components is not None:         autovetores = autovetores[:, :n_components]         autovalores = autovalores[:n_components]      X_pca = np.dot(X_centered, autovetores)      return X_pca, autovetores, autovalores, total_var In\u00a0[6]: Copied! <pre>Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\nplt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)')\nplt.legend(); plt.show()\n\nprint('Vari\u00e2ncia explicada:', autovalores / total_var)\nprint('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var))\n</pre> Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)  plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A') plt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B') plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)') plt.legend(); plt.show()  print('Vari\u00e2ncia explicada:', autovalores / total_var) print('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var)) <pre>Vari\u00e2ncia explicada: [0.52303265 0.15751841]\nVari\u00e2ncia Acumulada: 0.6805510605944183\n</pre> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> <p>Answers</p> <ol> <li>As duas classes est\u00e3o bem pr\u00f3ximas uma da outra, com uma quantidade significativa de sobreposi\u00e7\u00e3o. Caso n\u00e3o fossem duas classes diferentes, poder\u00edamos considerar que se tratam de uma \u00fanica classe.</li> <li>N\u00e3o \u00e9 possivel tra\u00e7ar uma linha que separa as duas classes de forma eficaz, sempre haver\u00e1 uma \u00e1rea de sobreposi\u00e7\u00e3o. Modelos lineares simples, como um Perceptron ou Regress\u00e3o Log\u00edstica, v\u00e3o ter dificuldade porque s\u00f3 conseguem aprender fronteiras lineares (hiperplanos). Eles errariam bastante nos pontos da regi\u00e3o central de overlap.</li> </ol> <p>2. Describe the Data:</p> <ol> <li>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</li> <li>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ol> <p>Answers:</p> <ol> <li><p>O dataset busca prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da Spaceship Titanic. Isso pode ser visto na coluna \"Transported\", que \u00e9 a coluna-alvo (sendo booleano).</p> </li> <li><p>Features: num\u00e9ricas vs categ\u00f3ricas</p> <p>Num\u00e9ricas</p> <ul> <li><p>Age \u2192 idade do passageiro</p> </li> <li><p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 valores gastos em diferentes servi\u00e7os</p> </li> </ul> <p>Categ\u00f3ricas</p> <ul> <li><p>HomePlanet \u2192 planeta de origem (ex: Earth, Europa, Mars)</p> </li> <li><p>CryoSleep \u2192 booleano (se o passageiro entrou em sono criog\u00eanico)</p> </li> <li><p>Cabin \u2192 cont\u00e9m m\u00faltiplas infos (deck/num/side). Pode ser decomposta em:</p> <ul> <li><p>Deck (categ\u00f3rica)</p> </li> <li><p>Num (num\u00e9rica)</p> </li> <li><p>Side (P/S \u2192 categ\u00f3rica bin\u00e1ria)</p> </li> </ul> </li> <li><p>Destination \u2192 destino da viagem (ex: TRAPPIST-1e, etc.)</p> </li> <li><p>VIP \u2192 booleano (pagou servi\u00e7o VIP)</p> </li> <li><p>Name \u2192 geralmente descartado (n\u00e3o tem valor preditivo direto)</p> </li> <li><p>PassengerId \u2192 identificador \u00fanico (n\u00e3o usado como feature)</p> </li> </ul> </li> <li><p>Colunas com valores faltantes:</p> </li> </ol> In\u00a0[7]: Copied! <pre>csv_path = \"train.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\nprint(\"Colunas:\", list(df.columns))\n</pre> csv_path = \"train.csv\" df = pd.read_csv(csv_path) print(\"Shape:\", df.shape) print(\"Colunas:\", list(df.columns)) <pre>Shape: (8693, 14)\nColunas: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n</pre> In\u00a0[8]: Copied! <pre>missing = df.isnull().sum()\nmissing_percent = 100 * missing / len(df)\nmissing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent})\nprint(missing_df[missing_df[\"Missing Values\"] &gt; 0])\n</pre> missing = df.isnull().sum() missing_percent = 100 * missing / len(df) missing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent}) print(missing_df[missing_df[\"Missing Values\"] &gt; 0]) <pre>              Missing Values   Percent\nHomePlanet               201  2.312205\nCryoSleep                217  2.496261\nCabin                    199  2.289198\nDestination              182  2.093639\nAge                      179  2.059128\nVIP                      203  2.335212\nRoomService              181  2.082135\nFoodCourt                183  2.105142\nShoppingMall             208  2.392730\nSpa                      183  2.105142\nVRDeck                   188  2.162660\nName                     200  2.300702\n</pre> <p>3. Preprocessing the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ol> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ol> In\u00a0[9]: Copied! <pre>df_processed = df.copy()\n\n\"\"\"Num\u00e9ricas: \nSolu\u00e7\u00e3o: Preencher com mediana, \nJustificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o.\n\"\"\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    median_val = df_processed[col].median()\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(median_val, inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")\n</pre> df_processed = df.copy()  \"\"\"Num\u00e9ricas:  Solu\u00e7\u00e3o: Preencher com mediana,  Justificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o. \"\"\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] for col in num_cols:     median_val = df_processed[col].median()     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(median_val, inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")  <pre>   - Age: 179 valores preenchidos com 27.00\n   - RoomService: 181 valores preenchidos com 0.00\n   - FoodCourt: 183 valores preenchidos com 0.00\n   - ShoppingMall: 208 valores preenchidos com 0.00\n   - Spa: 183 valores preenchidos com 0.00\n   - VRDeck: 188 valores preenchidos com 0.00\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\n</pre> In\u00a0[10]: Copied! <pre>\"\"\"Categ\u00f3ricas: \nSolu\u00e7\u00e3o: Preencher com \"Unknown\"\nJustificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios.\n\"\"\"\ncat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]\nfor col in cat_cols:\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(\"Unknown\", inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos\")\n</pre> \"\"\"Categ\u00f3ricas:  Solu\u00e7\u00e3o: Preencher com \"Unknown\" Justificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios. \"\"\" cat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"] for col in cat_cols:     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(\"Unknown\", inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos\")  <pre>   - HomePlanet: 201 valores preenchidos\n   - Destination: 182 valores preenchidos\n   - CryoSleep: 217 valores preenchidos\n   - VIP: 203 valores preenchidos\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\797796973.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(\"Unknown\", inplace=True)\n</pre> In\u00a0[11]: Copied! <pre>\"\"\"Cabin: \nSolu\u00e7\u00e3o: Separar em 3 colunas\nJustificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes.\n\"\"\"\ncabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True)\ndf_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\")\ndf_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")\n\n# Preencher CabinNum com mediana\ncabin_num_median = df_processed[\"CabinNum\"].median()\ndf_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> \"\"\"Cabin:  Solu\u00e7\u00e3o: Separar em 3 colunas Justificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes. \"\"\" cabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True) df_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\") df_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\") df_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")  # Preencher CabinNum com mediana cabin_num_median = df_processed[\"CabinNum\"].median() df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True) <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_7876\\2304158112.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> In\u00a0[12]: Copied! <pre>\"\"\"Name: \nSolu\u00e7\u00e3o: descartar\nJustificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido.\n\"\"\"\ndf_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)    \n\nprint(\"Ap\u00f3s tratamento:\", df_processed.shape)\n</pre> \"\"\"Name:  Solu\u00e7\u00e3o: descartar Justificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido. \"\"\" df_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)      print(\"Ap\u00f3s tratamento:\", df_processed.shape) <pre>Ap\u00f3s tratamento: (8693, 14)\n</pre> In\u00a0[13]: Copied! <pre>df_encoded = df_processed.copy()\n\n# Mapeamento booleano/tri-estado\nboolean_mappings = {\n    \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},\n    \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1}\n}\nfor col, mapping in boolean_mappings.items():\n    if col in df_encoded.columns:\n        df_encoded[col] = df_encoded[col].astype(str).map(mapping)\n        print(f\"   - {col}: {mapping}\")\n\n# One-hot para categ\u00f3ricas\ncategorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns]\nfor col in categorical_cols:\n    unique_values = df_encoded[col].astype(str).unique()\n    print(f\"   - {col}: {len(unique_values)} categorias\")\n\ndf_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False)\nprint(\"Ap\u00f3s encoding:\", df_encoded.shape)\n</pre> df_encoded = df_processed.copy()  # Mapeamento booleano/tri-estado boolean_mappings = {     \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},     \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1} } for col, mapping in boolean_mappings.items():     if col in df_encoded.columns:         df_encoded[col] = df_encoded[col].astype(str).map(mapping)         print(f\"   - {col}: {mapping}\")  # One-hot para categ\u00f3ricas categorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns] for col in categorical_cols:     unique_values = df_encoded[col].astype(str).unique()     print(f\"   - {col}: {len(unique_values)} categorias\")  df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False) print(\"Ap\u00f3s encoding:\", df_encoded.shape) <pre>   - CryoSleep: {'True': 1, 'False': 0, 'Unknown': -1}\n   - VIP: {'True': 1, 'False': 0, 'Unknown': -1}\n   - HomePlanet: 4 categorias\n   - Destination: 4 categorias\n   - Deck: 9 categorias\n   - Side: 3 categorias\nAp\u00f3s encoding: (8693, 30)\n</pre> In\u00a0[14]: Copied! <pre>print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\")\nprint(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")\n\ndef minmax_scale_to_neg1_pos1(series):\n    return 2 * ((series - series.min()) / (series.max() - series.min())) - 1\n\ndf_normalized = df_encoded.copy()\noriginal_data = {}\n\nscaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns]\nfor col in scaling_cols:\n    original_data[col] = df_normalized[col].copy()\n    original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"\n    df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])\n    normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"\n    print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")\n\nprint(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape)\n</pre> print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\") print(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")  def minmax_scale_to_neg1_pos1(series):     return 2 * ((series - series.min()) / (series.max() - series.min())) - 1  df_normalized = df_encoded.copy() original_data = {}  scaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns] for col in scaling_cols:     original_data[col] = df_normalized[col].copy()     original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"     df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])     normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"     print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")  print(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape) <pre>\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\nM\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\n   - Age: [0.0, 79.0] \u2192 [-1.000, 1.000]\n   - RoomService: [0.0, 14327.0] \u2192 [-1.000, 1.000]\n   - FoodCourt: [0.0, 29813.0] \u2192 [-1.000, 1.000]\n   - ShoppingMall: [0.0, 23492.0] \u2192 [-1.000, 1.000]\n   - Spa: [0.0, 22408.0] \u2192 [-1.000, 1.000]\n   - VRDeck: [0.0, 24133.0] \u2192 [-1.000, 1.000]\n   - CabinNum: [0.0, 1894.0] \u2192 [-1.000, 1.000]\nAp\u00f3s normaliza\u00e7\u00e3o: (8693, 30)\n</pre> In\u00a0[15]: Copied! <pre>target_col = \"Transported\"\nX = df_normalized.drop(columns=[target_col]).values\ny = df_normalized[target_col].map({True: 1, False: 0}).values\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n</pre> target_col = \"Transported\" X = df_normalized.drop(columns=[target_col]).values y = df_normalized[target_col].map({True: 1, False: 0}).values print(\"X shape:\", X.shape, \"| y shape:\", y.shape) <pre>X shape: (8693, 29) | y shape: (8693,)\n</pre> <p>4. Visualize the Data:</p> <ul> <li>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation</li> </ul> In\u00a0[16]: Copied! <pre>for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:\n    plt.figure(figsize=(12, 5))\n\n    # Antes\n    plt.subplot(1, 2, 1)\n    plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    # Depois\n    plt.subplot(1, 2, 2)\n    plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (normalizado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:     plt.figure(figsize=(12, 5))      # Antes     plt.subplot(1, 2, 1)     plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (original)\")     plt.ylabel(\"Frequ\u00eancia\")      # Depois     plt.subplot(1, 2, 2)     plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (normalizado)\")     plt.ylabel(\"Frequ\u00eancia\")      plt.tight_layout()     plt.show()"},{"location":"Exercicios/EX1/data/#1-data","title":"1. Data\u00b6","text":""},{"location":"Exercicios/EX1/data/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks\u00b6","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"Exercicios/EX1/data/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/EX1/data/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/EX1/data/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Preparing Real-World Data for a Neural Network This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"Exercicios/EX2/perceptron/","title":"2. Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [1.5, 1.5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [5, 5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]].</li> </ul> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> In\u00a0[2]: Copied! <pre>n_por_classe=1000\nnp.random.seed(42)\n\nmean0 = np.array([1.5, 1.5])\nmean1 = np.array([5, 5])\n\ncov = np.array([[0.5, 0], \n                [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia\n\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\nidx = np.random.permutation(len(X))\nX, y = X[idx], y[idx]\n</pre> n_por_classe=1000 np.random.seed(42)  mean0 = np.array([1.5, 1.5]) mean1 = np.array([5, 5])  cov = np.array([[0.5, 0],                  [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia  X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  idx = np.random.permutation(len(X)) X, y = X[idx], y[idx] In\u00a0[3]: Copied! <pre>plt.figure(figsize=(6, 6))\nplt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0')\nplt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Dados 2D - duas classes')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(6, 6)) plt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0') plt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1') plt.xlabel('x1') plt.ylabel('x2') plt.title('Dados 2D - duas classes') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li><p>Initialize weights (w) as a 2D vector (plus a bias term b).</p> </li> <li><p>Use the perceptron learning rule: For each misclassified sample <code>(x, y)</code>, update <code>w = w + \u03b7 * y * x</code> and <code>b = b + \u03b7 * y</code>, where <code>\u03b7</code> is the learning rate (start with 0.1).</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</p> </li> <li><p>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by <code>w * x + b = 0</code>) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> In\u00a0[4]: Copied! <pre>y_pm1 = np.where(y == 1, 1, -1).astype(int)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\nprint(\"w inicial:\", w, \"b inicial:\", b)\n\n# --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) ---\nscores = X @ w + b\npreds = np.where(scores &gt;= 0.0, 1, -1)\n\nprint(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5])\n</pre> y_pm1 = np.where(y == 1, 1, -1).astype(int) w = np.zeros(2, dtype=float) b = 0.0  print(\"w inicial:\", w, \"b inicial:\", b)  # --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) --- scores = X @ w + b preds = np.where(scores &gt;= 0.0, 1, -1)  print(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5]) <pre>w inicial: [0. 0.] b inicial: 0.0\nprimeiras predi\u00e7\u00f5es (sem treino): [1 1 1 1 1]\n</pre> In\u00a0[5]: Copied! <pre># --- Checar se um ponto foi mal classificado ---\nx0 = X[0]\ny0 = y_pm1[0]\nmisclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0\nprint(\"primeiro ponto est\u00e1 errado?\", misclassified)\n\n# --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) ---\neta = 0.1\nif misclassified:\n    w = w + eta * y0 * x0\n    b = b + eta * y0\n\nprint(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b)\n</pre> # --- Checar se um ponto foi mal classificado --- x0 = X[0] y0 = y_pm1[0] misclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0 print(\"primeiro ponto est\u00e1 errado?\", misclassified)  # --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) --- eta = 0.1 if misclassified:     w = w + eta * y0 * x0     b = b + eta * y0  print(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b) <pre>primeiro ponto est\u00e1 errado? True\nw ap\u00f3s poss\u00edvel update: [0.51299906 0.69042624] b: 0.1\n</pre> In\u00a0[6]: Copied! <pre># Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# Hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\nnp.random.seed(42)\n\n# Inicializa\u00e7\u00e3o\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:            # misclassified (ou na margem)\n            w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso\n            b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s\n            updates += 1\n\n    # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)\n    scores = X @ w + b\n    y_pred = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred == y_pm1).mean()\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    if updates == 0:\n        print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")\n        break\n\n# Resultados finais\nfinal_epoch = len(accuracies)\nfinal_acc = accuracies[-1]\nprint(\"\\n--------- Finais ---------\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas:\", final_epoch)\nprint(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\")\nprint(\"--------------------------\")\n</pre> # Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1 y_pm1 = np.where(y == 1, 1, -1).astype(int)  # Hiperpar\u00e2metros eta = 0.01 max_epochs = 100 np.random.seed(42)  # Inicializa\u00e7\u00e3o w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:            # misclassified (ou na margem)             w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso             b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s             updates += 1      # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)     scores = X @ w + b     y_pred = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred == y_pm1).mean()     accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      if updates == 0:         print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")         break  # Resultados finais final_epoch = len(accuracies) final_acc = accuracies[-1] print(\"\\n--------- Finais ---------\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas:\", final_epoch) print(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\") print(\"--------------------------\")  <pre>\u00c9poca   1 | updates:   60 | acc: 0.9905\n\u00c9poca   2 | updates:   24 | acc: 0.9995\n\u00c9poca   3 | updates:    8 | acc: 0.9950\n\u00c9poca   4 | updates:    6 | acc: 1.0000\n\u00c9poca   5 | updates:    0 | acc: 1.0000\nConverg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\n\n--------- Finais ---------\nw: [0.0643648  0.04329078]\nb: -0.36000000000000015\n\u00e9pocas: 5\nacur\u00e1cia final: 1.0000\n--------------------------\n</pre> In\u00a0[7]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# --- 3A) Acur\u00e1cia por \u00e9poca ---\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca')\naxes[0].grid(True, alpha=0.3)\n\n# --- 3B) Fronteira de decis\u00e3o sobre os dados + erros ---\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\n# fronteira\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\n# erros\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1],\n                    s=40, marker='x', linewidths=1.5,\n                    label=f'Erros ({mis_idx.size})')\n\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(\"w final:\", w, \"| b final:\", b)\n</pre> fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # --- 3A) Acur\u00e1cia por \u00e9poca --- axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca') axes[0].grid(True, alpha=0.3)  # --- 3B) Fronteira de decis\u00e3o sobre os dados + erros --- scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  # fronteira x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  # erros if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1],                     s=40, marker='x', linewidths=1.5,                     label=f'Erros ({mis_idx.size})')  axes[1].set_title('Perceptron \u2014 Dados e Fronteira') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show()  print(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\") print(\"w final:\", w, \"| b final:\", b)  <pre>Total de pontos: 2000 | Erros: 0 | Acur\u00e1cia final: 1.0000\nw final: [0.0643648  0.04329078] | b final: -0.36000000000000015\n</pre> <p>Answer: O perceptron funciona muito bem quando h\u00e1 separabilidade linear com boa margem, convergindo r\u00e1pido e com fronteira simples. Logo, os dados gerados s\u00e3o ideais para o perceptron, que consegue encontrar uma fronteira linear eficaz. A baixa vari\u00e2ncia e a dist\u00e2ncia entre as m\u00e9dias das classes minimizam sobreposi\u00e7\u00f5es, facilitando a classifica\u00e7\u00e3o correta. Assim, o perceptron atinge alta acur\u00e1cia rapidamente, demonstrando sua efic\u00e1cia em cen\u00e1rios de separabilidade linear clara.</p> <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [3, 3],</li> <li>Covariance = [[1.5, 0], [0, 1.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [4, 4],</li> <li>Covariance = [[1.5, 0], [0, 1.5]].</li> </ul> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> In\u00a0[8]: Copied! <pre># ---- Par\u00e2metros do Ex.2 ----\nnp.random.seed(42)\nn_por_classe = 1000\n\nmean0 = np.array([3.0, 3.0])\nmean1 = np.array([4.0, 4.0])\n\ncov = np.array([[1.5, 0.0],\n                [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap\n\n# ---- Amostragem (sem fun\u00e7\u00f5es) ----\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\n# Embaralha para uso posterior\nidx = np.random.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</pre> # ---- Par\u00e2metros do Ex.2 ---- np.random.seed(42) n_por_classe = 1000  mean0 = np.array([3.0, 3.0]) mean1 = np.array([4.0, 4.0])  cov = np.array([[1.5, 0.0],                 [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap  # ---- Amostragem (sem fun\u00e7\u00f5es) ---- X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  # Embaralha para uso posterior idx = np.random.permutation(len(X)) X = X[idx] y = y[idx]  In\u00a0[9]: Copied! <pre># ---- Visualiza\u00e7\u00e3o: overlap entre classes ----\nplt.figure(figsize=(6,6))\nplt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\nplt.xlabel('x1'); plt.ylabel('x2')\nplt.title('Ex.2 \u2014 Dados 2D (overlap parcial)')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # ---- Visualiza\u00e7\u00e3o: overlap entre classes ---- plt.figure(figsize=(6,6)) plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8) plt.xlabel('x1'); plt.ylabel('x2') plt.title('Ex.2 \u2014 Dados 2D (overlap parcial)') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li><p>Follow the same initialization, update rule, and training process.</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</p> </li> <li><p>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> In\u00a0[10]: Copied! <pre>np.random.seed(42)\n\n# r\u00f3tulos em {-1,+1}\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\n\n# inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    # embaralhar a ordem a cada \u00e9poca\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra (regra do perceptron)\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:           # misclassified\n            w = w + eta * yi * xi\n            b = b + eta * yi\n            updates += 1\n\n    # acur\u00e1cia na \u00e9poca (dataset completo)\n    scores = X @ w + b\n    y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred_pm1 == y_pm1).mean()\n\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)\n    if updates == 0:\n        print(\"Parada por aus\u00eancia de updates (raro com overlap).\")\n        break\n\n# m\u00e9tricas finais\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\nprint(\"\\n--- Finais ---\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas executadas:\", len(accuracies))\nprint(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(f\"erros: {mis_idx.size} de {len(X)}\")\n\n# ---------- Plots lado a lado ----------\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# (A) acur\u00e1cia por \u00e9poca\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)')\naxes[0].grid(True, alpha=0.3)\n\n# (B) fronteira de decis\u00e3o + erros\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',\n                    linewidths=1.5, label=f'Erros ({mis_idx.size})')\n\naxes[1].set_xlabel('x1'); axes[1].set_ylabel('x2')\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n</pre> np.random.seed(42)  # r\u00f3tulos em {-1,+1} y_pm1 = np.where(y == 1, 1, -1).astype(int)  # hiperpar\u00e2metros eta = 0.01 max_epochs = 100  # inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser) w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     # embaralhar a ordem a cada \u00e9poca     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra (regra do perceptron)     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:           # misclassified             w = w + eta * yi * xi             b = b + eta * yi             updates += 1      # acur\u00e1cia na \u00e9poca (dataset completo)     scores = X @ w + b     y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred_pm1 == y_pm1).mean()      accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)     if updates == 0:         print(\"Parada por aus\u00eancia de updates (raro com overlap).\")         break  # m\u00e9tricas finais scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  print(\"\\n--- Finais ---\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas executadas:\", len(accuracies)) print(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\") print(f\"erros: {mis_idx.size} de {len(X)}\")  # ---------- Plots lado a lado ---------- fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # (A) acur\u00e1cia por \u00e9poca axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)') axes[0].grid(True, alpha=0.3)  # (B) fronteira de decis\u00e3o + erros axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',                     linewidths=1.5, label=f'Erros ({mis_idx.size})')  axes[1].set_xlabel('x1'); axes[1].set_ylabel('x2') axes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show() <pre>\u00c9poca   1 | updates:  849 | acc: 0.6915\n\u00c9poca   2 | updates:  768 | acc: 0.6670\n\u00c9poca   3 | updates:  816 | acc: 0.6550\n\u00c9poca   4 | updates:  799 | acc: 0.5845\n\u00c9poca   5 | updates:  768 | acc: 0.6940\n\u00c9poca   6 | updates:  780 | acc: 0.6590\n\u00c9poca   7 | updates:  799 | acc: 0.6025\n\u00c9poca   8 | updates:  764 | acc: 0.5430\n\u00c9poca   9 | updates:  784 | acc: 0.6835\n\u00c9poca  10 | updates:  757 | acc: 0.5690\n\u00c9poca  11 | updates:  743 | acc: 0.5015\n\u00c9poca  12 | updates:  755 | acc: 0.6795\n\u00c9poca  13 | updates:  748 | acc: 0.6630\n\u00c9poca  14 | updates:  752 | acc: 0.5005\n\u00c9poca  15 | updates:  777 | acc: 0.5320\n\u00c9poca  16 | updates:  737 | acc: 0.6680\n</pre> <pre>\u00c9poca  17 | updates:  773 | acc: 0.5640\n\u00c9poca  18 | updates:  790 | acc: 0.5865\n\u00c9poca  19 | updates:  790 | acc: 0.6645\n\u00c9poca  20 | updates:  798 | acc: 0.6375\n\u00c9poca  21 | updates:  782 | acc: 0.7050\n\u00c9poca  22 | updates:  768 | acc: 0.6740\n\u00c9poca  23 | updates:  750 | acc: 0.5000\n\u00c9poca  24 | updates:  776 | acc: 0.5000\n\u00c9poca  25 | updates:  741 | acc: 0.5000\n\u00c9poca  26 | updates:  762 | acc: 0.6285\n\u00c9poca  27 | updates:  778 | acc: 0.6725\n\u00c9poca  28 | updates:  770 | acc: 0.6980\n\u00c9poca  29 | updates:  720 | acc: 0.6210\n\u00c9poca  30 | updates:  761 | acc: 0.5010\n\u00c9poca  31 | updates:  776 | acc: 0.6595\n\u00c9poca  32 | updates:  761 | acc: 0.5620\n\u00c9poca  33 | updates:  775 | acc: 0.5000\n\u00c9poca  34 | updates:  772 | acc: 0.5000\n\u00c9poca  35 | updates:  761 | acc: 0.6105\n\u00c9poca  36 | updates:  779 | acc: 0.6980\n\u00c9poca  37 | updates:  778 | acc: 0.5815\n\u00c9poca  38 | updates:  742 | acc: 0.6785\n</pre> <pre>\u00c9poca  39 | updates:  785 | acc: 0.6925\n\u00c9poca  40 | updates:  749 | acc: 0.6985\n\u00c9poca  41 | updates:  746 | acc: 0.6550\n\u00c9poca  42 | updates:  799 | acc: 0.5900\n\u00c9poca  43 | updates:  780 | acc: 0.5145\n\u00c9poca  44 | updates:  745 | acc: 0.5010\n\u00c9poca  45 | updates:  770 | acc: 0.6335\n\u00c9poca  46 | updates:  784 | acc: 0.5845\n\u00c9poca  47 | updates:  789 | acc: 0.6975\n\u00c9poca  48 | updates:  767 | acc: 0.5185\n\u00c9poca  49 | updates:  751 | acc: 0.5820\n\u00c9poca  50 | updates:  759 | acc: 0.5000\n\u00c9poca  51 | updates:  762 | acc: 0.5020\n\u00c9poca  52 | updates:  753 | acc: 0.5230\n\u00c9poca  53 | updates:  765 | acc: 0.5780\n\u00c9poca  54 | updates:  778 | acc: 0.5505\n</pre> <pre>\u00c9poca  55 | updates:  728 | acc: 0.6365\n\u00c9poca  56 | updates:  780 | acc: 0.6035\n\u00c9poca  57 | updates:  789 | acc: 0.7045\n\u00c9poca  58 | updates:  761 | acc: 0.6790\n\u00c9poca  59 | updates:  755 | acc: 0.6195\n\u00c9poca  60 | updates:  755 | acc: 0.5770\n\u00c9poca  61 | updates:  789 | acc: 0.6650\n\u00c9poca  62 | updates:  768 | acc: 0.6520\n\u00c9poca  63 | updates:  776 | acc: 0.6225\n\u00c9poca  64 | updates:  768 | acc: 0.6635\n\u00c9poca  65 | updates:  800 | acc: 0.6935\n\u00c9poca  66 | updates:  780 | acc: 0.6580\n\u00c9poca  67 | updates:  773 | acc: 0.6560\n\u00c9poca  68 | updates:  768 | acc: 0.5915\n\u00c9poca  69 | updates:  777 | acc: 0.6385\n\u00c9poca  70 | updates:  762 | acc: 0.5000\n\u00c9poca  71 | updates:  811 | acc: 0.6360\n\u00c9poca  72 | updates:  761 | acc: 0.5865\n\u00c9poca  73 | updates:  762 | acc: 0.6370\n\u00c9poca  74 | updates:  767 | acc: 0.6595\n\u00c9poca  75 | updates:  771 | acc: 0.7005\n\u00c9poca  76 | updates:  775 | acc: 0.7005\n</pre> <pre>\u00c9poca  77 | updates:  783 | acc: 0.5265\n\u00c9poca  78 | updates:  786 | acc: 0.6995\n\u00c9poca  79 | updates:  763 | acc: 0.5000\n\u00c9poca  80 | updates:  773 | acc: 0.6415\n\u00c9poca  81 | updates:  748 | acc: 0.5685\n\u00c9poca  82 | updates:  738 | acc: 0.6575\n\u00c9poca  83 | updates:  793 | acc: 0.5815\n\u00c9poca  84 | updates:  764 | acc: 0.5745\n\u00c9poca  85 | updates:  779 | acc: 0.5870\n\u00c9poca  86 | updates:  765 | acc: 0.5395\n\u00c9poca  87 | updates:  787 | acc: 0.6665\n\u00c9poca  88 | updates:  770 | acc: 0.5475\n\u00c9poca  89 | updates:  753 | acc: 0.5015\n\u00c9poca  90 | updates:  779 | acc: 0.5735\n\u00c9poca  91 | updates:  801 | acc: 0.5005\n\u00c9poca  92 | updates:  787 | acc: 0.6385\n</pre> <pre>\u00c9poca  93 | updates:  747 | acc: 0.7025\n\u00c9poca  94 | updates:  793 | acc: 0.5675\n\u00c9poca  95 | updates:  768 | acc: 0.6485\n\u00c9poca  96 | updates:  778 | acc: 0.6695\n\u00c9poca  97 | updates:  760 | acc: 0.6520\n\u00c9poca  98 | updates:  777 | acc: 0.7025\n\u00c9poca  99 | updates:  784 | acc: 0.6280\n\u00c9poca 100 | updates:  807 | acc: 0.5035\n\n--- Finais ---\nw: [0.02953414 0.05292094]\nb: -0.5200000000000002\n\u00e9pocas executadas: 100\nacur\u00e1cia final: 0.5035\nerros: 993 de 2000\n</pre> <p>Answer: O overlap introduzido no Ex.2 mostra uma limita\u00e7\u00e3o fundamental do perceptron cl\u00e1ssico: ele s\u00f3 converge se os dados forem linearmente separ\u00e1veis. Em cen\u00e1rios mais realistas, com ru\u00eddo ou vari\u00e2ncia maior, o perceptron n\u00e3o converge e a acur\u00e1cia fica limitada. Isso motiva a ado\u00e7\u00e3o de variantes como o Perceptron com Margem, regress\u00e3o log\u00edstica, SVM ou redes neurais multicamadas, que lidam melhor com dados n\u00e3o separ\u00e1veis.</p>"},{"location":"Exercicios/EX2/perceptron/#2-perceptron","title":"2. Perceptron\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#activity-understanding-perceptrons-and-their-limitations","title":"Activity: Understanding Perceptrons and Their Limitations\u00b6","text":"<p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"Exercicios/EX2/perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""}]}