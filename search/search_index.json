{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ementa","text":"Teste GitHub Pages Teste com Lorem Ipsum <p>Esse \u00e9 um exemplo de p\u00e1gina simples para GitHub Pages.</p> Se\u00e7\u00e3o 1 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet felis in nunc fringilla ullamcorper. Proin non lacus vitae ligula pulvinar facilisis.</p> Se\u00e7\u00e3o 2 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vitae venenatis ligula. Ut malesuada augue nec mi tempor, eu malesuada libero hendrerit.</p> Se\u00e7\u00e3o 3 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tincidunt, ipsum at sagittis tincidunt, risus ipsum cursus lorem, non dictum ipsum sapien quis elit.</p> <p>Rodap\u00e9 - P\u00e1gina de Teste</p>"},{"location":"Exercicios/EX1/data/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class</p> In\u00a0[2]: Copied! <pre>np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.\n\n# M\u00e9dias e desvios para cada classe\nmeans = [(2, 3), (5, 6), (8, 1), (15, 4)]\nstds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]\n\ndata = []\nlabels = []\n\nfor i in range(len(means)):\n    mean = means[i]\n    std = stds[i]\n\n    x = np.random.normal(loc=mean[0], scale=std[0], size=100)\n    y = np.random.normal(loc=mean[1], scale=std[1], size=100)\n\n    points = []\n    for j in range(100):\n        points.append([x[j], y[j]])\n\n    data.extend(points)\n\n    for j in range(100):\n        labels.append(i)\n\ndata = np.array(data)\nlabels = np.array(labels)\n</pre> np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.  # M\u00e9dias e desvios para cada classe means = [(2, 3), (5, 6), (8, 1), (15, 4)] stds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]  data = [] labels = []  for i in range(len(means)):     mean = means[i]     std = stds[i]      x = np.random.normal(loc=mean[0], scale=std[0], size=100)     y = np.random.normal(loc=mean[1], scale=std[1], size=100)      points = []     for j in range(100):         points.append([x[j], y[j]])      data.extend(points)      for j in range(100):         labels.append(i)  data = np.array(data) labels = np.array(labels) <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nfor cls in range(4):\n    plt.scatter(data[labels == cls, 0],\n                data[labels == cls, 1],\n                label=f'Classe {cls}',\n                alpha=0.7)\nplt.legend()\nplt.xlabel('Eixo X')\nplt.ylabel('Eixo Y')\nplt.title('Dispers\u00e3o das 4 classes em 2D')\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) for cls in range(4):     plt.scatter(data[labels == cls, 0],                 data[labels == cls, 1],                 label=f'Classe {cls}',                 alpha=0.7) plt.legend() plt.xlabel('Eixo X') plt.ylabel('Eixo Y') plt.title('Dispers\u00e3o das 4 classes em 2D') plt.show() <p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p>Answers</p> <ol> <li>O scatter plot mostra que as quatro classes est\u00e3o distribuidas de forma relativamente clara, com alguma sobreposi\u00e7\u00e3o entre elas. As classes 0 e 1 est\u00e3o mais pr\u00f3ximas uma da outra, enquanto as classes 2 ainda encosta um pouco na classe 1, e longe de todas as outras temos a classe 3.</li> <li>N\u00e3o, uma fronteira linear simples n\u00e3o seria capaz de separar todas as classes de forma eficaz, especialmente devido \u00e0 sobreposi\u00e7\u00e3o entre as classes 0 e 1.</li> <li>Pode ser visto no gr\u00e1fico abaixo:</li> </ol> <p></p> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B.</p> In\u00a0[4]: Copied! <pre>mu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1.0]])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],\n                    [-0.7, 1.5,  0.4, 0.0, 0.0],\n                    [0.2,  0.4,  1.5, 0.6, 0.0],\n                    [0.0,  0.0,  0.6, 1.5, 0.3],\n                    [0.0,  0.0,  0.0, 0.3, 1.5]])\n\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)\n</pre> mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],                     [0.8, 1.0, 0.3, 0.0, 0.0],                     [0.1, 0.3, 1.0, 0.5, 0.0],                     [0.0, 0.0, 0.5, 1.0, 0.2],                     [0.0, 0.0, 0.0, 0.2, 1.0]])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],                     [-0.7, 1.5,  0.4, 0.0, 0.0],                     [0.2,  0.4,  1.5, 0.6, 0.0],                     [0.0,  0.0,  0.6, 1.5, 0.3],                     [0.0,  0.0,  0.0, 0.3, 1.5]])  XA = np.random.multivariate_normal(mu_A, Sigma_A, size=500) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack([XA, XB]) y = np.array([0]*500 + [1]*500) <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>PCA:</p> <ol> <li>Centralizar os dados (tirar a m\u00e9dia).</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Extrair autovalores e autovetores da matriz de covari\u00e2ncia.</li> <li>Ordenar autovetores pelos maiores autovalores.</li> <li>Projetar os dados nos autovetores escolhidos.</li> </ol> In\u00a0[5]: Copied! <pre>def my_pca(X, n_components=None):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered, rowvar=False)\n    autovalores, autovetores = np.linalg.eigh(cov_matrix)\n\n    sorted_idx = np.argsort(autovalores)[::-1]\n    autovalores = autovalores[sorted_idx]\n    autovetores = autovetores[:, sorted_idx]\n\n    total_var = np.sum(autovalores)\n\n    if n_components is not None:\n        autovetores = autovetores[:, :n_components]\n        autovalores = autovalores[:n_components]\n\n    X_pca = np.dot(X_centered, autovetores)\n\n    return X_pca, autovetores, autovalores, total_var\n</pre> def my_pca(X, n_components=None):     X_centered = X - np.mean(X, axis=0)     cov_matrix = np.cov(X_centered, rowvar=False)     autovalores, autovetores = np.linalg.eigh(cov_matrix)      sorted_idx = np.argsort(autovalores)[::-1]     autovalores = autovalores[sorted_idx]     autovetores = autovetores[:, sorted_idx]      total_var = np.sum(autovalores)      if n_components is not None:         autovetores = autovetores[:, :n_components]         autovalores = autovalores[:n_components]      X_pca = np.dot(X_centered, autovetores)      return X_pca, autovetores, autovalores, total_var In\u00a0[6]: Copied! <pre>Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\nplt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)')\nplt.legend(); plt.show()\n\nprint('Vari\u00e2ncia explicada:', autovalores / total_var)\nprint('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var))\n</pre> Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)  plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A') plt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B') plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)') plt.legend(); plt.show()  print('Vari\u00e2ncia explicada:', autovalores / total_var) print('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var)) <pre>Vari\u00e2ncia explicada: [0.52303265 0.15751841]\nVari\u00e2ncia Acumulada: 0.6805510605944183\n</pre> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> <p>Answers</p> <ol> <li>As duas classes est\u00e3o bem pr\u00f3ximas uma da outra, com uma quantidade significativa de sobreposi\u00e7\u00e3o. Caso n\u00e3o fossem duas classes diferentes, poder\u00edamos considerar que se tratam de uma \u00fanica classe.</li> <li>N\u00e3o \u00e9 possivel tra\u00e7ar uma linha que separa as duas classes de forma eficaz, sempre haver\u00e1 uma \u00e1rea de sobreposi\u00e7\u00e3o. Modelos lineares simples, como um Perceptron ou Regress\u00e3o Log\u00edstica, v\u00e3o ter dificuldade porque s\u00f3 conseguem aprender fronteiras lineares (hiperplanos). Eles errariam bastante nos pontos da regi\u00e3o central de overlap.</li> </ol> <p>2. Describe the Data:</p> <ol> <li>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</li> <li>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ol> <p>Answers:</p> <ol> <li><p>O dataset busca prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da Spaceship Titanic. Isso pode ser visto na coluna \"Transported\", que \u00e9 a coluna-alvo (sendo booleano).</p> </li> <li><p>Features: num\u00e9ricas vs categ\u00f3ricas</p> <p>Num\u00e9ricas</p> <ul> <li><p>Age \u2192 idade do passageiro</p> </li> <li><p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 valores gastos em diferentes servi\u00e7os</p> </li> </ul> <p>Categ\u00f3ricas</p> <ul> <li><p>HomePlanet \u2192 planeta de origem (ex: Earth, Europa, Mars)</p> </li> <li><p>CryoSleep \u2192 booleano (se o passageiro entrou em sono criog\u00eanico)</p> </li> <li><p>Cabin \u2192 cont\u00e9m m\u00faltiplas infos (deck/num/side). Pode ser decomposta em:</p> <ul> <li><p>Deck (categ\u00f3rica)</p> </li> <li><p>Num (num\u00e9rica)</p> </li> <li><p>Side (P/S \u2192 categ\u00f3rica bin\u00e1ria)</p> </li> </ul> </li> <li><p>Destination \u2192 destino da viagem (ex: TRAPPIST-1e, etc.)</p> </li> <li><p>VIP \u2192 booleano (pagou servi\u00e7o VIP)</p> </li> <li><p>Name \u2192 geralmente descartado (n\u00e3o tem valor preditivo direto)</p> </li> <li><p>PassengerId \u2192 identificador \u00fanico (n\u00e3o usado como feature)</p> </li> </ul> </li> <li><p>Colunas com valores faltantes:</p> </li> </ol> In\u00a0[7]: Copied! <pre>csv_path = \"train.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\nprint(\"Colunas:\", list(df.columns))\n</pre> csv_path = \"train.csv\" df = pd.read_csv(csv_path) print(\"Shape:\", df.shape) print(\"Colunas:\", list(df.columns)) <pre>Shape: (8693, 14)\nColunas: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n</pre> In\u00a0[8]: Copied! <pre>missing = df.isnull().sum()\nmissing_percent = 100 * missing / len(df)\nmissing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent})\nprint(missing_df[missing_df[\"Missing Values\"] &gt; 0])\n</pre> missing = df.isnull().sum() missing_percent = 100 * missing / len(df) missing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent}) print(missing_df[missing_df[\"Missing Values\"] &gt; 0]) <pre>              Missing Values   Percent\nHomePlanet               201  2.312205\nCryoSleep                217  2.496261\nCabin                    199  2.289198\nDestination              182  2.093639\nAge                      179  2.059128\nVIP                      203  2.335212\nRoomService              181  2.082135\nFoodCourt                183  2.105142\nShoppingMall             208  2.392730\nSpa                      183  2.105142\nVRDeck                   188  2.162660\nName                     200  2.300702\n</pre> <p>3. Preprocessing the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ol> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ol> In\u00a0[9]: Copied! <pre>df_processed = df.copy()\n\n\"\"\"Num\u00e9ricas: \nSolu\u00e7\u00e3o: Preencher com mediana, \nJustificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o.\n\"\"\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    median_val = df_processed[col].median()\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(median_val, inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")\n</pre> df_processed = df.copy()  \"\"\"Num\u00e9ricas:  Solu\u00e7\u00e3o: Preencher com mediana,  Justificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o. \"\"\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] for col in num_cols:     median_val = df_processed[col].median()     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(median_val, inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")  <pre>   - Age: 179 valores preenchidos com 27.00\n   - RoomService: 181 valores preenchidos com 0.00\n   - FoodCourt: 183 valores preenchidos com 0.00\n   - ShoppingMall: 208 valores preenchidos com 0.00\n   - Spa: 183 valores preenchidos com 0.00\n   - VRDeck: 188 valores preenchidos com 0.00\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\n</pre> In\u00a0[10]: Copied! <pre>\"\"\"Categ\u00f3ricas: \nSolu\u00e7\u00e3o: Preencher com \"Unknown\"\nJustificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios.\n\"\"\"\ncat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]\nfor col in cat_cols:\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(\"Unknown\", inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos\")\n</pre> \"\"\"Categ\u00f3ricas:  Solu\u00e7\u00e3o: Preencher com \"Unknown\" Justificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios. \"\"\" cat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"] for col in cat_cols:     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(\"Unknown\", inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos\")  <pre>   - HomePlanet: 201 valores preenchidos\n   - Destination: 182 valores preenchidos\n   - CryoSleep: 217 valores preenchidos\n   - VIP: 203 valores preenchidos\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\797796973.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(\"Unknown\", inplace=True)\n</pre> In\u00a0[11]: Copied! <pre>\"\"\"Cabin: \nSolu\u00e7\u00e3o: Separar em 3 colunas\nJustificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes.\n\"\"\"\ncabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True)\ndf_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\")\ndf_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")\n\n# Preencher CabinNum com mediana\ncabin_num_median = df_processed[\"CabinNum\"].median()\ndf_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> \"\"\"Cabin:  Solu\u00e7\u00e3o: Separar em 3 colunas Justificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes. \"\"\" cabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True) df_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\") df_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\") df_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")  # Preencher CabinNum com mediana cabin_num_median = df_processed[\"CabinNum\"].median() df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True) <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_3180\\2304158112.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> In\u00a0[12]: Copied! <pre>\"\"\"Name: \nSolu\u00e7\u00e3o: descartar\nJustificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido.\n\"\"\"\ndf_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)    \n\nprint(\"Ap\u00f3s tratamento:\", df_processed.shape)\n</pre> \"\"\"Name:  Solu\u00e7\u00e3o: descartar Justificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido. \"\"\" df_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)      print(\"Ap\u00f3s tratamento:\", df_processed.shape) <pre>Ap\u00f3s tratamento: (8693, 14)\n</pre> In\u00a0[13]: Copied! <pre>df_encoded = df_processed.copy()\n\n# Mapeamento booleano/tri-estado\nboolean_mappings = {\n    \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},\n    \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1}\n}\nfor col, mapping in boolean_mappings.items():\n    if col in df_encoded.columns:\n        df_encoded[col] = df_encoded[col].astype(str).map(mapping)\n        print(f\"   - {col}: {mapping}\")\n\n# One-hot para categ\u00f3ricas\ncategorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns]\nfor col in categorical_cols:\n    unique_values = df_encoded[col].astype(str).unique()\n    print(f\"   - {col}: {len(unique_values)} categorias\")\n\ndf_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False)\nprint(\"Ap\u00f3s encoding:\", df_encoded.shape)\n</pre> df_encoded = df_processed.copy()  # Mapeamento booleano/tri-estado boolean_mappings = {     \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},     \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1} } for col, mapping in boolean_mappings.items():     if col in df_encoded.columns:         df_encoded[col] = df_encoded[col].astype(str).map(mapping)         print(f\"   - {col}: {mapping}\")  # One-hot para categ\u00f3ricas categorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns] for col in categorical_cols:     unique_values = df_encoded[col].astype(str).unique()     print(f\"   - {col}: {len(unique_values)} categorias\")  df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False) print(\"Ap\u00f3s encoding:\", df_encoded.shape) <pre>   - CryoSleep: {'True': 1, 'False': 0, 'Unknown': -1}\n   - VIP: {'True': 1, 'False': 0, 'Unknown': -1}\n   - HomePlanet: 4 categorias\n   - Destination: 4 categorias\n   - Deck: 9 categorias\n   - Side: 3 categorias\nAp\u00f3s encoding: (8693, 30)\n</pre> In\u00a0[14]: Copied! <pre>print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\")\nprint(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")\n\ndef minmax_scale_to_neg1_pos1(series):\n    return 2 * ((series - series.min()) / (series.max() - series.min())) - 1\n\ndf_normalized = df_encoded.copy()\noriginal_data = {}\n\nscaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns]\nfor col in scaling_cols:\n    original_data[col] = df_normalized[col].copy()\n    original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"\n    df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])\n    normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"\n    print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")\n\nprint(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape)\n</pre> print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\") print(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")  def minmax_scale_to_neg1_pos1(series):     return 2 * ((series - series.min()) / (series.max() - series.min())) - 1  df_normalized = df_encoded.copy() original_data = {}  scaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns] for col in scaling_cols:     original_data[col] = df_normalized[col].copy()     original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"     df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])     normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"     print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")  print(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape) <pre>\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\nM\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\n   - Age: [0.0, 79.0] \u2192 [-1.000, 1.000]\n   - RoomService: [0.0, 14327.0] \u2192 [-1.000, 1.000]\n   - FoodCourt: [0.0, 29813.0] \u2192 [-1.000, 1.000]\n   - ShoppingMall: [0.0, 23492.0] \u2192 [-1.000, 1.000]\n   - Spa: [0.0, 22408.0] \u2192 [-1.000, 1.000]\n   - VRDeck: [0.0, 24133.0] \u2192 [-1.000, 1.000]\n   - CabinNum: [0.0, 1894.0] \u2192 [-1.000, 1.000]\nAp\u00f3s normaliza\u00e7\u00e3o: (8693, 30)\n</pre> In\u00a0[15]: Copied! <pre>target_col = \"Transported\"\nX = df_normalized.drop(columns=[target_col]).values\ny = df_normalized[target_col].map({True: 1, False: 0}).values\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n</pre> target_col = \"Transported\" X = df_normalized.drop(columns=[target_col]).values y = df_normalized[target_col].map({True: 1, False: 0}).values print(\"X shape:\", X.shape, \"| y shape:\", y.shape) <pre>X shape: (8693, 29) | y shape: (8693,)\n</pre> <p>4. Visualize the Data:</p> <ul> <li>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation</li> </ul> In\u00a0[16]: Copied! <pre>for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:\n    plt.figure(figsize=(12, 5))\n\n    # Antes\n    plt.subplot(1, 2, 1)\n    plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    # Depois\n    plt.subplot(1, 2, 2)\n    plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (normalizado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:     plt.figure(figsize=(12, 5))      # Antes     plt.subplot(1, 2, 1)     plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (original)\")     plt.ylabel(\"Frequ\u00eancia\")      # Depois     plt.subplot(1, 2, 2)     plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (normalizado)\")     plt.ylabel(\"Frequ\u00eancia\")      plt.tight_layout()     plt.show()"},{"location":"Exercicios/EX1/data/#1-data","title":"1. Data\u00b6","text":""},{"location":"Exercicios/EX1/data/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks\u00b6","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"Exercicios/EX1/data/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/EX1/data/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/EX1/data/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Preparing Real-World Data for a Neural Network This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"Exercicios/EX2/perceptron/","title":"2. Perceptron","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [1.5, 1.5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [5, 5],</li> <li>Covariance = [[0.5, 0], [0, 0.5]].</li> </ul> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> In\u00a0[2]: Copied! <pre>n_por_classe=1000\nnp.random.seed(42)\n\nmean0 = np.array([1.5, 1.5])\nmean1 = np.array([5, 5])\n\ncov = np.array([[0.5, 0], \n                [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia\n\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\nidx = np.random.permutation(len(X))\nX, y = X[idx], y[idx]\n</pre> n_por_classe=1000 np.random.seed(42)  mean0 = np.array([1.5, 1.5]) mean1 = np.array([5, 5])  cov = np.array([[0.5, 0],                  [0, 0.5]])   # vari\u00e2ncia 0.5 em cada eixo, sem covari\u00e2ncia  X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  idx = np.random.permutation(len(X)) X, y = X[idx], y[idx] In\u00a0[3]: Copied! <pre>plt.figure(figsize=(6, 6))\nplt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0')\nplt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Dados 2D - duas classes')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(6, 6)) plt.scatter(X[y==0, 0], X[y==0, 1], s=8, label='Classe 0') plt.scatter(X[y==1, 0], X[y==1, 1], s=8, label='Classe 1') plt.xlabel('x1') plt.ylabel('x2') plt.title('Dados 2D - duas classes') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li><p>Initialize weights (w) as a 2D vector (plus a bias term b).</p> </li> <li><p>Use the perceptron learning rule: For each misclassified sample <code>(x, y)</code>, update <code>w = w + \u03b7 * y * x</code> and <code>b = b + \u03b7 * y</code>, where <code>\u03b7</code> is the learning rate (start with 0.1).</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</p> </li> <li><p>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by <code>w * x + b = 0</code>) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> In\u00a0[4]: Copied! <pre>y_pm1 = np.where(y == 1, 1, -1).astype(int)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\nprint(\"w inicial:\", w, \"b inicial:\", b)\n\n# --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) ---\nscores = X @ w + b\npreds = np.where(scores &gt;= 0.0, 1, -1)\n\nprint(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5])\n</pre> y_pm1 = np.where(y == 1, 1, -1).astype(int) w = np.zeros(2, dtype=float) b = 0.0  print(\"w inicial:\", w, \"b inicial:\", b)  # --- Fazer predi\u00e7\u00e3o (signo de w\u00b7x + b) --- scores = X @ w + b preds = np.where(scores &gt;= 0.0, 1, -1)  print(\"primeiras predi\u00e7\u00f5es (sem treino):\", preds[:5]) <pre>w inicial: [0. 0.] b inicial: 0.0\nprimeiras predi\u00e7\u00f5es (sem treino): [1 1 1 1 1]\n</pre> In\u00a0[5]: Copied! <pre># --- Checar se um ponto foi mal classificado ---\nx0 = X[0]\ny0 = y_pm1[0]\nmisclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0\nprint(\"primeiro ponto est\u00e1 errado?\", misclassified)\n\n# --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) ---\neta = 0.1\nif misclassified:\n    w = w + eta * y0 * x0\n    b = b + eta * y0\n\nprint(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b)\n</pre> # --- Checar se um ponto foi mal classificado --- x0 = X[0] y0 = y_pm1[0] misclassified = y0 * (np.dot(w, x0) + b) &lt;= 0.0 print(\"primeiro ponto est\u00e1 errado?\", misclassified)  # --- Aplicar regra de atualiza\u00e7\u00e3o (s\u00f3 se errou) --- eta = 0.1 if misclassified:     w = w + eta * y0 * x0     b = b + eta * y0  print(\"w ap\u00f3s poss\u00edvel update:\", w, \"b:\", b) <pre>primeiro ponto est\u00e1 errado? True\nw ap\u00f3s poss\u00edvel update: [0.51299906 0.69042624] b: 0.1\n</pre> In\u00a0[6]: Copied! <pre># Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# Hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\nnp.random.seed(42)\n\n# Inicializa\u00e7\u00e3o\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:            # misclassified (ou na margem)\n            w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso\n            b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s\n            updates += 1\n\n    # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)\n    scores = X @ w + b\n    y_pred = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred == y_pm1).mean()\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    if updates == 0:\n        print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")\n        break\n\n# Resultados finais\nfinal_epoch = len(accuracies)\nfinal_acc = accuracies[-1]\nprint(\"\\n--------- Finais ---------\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas:\", final_epoch)\nprint(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\")\nprint(\"--------------------------\")\n</pre> # Pressup\u00f5e que X (n,2) e y em {0,1} j\u00e1 existem da Parte 1 y_pm1 = np.where(y == 1, 1, -1).astype(int)  # Hiperpar\u00e2metros eta = 0.01 max_epochs = 100 np.random.seed(42)  # Inicializa\u00e7\u00e3o w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:            # misclassified (ou na margem)             w = w + eta * yi * xi    # atualiza\u00e7\u00e3o do peso             b = b + eta * yi         # atualiza\u00e7\u00e3o do vi\u00e9s             updates += 1      # medir acur\u00e1cia nesta \u00e9poca (no dataset completo)     scores = X @ w + b     y_pred = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred == y_pm1).mean()     accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      if updates == 0:         print(\"Converg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\")         break  # Resultados finais final_epoch = len(accuracies) final_acc = accuracies[-1] print(\"\\n--------- Finais ---------\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas:\", final_epoch) print(\"acur\u00e1cia final:\", f\"{final_acc:.4f}\") print(\"--------------------------\")  <pre>\u00c9poca   1 | updates:   60 | acc: 0.9905\n\u00c9poca   2 | updates:   24 | acc: 0.9995\n\u00c9poca   3 | updates:    8 | acc: 0.9950\n\u00c9poca   4 | updates:    6 | acc: 1.0000\n\u00c9poca   5 | updates:    0 | acc: 1.0000\nConverg\u00eancia atingida (nenhuma atualiza\u00e7\u00e3o nesta \u00e9poca).\n\n--------- Finais ---------\nw: [0.0643648  0.04329078]\nb: -0.36000000000000015\n\u00e9pocas: 5\nacur\u00e1cia final: 1.0000\n--------------------------\n</pre> In\u00a0[7]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# --- 3A) Acur\u00e1cia por \u00e9poca ---\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca')\naxes[0].grid(True, alpha=0.3)\n\n# --- 3B) Fronteira de decis\u00e3o sobre os dados + erros ---\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\n# fronteira\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\n# erros\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1],\n                    s=40, marker='x', linewidths=1.5,\n                    label=f'Erros ({mis_idx.size})')\n\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(\"w final:\", w, \"| b final:\", b)\n</pre> fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # --- 3A) Acur\u00e1cia por \u00e9poca --- axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca') axes[0].grid(True, alpha=0.3)  # --- 3B) Fronteira de decis\u00e3o sobre os dados + erros --- scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  # fronteira x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  # erros if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1],                     s=40, marker='x', linewidths=1.5,                     label=f'Erros ({mis_idx.size})')  axes[1].set_title('Perceptron \u2014 Dados e Fronteira') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show()  print(f\"Total de pontos: {len(X)} | Erros: {mis_idx.size} | Acur\u00e1cia final: {accuracies[-1]:.4f}\") print(\"w final:\", w, \"| b final:\", b)  <pre>Total de pontos: 2000 | Erros: 0 | Acur\u00e1cia final: 1.0000\nw final: [0.0643648  0.04329078] | b final: -0.36000000000000015\n</pre> <p>Answer: O perceptron funciona muito bem quando h\u00e1 separabilidade linear com boa margem, convergindo r\u00e1pido e com fronteira simples. Logo, os dados gerados s\u00e3o ideais para o perceptron, que consegue encontrar uma fronteira linear eficaz. A baixa vari\u00e2ncia e a dist\u00e2ncia entre as m\u00e9dias das classes minimizam sobreposi\u00e7\u00f5es, facilitando a classifica\u00e7\u00e3o correta. Assim, o perceptron atinge alta acur\u00e1cia rapidamente, demonstrando sua efic\u00e1cia em cen\u00e1rios de separabilidade linear clara.</p> <p>Data Generation Task: Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li><p>Class 0:</p> <ul> <li>Mean = [3, 3],</li> <li>Covariance = [[1.5, 0], [0, 1.5]] (i.e., variance of along each dimension, no covariance).</li> </ul> </li> <li><p>Class 1:</p> <ul> <li>Mean = [4, 4],</li> <li>Covariance = [[1.5, 0], [0, 1.5]].</li> </ul> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> In\u00a0[8]: Copied! <pre># ---- Par\u00e2metros do Ex.2 ----\nnp.random.seed(42)\nn_por_classe = 1000\n\nmean0 = np.array([3.0, 3.0])\nmean1 = np.array([4.0, 4.0])\n\ncov = np.array([[1.5, 0.0],\n                [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap\n\n# ---- Amostragem (sem fun\u00e7\u00f5es) ----\nX0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe)\nX1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_por_classe, dtype=int),\n               np.ones(n_por_classe, dtype=int)])\n\n# Embaralha para uso posterior\nidx = np.random.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</pre> # ---- Par\u00e2metros do Ex.2 ---- np.random.seed(42) n_por_classe = 1000  mean0 = np.array([3.0, 3.0]) mean1 = np.array([4.0, 4.0])  cov = np.array([[1.5, 0.0],                 [0.0, 1.5]])   # vari\u00e2ncia maior (1.5) -&gt; mais overlap  # ---- Amostragem (sem fun\u00e7\u00f5es) ---- X0 = np.random.multivariate_normal(mean0, cov, size=n_por_classe) X1 = np.random.multivariate_normal(mean1, cov, size=n_por_classe)  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_por_classe, dtype=int),                np.ones(n_por_classe, dtype=int)])  # Embaralha para uso posterior idx = np.random.permutation(len(X)) X = X[idx] y = y[idx]  In\u00a0[9]: Copied! <pre># ---- Visualiza\u00e7\u00e3o: overlap entre classes ----\nplt.figure(figsize=(6,6))\nplt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\nplt.xlabel('x1'); plt.ylabel('x2')\nplt.title('Ex.2 \u2014 Dados 2D (overlap parcial)')\nplt.legend()\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # ---- Visualiza\u00e7\u00e3o: overlap entre classes ---- plt.figure(figsize=(6,6)) plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8) plt.xlabel('x1'); plt.ylabel('x2') plt.title('Ex.2 \u2014 Dados 2D (overlap parcial)') plt.legend() plt.axis('equal') plt.tight_layout() plt.show() <p>Perceptron Implementation Task: Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.</p> <ul> <li><p>Follow the same initialization, update rule, and training process.</p> </li> <li><p>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</p> </li> <li><p>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</p> </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> In\u00a0[10]: Copied! <pre>np.random.seed(42)\n\n# r\u00f3tulos em {-1,+1}\ny_pm1 = np.where(y == 1, 1, -1).astype(int)\n\n# hiperpar\u00e2metros\neta = 0.01\nmax_epochs = 100\n\n# inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser)\nw = np.zeros(2, dtype=float)\nb = 0.0\n\naccuracies = []\nupdates_per_epoch = []\n\nn = X.shape[0]\n\nfor epoch in range(1, max_epochs + 1):\n    # embaralhar a ordem a cada \u00e9poca\n    idx = np.random.permutation(n)\n    X_epoch = X[idx]\n    y_epoch = y_pm1[idx]\n\n    updates = 0\n\n    # varrer amostra a amostra (regra do perceptron)\n    for xi, yi in zip(X_epoch, y_epoch):\n        margin = yi * (np.dot(w, xi) + b)\n        if margin &lt;= 0.0:           # misclassified\n            w = w + eta * yi * xi\n            b = b + eta * yi\n            updates += 1\n\n    # acur\u00e1cia na \u00e9poca (dataset completo)\n    scores = X @ w + b\n    y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)\n    acc = (y_pred_pm1 == y_pm1).mean()\n\n    accuracies.append(acc)\n    updates_per_epoch.append(updates)\n\n    print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")\n\n    # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)\n    if updates == 0:\n        print(\"Parada por aus\u00eancia de updates (raro com overlap).\")\n        break\n\n# m\u00e9tricas finais\nscores_final = X @ w + b\ny_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1)\nmis_idx = np.where(y_pred_pm1 != y_pm1)[0]\n\nprint(\"\\n--- Finais ---\")\nprint(\"w:\", w)\nprint(\"b:\", b)\nprint(\"\u00e9pocas executadas:\", len(accuracies))\nprint(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\")\nprint(f\"erros: {mis_idx.size} de {len(X)}\")\n\n# ---------- Plots lado a lado ----------\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# (A) acur\u00e1cia por \u00e9poca\naxes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o')\naxes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia')\naxes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)')\naxes[0].grid(True, alpha=0.3)\n\n# (B) fronteira de decis\u00e3o + erros\naxes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8)\naxes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)\n\nx1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\nxs = np.linspace(x1_min, x1_max, 200)\nif abs(w[1]) &gt; 1e-12:\n    ys = -(w[0]*xs + b) / w[1]\n    axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)')\nelse:\n    x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)\n    axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')\n\nif mis_idx.size &gt; 0:\n    axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',\n                    linewidths=1.5, label=f'Erros ({mis_idx.size})')\n\naxes[1].set_xlabel('x1'); axes[1].set_ylabel('x2')\naxes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)')\naxes[1].legend()\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n</pre> np.random.seed(42)  # r\u00f3tulos em {-1,+1} y_pm1 = np.where(y == 1, 1, -1).astype(int)  # hiperpar\u00e2metros eta = 0.01 max_epochs = 100  # inicializa\u00e7\u00e3o (zeros p/ reprodutibilidade; troque por randn se quiser) w = np.zeros(2, dtype=float) b = 0.0  accuracies = [] updates_per_epoch = []  n = X.shape[0]  for epoch in range(1, max_epochs + 1):     # embaralhar a ordem a cada \u00e9poca     idx = np.random.permutation(n)     X_epoch = X[idx]     y_epoch = y_pm1[idx]      updates = 0      # varrer amostra a amostra (regra do perceptron)     for xi, yi in zip(X_epoch, y_epoch):         margin = yi * (np.dot(w, xi) + b)         if margin &lt;= 0.0:           # misclassified             w = w + eta * yi * xi             b = b + eta * yi             updates += 1      # acur\u00e1cia na \u00e9poca (dataset completo)     scores = X @ w + b     y_pred_pm1 = np.where(scores &gt;= 0.0, 1, -1)     acc = (y_pred_pm1 == y_pm1).mean()      accuracies.append(acc)     updates_per_epoch.append(updates)      print(f\"\u00c9poca {epoch:3d} | updates: {updates:4d} | acc: {acc:.4f}\")      # crit\u00e9rio de parada por 'converg\u00eancia' (aqui pode n\u00e3o ocorrer por causa do overlap)     if updates == 0:         print(\"Parada por aus\u00eancia de updates (raro com overlap).\")         break  # m\u00e9tricas finais scores_final = X @ w + b y_pred_pm1 = np.where(scores_final &gt;= 0.0, 1, -1) mis_idx = np.where(y_pred_pm1 != y_pm1)[0]  print(\"\\n--- Finais ---\") print(\"w:\", w) print(\"b:\", b) print(\"\u00e9pocas executadas:\", len(accuracies)) print(f\"acur\u00e1cia final: {accuracies[-1]:.4f}\") print(f\"erros: {mis_idx.size} de {len(X)}\")  # ---------- Plots lado a lado ---------- fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # (A) acur\u00e1cia por \u00e9poca axes[0].plot(range(1, len(accuracies)+1), accuracies, marker='o') axes[0].set_xlabel('\u00c9poca'); axes[0].set_ylabel('Acur\u00e1cia') axes[0].set_title('Perceptron \u2014 Acur\u00e1cia por \u00c9poca (Ex.2)') axes[0].grid(True, alpha=0.3)  # (B) fronteira de decis\u00e3o + erros axes[1].scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0', alpha=0.8) axes[1].scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1', alpha=0.8)  x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5 xs = np.linspace(x1_min, x1_max, 200) if abs(w[1]) &gt; 1e-12:     ys = -(w[0]*xs + b) / w[1]     axes[1].plot(xs, ys, linewidth=2, label='Fronteira (w\u00b7x+b=0)') else:     x_vertical = -b / (w[0] if abs(w[0]) &gt; 1e-12 else 1e-12)     axes[1].axvline(x_vertical, linewidth=2, label='Fronteira (vertical)')  if mis_idx.size &gt; 0:     axes[1].scatter(X[mis_idx,0], X[mis_idx,1], s=40, marker='x',                     linewidths=1.5, label=f'Erros ({mis_idx.size})')  axes[1].set_xlabel('x1'); axes[1].set_ylabel('x2') axes[1].set_title('Perceptron \u2014 Dados e Fronteira (Ex.2)') axes[1].legend() axes[1].axis('equal')  plt.tight_layout() plt.show() <pre>\u00c9poca   1 | updates:  849 | acc: 0.6915\n\u00c9poca   2 | updates:  768 | acc: 0.6670\n\u00c9poca   3 | updates:  816 | acc: 0.6550\n\u00c9poca   4 | updates:  799 | acc: 0.5845\n\u00c9poca   5 | updates:  768 | acc: 0.6940\n\u00c9poca   6 | updates:  780 | acc: 0.6590\n\u00c9poca   7 | updates:  799 | acc: 0.6025\n\u00c9poca   8 | updates:  764 | acc: 0.5430\n\u00c9poca   9 | updates:  784 | acc: 0.6835\n\u00c9poca  10 | updates:  757 | acc: 0.5690\n\u00c9poca  11 | updates:  743 | acc: 0.5015\n\u00c9poca  12 | updates:  755 | acc: 0.6795\n\u00c9poca  13 | updates:  748 | acc: 0.6630\n</pre> <pre>\u00c9poca  14 | updates:  752 | acc: 0.5005\n\u00c9poca  15 | updates:  777 | acc: 0.5320\n\u00c9poca  16 | updates:  737 | acc: 0.6680\n\u00c9poca  17 | updates:  773 | acc: 0.5640\n\u00c9poca  18 | updates:  790 | acc: 0.5865\n\u00c9poca  19 | updates:  790 | acc: 0.6645\n\u00c9poca  20 | updates:  798 | acc: 0.6375\n\u00c9poca  21 | updates:  782 | acc: 0.7050\n\u00c9poca  22 | updates:  768 | acc: 0.6740\n\u00c9poca  23 | updates:  750 | acc: 0.5000\n\u00c9poca  24 | updates:  776 | acc: 0.5000\n\u00c9poca  25 | updates:  741 | acc: 0.5000\n\u00c9poca  26 | updates:  762 | acc: 0.6285\n\u00c9poca  27 | updates:  778 | acc: 0.6725\n\u00c9poca  28 | updates:  770 | acc: 0.6980\n\u00c9poca  29 | updates:  720 | acc: 0.6210\n\u00c9poca  30 | updates:  761 | acc: 0.5010\n\u00c9poca  31 | updates:  776 | acc: 0.6595\n\u00c9poca  32 | updates:  761 | acc: 0.5620\n\u00c9poca  33 | updates:  775 | acc: 0.5000\n\u00c9poca  34 | updates:  772 | acc: 0.5000\n\u00c9poca  35 | updates:  761 | acc: 0.6105\n\u00c9poca  36 | updates:  779 | acc: 0.6980\n\u00c9poca  37 | updates:  778 | acc: 0.5815\n\u00c9poca  38 | updates:  742 | acc: 0.6785\n</pre> <pre>\u00c9poca  39 | updates:  785 | acc: 0.6925\n\u00c9poca  40 | updates:  749 | acc: 0.6985\n\u00c9poca  41 | updates:  746 | acc: 0.6550\n\u00c9poca  42 | updates:  799 | acc: 0.5900\n\u00c9poca  43 | updates:  780 | acc: 0.5145\n\u00c9poca  44 | updates:  745 | acc: 0.5010\n\u00c9poca  45 | updates:  770 | acc: 0.6335\n\u00c9poca  46 | updates:  784 | acc: 0.5845\n\u00c9poca  47 | updates:  789 | acc: 0.6975\n\u00c9poca  48 | updates:  767 | acc: 0.5185\n\u00c9poca  49 | updates:  751 | acc: 0.5820\n</pre> <pre>\u00c9poca  50 | updates:  759 | acc: 0.5000\n\u00c9poca  51 | updates:  762 | acc: 0.5020\n\u00c9poca  52 | updates:  753 | acc: 0.5230\n\u00c9poca  53 | updates:  765 | acc: 0.5780\n\u00c9poca  54 | updates:  778 | acc: 0.5505\n\u00c9poca  55 | updates:  728 | acc: 0.6365\n\u00c9poca  56 | updates:  780 | acc: 0.6035\n\u00c9poca  57 | updates:  789 | acc: 0.7045\n\u00c9poca  58 | updates:  761 | acc: 0.6790\n\u00c9poca  59 | updates:  755 | acc: 0.6195\n\u00c9poca  60 | updates:  755 | acc: 0.5770\n\u00c9poca  61 | updates:  789 | acc: 0.6650\n\u00c9poca  62 | updates:  768 | acc: 0.6520\n\u00c9poca  63 | updates:  776 | acc: 0.6225\n\u00c9poca  64 | updates:  768 | acc: 0.6635\n\u00c9poca  65 | updates:  800 | acc: 0.6935\n\u00c9poca  66 | updates:  780 | acc: 0.6580\n\u00c9poca  67 | updates:  773 | acc: 0.6560\n\u00c9poca  68 | updates:  768 | acc: 0.5915\n\u00c9poca  69 | updates:  777 | acc: 0.6385\n\u00c9poca  70 | updates:  762 | acc: 0.5000\n\u00c9poca  71 | updates:  811 | acc: 0.6360\n\u00c9poca  72 | updates:  761 | acc: 0.5865\n\u00c9poca  73 | updates:  762 | acc: 0.6370\n\u00c9poca  74 | updates:  767 | acc: 0.6595\n\u00c9poca  75 | updates:  771 | acc: 0.7005\n</pre> <pre>\u00c9poca  76 | updates:  775 | acc: 0.7005\n\u00c9poca  77 | updates:  783 | acc: 0.5265\n\u00c9poca  78 | updates:  786 | acc: 0.6995\n\u00c9poca  79 | updates:  763 | acc: 0.5000\n\u00c9poca  80 | updates:  773 | acc: 0.6415\n\u00c9poca  81 | updates:  748 | acc: 0.5685\n\u00c9poca  82 | updates:  738 | acc: 0.6575\n\u00c9poca  83 | updates:  793 | acc: 0.5815\n\u00c9poca  84 | updates:  764 | acc: 0.5745\n\u00c9poca  85 | updates:  779 | acc: 0.5870\n\u00c9poca  86 | updates:  765 | acc: 0.5395\n\u00c9poca  87 | updates:  787 | acc: 0.6665\n\u00c9poca  88 | updates:  770 | acc: 0.5475\n\u00c9poca  89 | updates:  753 | acc: 0.5015\n</pre> <pre>\u00c9poca  90 | updates:  779 | acc: 0.5735\n\u00c9poca  91 | updates:  801 | acc: 0.5005\n\u00c9poca  92 | updates:  787 | acc: 0.6385\n\u00c9poca  93 | updates:  747 | acc: 0.7025\n\u00c9poca  94 | updates:  793 | acc: 0.5675\n\u00c9poca  95 | updates:  768 | acc: 0.6485\n\u00c9poca  96 | updates:  778 | acc: 0.6695\n\u00c9poca  97 | updates:  760 | acc: 0.6520\n\u00c9poca  98 | updates:  777 | acc: 0.7025\n\u00c9poca  99 | updates:  784 | acc: 0.6280\n\u00c9poca 100 | updates:  807 | acc: 0.5035\n\n--- Finais ---\nw: [0.02953414 0.05292094]\nb: -0.5200000000000002\n\u00e9pocas executadas: 100\nacur\u00e1cia final: 0.5035\nerros: 993 de 2000\n</pre> <p>Answer: O overlap introduzido no Ex.2 mostra uma limita\u00e7\u00e3o fundamental do perceptron cl\u00e1ssico: ele s\u00f3 converge se os dados forem linearmente separ\u00e1veis. Em cen\u00e1rios mais realistas, com ru\u00eddo ou vari\u00e2ncia maior, o perceptron n\u00e3o converge e a acur\u00e1cia fica limitada. Isso motiva a ado\u00e7\u00e3o de variantes como o Perceptron com Margem, regress\u00e3o log\u00edstica, SVM ou redes neurais multicamadas, que lidam melhor com dados n\u00e3o separ\u00e1veis.</p>"},{"location":"Exercicios/EX2/perceptron/#2-perceptron","title":"2. Perceptron\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#activity-understanding-perceptrons-and-their-limitations","title":"Activity: Understanding Perceptrons and Their Limitations\u00b6","text":"<p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"Exercicios/EX2/perceptron/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX2/perceptron/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/","title":"3. MLP","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): L = 1/N * (y - \u0177)\u00b2, where \u0177 is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li><p>Input and output vectors:</p> <ul> <li>X: [0.5, -0.2]</li> <li>Y: 1.0</li> </ul> </li> <li><p>Hidden layer weights:</p> <ul> <li>W\u00b9 = [[0.3, -0.1], [0.2, 0.4]]  (2x2 matrix)</li> </ul> </li> <li><p>Hidden layer biases:</p> <ul> <li>b\u00b9 = [0.1, -0.2]  (1x2 vector)</li> </ul> </li> <li><p>Output layer weights:</p> <ul> <li>W\u00b2 = [0.5, -0.3]</li> </ul> </li> <li><p>Output layer bias:</p> <ul> <li>b\u00b2 = 0.2</li> </ul> </li> <li><p>Learning rate:</p> <ul> <li>\u03b7 = 0.3</li> </ul> </li> <li><p>Activation function: tanh</p> </li> </ul> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li><p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: Z\u00b9 = W\u00b9 * X + b\u00b9.</li> <li>Apply tanh to get hidden activations: a\u00b9 = tanh(Z\u00b9).</li> <li>Compute the output pre-activation: Z\u00b2 = W\u00b2 * a\u00b9 + b\u00b2.</li> <li>Compute the final output: \u0177 = tanh(Z\u00b2).</li> </ul> </li> <li><p>Loss Calculation:</p> <ul> <li>Compute the loss: L = 1/N * (Y - \u0177)\u00b2.</li> </ul> </li> <li><p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with delL/del\u0177 then compute:</p> <ul> <li>delL/delZ\u00b2 (using the tanh derivative: del/delZ tanh(Z) = 1 - tanh\u00b2(Z)).</li> <li>Gradients for output layer: delL/delW\u00b2, delL/delb\u00b2.</li> <li>Propagate to hidden layer: delL/delA\u00b9, delL/delZ\u00b9.</li> <li>Gradients for hidden layer: delL/delW\u00b9, delL/delb\u00b9.</li> <li>Show all intermediate steps and calculations.</li> </ul> </li> <li><p>Parameter Update: Using the learning rate \u03b7 = 0.1, update all weights and biases via gradient descent:</p> <ul> <li>W\u00b2 &lt;- W\u00b2 - \u03b7 * delL/delW\u00b2</li> <li>b\u00b2 &lt;- b\u00b2 - \u03b7 * delL/delb\u00b2</li> <li>W\u00b9 &lt;- W\u00b9 - \u03b7 * delL/delW\u00b9</li> <li>b\u00b9 &lt;- b\u00b9 - \u03b7 * delL/delb\u00b9</li> <li>Provide the numerical values for all updated parameters.</li> </ul> </li> </ol> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p> In\u00a0[2]: Copied! <pre># --- Helpers ---\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(z):\n    return 1.0 - np.tanh(z)**2\n\ndef fmt(x):\n    if isinstance(x, float):\n        return f\"{x:.6f}\"\n    arr = np.array(x, dtype=float)\n    return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},\n                           floatmode='maxprec', suppress_small=False)\n\ndef p(title, value):\n    print(f\"{title}: {fmt(value)}\")\n\n# --- Dados do exerc\u00edcio ---\nX = np.array([0.5, -0.2], dtype=float)\nY = 1.0 \n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)\n\nb1 = np.array([0.1, -0.2], dtype=float)\n\nW2 = np.array([0.5, -0.3], dtype=float)\nb2 = 0.2\n\neta_update = 0.1\n\nprint(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\")\np(\"X\", X); p(\"Y\", Y)\nprint(\"\\n--- Par\u00e2metros iniciais ---\")\np(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)\n</pre> # --- Helpers --- def tanh(x):     return np.tanh(x)  def dtanh(z):     return 1.0 - np.tanh(z)**2  def fmt(x):     if isinstance(x, float):         return f\"{x:.6f}\"     arr = np.array(x, dtype=float)     return np.array2string(arr, formatter={'float_kind':lambda v: f\"{v:.6f}\"},                            floatmode='maxprec', suppress_small=False)  def p(title, value):     print(f\"{title}: {fmt(value)}\")  # --- Dados do exerc\u00edcio --- X = np.array([0.5, -0.2], dtype=float) Y = 1.0   W1 = np.array([[0.3, -0.1],                [0.2,  0.4]], dtype=float)  b1 = np.array([0.1, -0.2], dtype=float)  W2 = np.array([0.5, -0.3], dtype=float) b2 = 0.2  eta_update = 0.1  print(\"=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\\n\") p(\"X\", X); p(\"Y\", Y) print(\"\\n--- Par\u00e2metros iniciais ---\") p(\"W1\", W1); p(\"b1\", b1); p(\"W2\", W2); p(\"b2\", b2)  <pre>=== EXERC\u00cdCIO 1 \u2014 MLP (tanh, MSE) ===\n\nX: [0.500000 -0.200000]\nY: 1.000000\n\n--- Par\u00e2metros iniciais ---\nW1: [[0.300000 -0.100000]\n [0.200000 0.400000]]\nb1: [0.100000 -0.200000]\nW2: [0.500000 -0.300000]\nb2: 0.200000\n</pre> In\u00a0[3]: Copied! <pre># === 1) Forward pass ===\nZ1 = W1 @ X + b1\nA1 = tanh(Z1)\nZ2 = float(W2 @ A1 + b2)\nY_hat = float(tanh(Z2))\n\nprint(\"\\n--- Forward Pass ---\")\np(\"Z1 = W1 @ X + b1\", Z1)\np(\"A1 = tanh(Z1)\", A1)\np(\"Z2 = W2 \u00b7 A1 + b2\", Z2)\np(\"\u0177 = tanh(Z2)\", Y_hat)\n\n# === 2) Loss ===\nL = (Y - Y_hat)**2\nprint(\"\\n--- Loss (MSE) ---\")\np(\"L = (Y - \u0177)^2\", L)\n</pre> # === 1) Forward pass === Z1 = W1 @ X + b1 A1 = tanh(Z1) Z2 = float(W2 @ A1 + b2) Y_hat = float(tanh(Z2))  print(\"\\n--- Forward Pass ---\") p(\"Z1 = W1 @ X + b1\", Z1) p(\"A1 = tanh(Z1)\", A1) p(\"Z2 = W2 \u00b7 A1 + b2\", Z2) p(\"\u0177 = tanh(Z2)\", Y_hat)  # === 2) Loss === L = (Y - Y_hat)**2 print(\"\\n--- Loss (MSE) ---\") p(\"L = (Y - \u0177)^2\", L)  <pre>\n--- Forward Pass ---\nZ1 = W1 @ X + b1: [0.270000 -0.180000]\nA1 = tanh(Z1): [0.263625 -0.178081]\nZ2 = W2 \u00b7 A1 + b2: 0.385237\n\u0177 = tanh(Z2): 0.367247\n\n--- Loss (MSE) ---\nL = (Y - \u0177)^2: 0.400377\n</pre> In\u00a0[4]: Copied! <pre># === 3) Backpropagation ===\n\n# Sa\u00edda\ndL_dYhat = 2.0*(Y_hat - Y)\ndYhat_dZ2 = dtanh(Z2)\ndL_dZ2 = dL_dYhat * dYhat_dZ2\n\nprint(\"\\n-- Sa\u00edda --\")\np(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat)\np(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2)\np(\"dL/dZ2\", dL_dZ2)\n\n# Gradientes da camada de sa\u00edda\ndL_dW2 = dL_dZ2 * A1            # (2,)\ndL_db2 = dL_dZ2                 # escalar\n\nprint(\"\\n-- Gradientes camada de sa\u00edda --\")\np(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2)\np(\"dL/db2 = dL/dZ2\", dL_db2)\n\n# Propaga\u00e7\u00e3o p/ camada oculta\ndL_dA1 = dL_dZ2 * W2            # (2,)\ndA1_dZ1 = dtanh(Z1)             # (2,)\ndL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)\n\nprint(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\")\np(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1)\np(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1)\np(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)\n\n# Gradientes da camada oculta\ndL_dW1 = np.outer(dL_dZ1, X)    # (2,2)\ndL_db1 = dL_dZ1                 # (2,)\n\nprint(\"\\n-- Gradientes camada oculta --\")\np(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1)\np(\"dL/db1 = dL/dZ1\", dL_db1)\n</pre> # === 3) Backpropagation ===  # Sa\u00edda dL_dYhat = 2.0*(Y_hat - Y) dYhat_dZ2 = dtanh(Z2) dL_dZ2 = dL_dYhat * dYhat_dZ2  print(\"\\n-- Sa\u00edda --\") p(\"dL/d\u0177 = 2*(\u0177 - Y)\", dL_dYhat) p(\"dtanh(Z2) = 1 - tanh^2(Z2)\", dYhat_dZ2) p(\"dL/dZ2\", dL_dZ2)  # Gradientes da camada de sa\u00edda dL_dW2 = dL_dZ2 * A1            # (2,) dL_db2 = dL_dZ2                 # escalar  print(\"\\n-- Gradientes camada de sa\u00edda --\") p(\"dL/dW2 = dL/dZ2 * A1\", dL_dW2) p(\"dL/db2 = dL/dZ2\", dL_db2)  # Propaga\u00e7\u00e3o p/ camada oculta dL_dA1 = dL_dZ2 * W2            # (2,) dA1_dZ1 = dtanh(Z1)             # (2,) dL_dZ1 = dL_dA1 * dA1_dZ1       # (2,)  print(\"\\n-- Propaga\u00e7\u00e3o para a oculta --\") p(\"dL/dA1 = dL/dZ2 * W2\", dL_dA1) p(\"dtanh(Z1) = 1 - tanh^2(Z1)\", dA1_dZ1) p(\"dL/dZ1 = dL/dA1 \u2299 dtanh(Z1)\", dL_dZ1)  # Gradientes da camada oculta dL_dW1 = np.outer(dL_dZ1, X)    # (2,2) dL_db1 = dL_dZ1                 # (2,)  print(\"\\n-- Gradientes camada oculta --\") p(\"dL/dW1 = outer(dL/dZ1, X)\", dL_dW1) p(\"dL/db1 = dL/dZ1\", dL_db1) <pre>\n-- Sa\u00edda --\ndL/d\u0177 = 2*(\u0177 - Y): -1.265507\ndtanh(Z2) = 1 - tanh^2(Z2): 0.865130\ndL/dZ2: -1.094828\n\n-- Gradientes camada de sa\u00edda --\ndL/dW2 = dL/dZ2 * A1: [-0.288624 0.194968]\ndL/db2 = dL/dZ2: -1.094828\n\n-- Propaga\u00e7\u00e3o para a oculta --\ndL/dA1 = dL/dZ2 * W2: [-0.547414 0.328448]\ndtanh(Z1) = 1 - tanh^2(Z1): [0.930502 0.968287]\ndL/dZ1 = dL/dA1 \u2299 dtanh(Z1): [-0.509370 0.318032]\n\n-- Gradientes camada oculta --\ndL/dW1 = outer(dL/dZ1, X): [[-0.254685 0.101874]\n [0.159016 -0.063606]]\ndL/db1 = dL/dZ1: [-0.509370 0.318032]\n</pre> In\u00a0[5]: Copied! <pre># === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) ===\nW2_new = W2 - eta_update * dL_dW2\nb2_new = b2 - eta_update * dL_db2\nW1_new = W1 - eta_update * dL_dW1\nb1_new = b1 - eta_update * dL_db1\n\np(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new)\np(\"b2_new = b2 - \u03b7*dL/db2\", b2_new)\np(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new)\np(\"b1_new = b1 - \u03b7*dL/db1\", b1_new)\n</pre> # === 4) Atualiza\u00e7\u00e3o de par\u00e2metros (\u03b7 = 0.1) === W2_new = W2 - eta_update * dL_dW2 b2_new = b2 - eta_update * dL_db2 W1_new = W1 - eta_update * dL_dW1 b1_new = b1 - eta_update * dL_db1  p(\"\\nW2_new = W2 - \u03b7*dL/dW2\", W2_new) p(\"b2_new = b2 - \u03b7*dL/db2\", b2_new) p(\"W1_new = W1 - \u03b7*dL/dW1\", W1_new) p(\"b1_new = b1 - \u03b7*dL/db1\", b1_new) <pre>\nW2_new = W2 - \u03b7*dL/dW2: [0.528862 -0.319497]\nb2_new = b2 - \u03b7*dL/db2: 0.309483\nW1_new = W1 - \u03b7*dL/dW1: [[0.325468 -0.110187]\n [0.184098 0.406361]]\nb1_new = b1 - \u03b7*dL/db1: [0.150937 -0.231803]\n</pre> In\u00a0[6]: Copied! <pre># === Checagem opcional: forward com par\u00e2metros atualizados ===\nZ1_new = W1_new @ X + b1_new\nA1_new = tanh(Z1_new)\nZ2_new = float(W2_new @ A1_new + b2_new)\nY_hat_new = float(tanh(Z2_new))\nL_new = (Y - Y_hat_new)**2\n\np(\"\\n\u0177 (antes)\", Y_hat)\np(\"L (antes)\", L)\np(\"\u0177 (depois)\", Y_hat_new)\np(\"L (depois)\", L_new)\n</pre> # === Checagem opcional: forward com par\u00e2metros atualizados === Z1_new = W1_new @ X + b1_new A1_new = tanh(Z1_new) Z2_new = float(W2_new @ A1_new + b2_new) Y_hat_new = float(tanh(Z2_new)) L_new = (Y - Y_hat_new)**2  p(\"\\n\u0177 (antes)\", Y_hat) p(\"L (antes)\", L) p(\"\u0177 (depois)\", Y_hat_new) p(\"L (depois)\", L_new)  <pre>\n\u0177 (antes): 0.367247\nL (antes): 0.400377\n\u0177 (depois): 0.500620\nL (depois): 0.249380\n</pre> <p>Using the <code>make_classification</code> function from scikit-learn, generate a synthetic dataset with the following specifications:</p> <ul> <li><p>Number of samples: 1000</p> </li> <li><p>Number of classes: 2</p> </li> <li><p>Number of clusters per class: Use the n_clusters_per_class parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</p> </li> <li><p>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</p> </li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li><p>Number of hidden layers (at least 1)</p> </li> <li><p>Number of neurons per layer</p> </li> <li><p>Activation functions (e.g., sigmoid, ReLU, tanh)</p> </li> <li><p>Loss function (e.g., binary cross-entropy)</p> </li> <li><p>Optimizer (e.g., gradient descent, with a chosen learning rate)</p> </li> </ul> <p>Steps to follow:</p> <ol> <li><p>Generate and split the data into training (80%) and testing (20%) sets.</p> </li> <li><p>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</p> </li> <li><p>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</p> </li> <li><p>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</p> </li> <li><p>Submit your code and results, including any visualizations.</p> </li> </ol> In\u00a0[7]: Copied! <pre>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(42)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid(a):\n    # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)\n    return a * (1.0 - a)\n\ndef bce_loss(y_true, y_pred, eps=1e-12):\n    # y_true, y_pred com shape (m,1)\n    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n    return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))\n\ndef accuracy(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    return (y_hat == y_true).mean()\n\ndef confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):\n    y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)\n    tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())\n    tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())\n    fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())\n    fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())\n    return np.array([[tn, fp],\n                     [fn, tp]])\n</pre> from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  np.random.seed(42)  def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))  def dsigmoid(a):     # Se j\u00e1 temos a = sigmoid(z), d/dz sigmoid = a*(1-a)     return a * (1.0 - a)  def bce_loss(y_true, y_pred, eps=1e-12):     # y_true, y_pred com shape (m,1)     y_pred = np.clip(y_pred, eps, 1.0 - eps)     return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))  def accuracy(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     return (y_hat == y_true).mean()  def confusion_matrix_manual(y_true, y_pred_prob, thresh=0.5):     y_hat = (y_pred_prob &gt;= thresh).astype(np.int32)     tp = int(((y_true == 1) &amp; (y_hat == 1)).sum())     tn = int(((y_true == 0) &amp; (y_hat == 0)).sum())     fp = int(((y_true == 0) &amp; (y_hat == 1)).sum())     fn = int(((y_true == 1) &amp; (y_hat == 0)).sum())     return np.array([[tn, fp],                      [fn, tp]]) In\u00a0[8]: Copied! <pre># Par\u00e2metros gerais\nN_total = 1000\nclass_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador\nflip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados)\nrandom_state = 42\n\n# 1) Classe 0 com 1 cluster\nX0, y0 = make_classification(\n    n_samples=N_total//2, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=1, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state\n)\n# Filtra apenas a classe 0\nX0 = X0[y0 == 0]\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1\nX1_full, y1_full = make_classification(\n    n_samples=N_total, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=2, n_classes=2,\n    class_sep=class_sep, flip_y=flip_y,\n    random_state=random_state + 1\n)\nX1 = X1_full[y1_full == 1]\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Balanceia o tamanho: escolhe min entre os dois lados\nn = min(len(X0), len(X1))\nX0 = X0[:n]\ny0 = y0[:n]\nX1 = X1[:n]\ny1 = y1[:n]\n\n# Combina\nX = np.vstack([X0, X1])\ny = np.concatenate([y0, y1])\n\n# Embaralha\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")\n\n# Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o)\nplt.figure()\nplt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8)\nplt.title(\"Dados sint\u00e9ticos (antes do split)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Par\u00e2metros gerais N_total = 1000 class_sep = 1.6      # pode ajustar p/ ficar mais ou menos desafiador flip_y = 0.02        # fra\u00e7\u00e3o de ru\u00eddo (r\u00f3tulos trocados) random_state = 42  # 1) Classe 0 com 1 cluster X0, y0 = make_classification(     n_samples=N_total//2, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=1, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state ) # Filtra apenas a classe 0 X0 = X0[y0 == 0] y0 = np.zeros((X0.shape[0],), dtype=int)  # 2) Classe 1 com 2 clusters: gera um conjunto com n_clusters_per_class=2 e pega s\u00f3 a classe 1 X1_full, y1_full = make_classification(     n_samples=N_total, n_features=2,     n_redundant=0, n_informative=2,     n_clusters_per_class=2, n_classes=2,     class_sep=class_sep, flip_y=flip_y,     random_state=random_state + 1 ) X1 = X1_full[y1_full == 1] y1 = np.ones((X1.shape[0],), dtype=int)  # Balanceia o tamanho: escolhe min entre os dois lados n = min(len(X0), len(X1)) X0 = X0[:n] y0 = y0[:n] X1 = X1[:n] y1 = y1[:n]  # Combina X = np.vstack([X0, X1]) y = np.concatenate([y0, y1])  # Embaralha perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(f\"X shape: {X.shape}, y shape: {y.shape}, classe 0: {np.sum(y==0)}, classe 1: {np.sum(y==1)}\")  # Visualiza\u00e7\u00e3o bruta (sem padroniza\u00e7\u00e3o) plt.figure() plt.scatter(X[y==0,0], X[y==0,1], s=12, label=\"Classe 0\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=12, label=\"Classe 1 (2 clusters)\", alpha=0.8) plt.title(\"Dados sint\u00e9ticos (antes do split)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>X shape: (498, 2), y shape: (498,), classe 0: 249, classe 1: 249\n</pre> In\u00a0[9]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Ajustar shapes de y p/ coluna\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  # Ajustar shapes de y p/ coluna y_train = y_train.reshape(-1, 1) y_test = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) <pre>(398, 2) (398, 1) (100, 2) (100, 1)\n</pre> In\u00a0[10]: Copied! <pre># Arquitetura\nn_in = 2\nn_hidden = 16\nn_out = 1\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef forward(Xb):\n    # Xb: (m, 2)\n    z1 = Xb @ W1 + b1          # (m, hidden)\n    a1 = tanh(z1)              # (m, hidden)\n    z2 = a1 @ W2 + b2          # (m, 1)\n    a2 = sigmoid(z2)           # (m, 1)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb):\n    # yb: (m,1)\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m\n    dz2 = (a2 - yb) / m                     # (m,1)\n    dW2 = a1.T @ dz2                        # (hidden,1)\n    db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)\n\n    da1 = dz2 @ W2.T                        # (m,hidden)\n    dz1 = da1 * dtanh(a1)                   # (m,hidden)\n    dW1 = Xb.T @ dz1                        # (2,hidden)\n    db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)\n\n    return dW1, db1, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = 2 n_hidden = 16 n_out = 1  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def forward(Xb):     # Xb: (m, 2)     z1 = Xb @ W1 + b1          # (m, hidden)     a1 = tanh(z1)              # (m, hidden)     z2 = a1 @ W2 + b2          # (m, 1)     a2 = sigmoid(z2)           # (m, 1)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb):     # yb: (m,1)     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # BCE + sigmoid -&gt; dL/dz2 = (a2 - y) / m     dz2 = (a2 - yb) / m                     # (m,1)     dW2 = a1.T @ dz2                        # (hidden,1)     db2 = np.sum(dz2, axis=0, keepdims=True)# (1,1)      da1 = dz2 @ W2.T                        # (m,hidden)     dz1 = da1 * dtanh(a1)                   # (m,hidden)     dW1 = Xb.T @ dz1                        # (2,hidden)     db1 = np.sum(dz1, axis=0, keepdims=True)# (1,hidden)      return dW1, db1, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[11]: Copied! <pre># Hiperpar\u00e2metros de treino\nepochs = 300\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nindices = np.arange(m)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(indices)\n    X_train_shuf = X_train[indices]\n    y_train_shuf = y_train[indices]\n\n    # mini-batches\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = X_train_shuf[start:end]\n        yb = y_train_shuf[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = bce_loss(yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        # Acur\u00e1cia de treino r\u00e1pida\n        yhat_full, _ = forward(X_train)\n        acc_tr = accuracy(y_train, yhat_full)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\n# Curva de loss\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist, marker=None)\nplt.title(\"Loss (treino) por \u00e9poca\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Binary Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Hiperpar\u00e2metros de treino epochs = 300 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] indices = np.arange(m)  for ep in range(1, epochs+1):     np.random.shuffle(indices)     X_train_shuf = X_train[indices]     y_train_shuf = y_train[indices]      # mini-batches     ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = X_train_shuf[start:end]         yb = y_train_shuf[start:end]          yhat, cache = forward(Xb)         loss = bce_loss(yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         # Acur\u00e1cia de treino r\u00e1pida         yhat_full, _ = forward(X_train)         acc_tr = accuracy(y_train, yhat_full)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  # Curva de loss plt.figure() plt.plot(range(1, epochs+1), loss_hist, marker=None) plt.title(\"Loss (treino) por \u00e9poca\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Binary Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.5989 | acc_train=0.9271\nEpoch   25 | loss=0.1912 | acc_train=0.9372\nEpoch   50 | loss=0.1536 | acc_train=0.9422\nEpoch   75 | loss=0.1555 | acc_train=0.9372\nEpoch  100 | loss=0.1495 | acc_train=0.9397\nEpoch  125 | loss=0.1472 | acc_train=0.9397\n</pre> <pre>Epoch  150 | loss=0.1438 | acc_train=0.9422\nEpoch  175 | loss=0.1459 | acc_train=0.9422\nEpoch  200 | loss=0.1496 | acc_train=0.9397\nEpoch  225 | loss=0.1481 | acc_train=0.9422\nEpoch  250 | loss=0.1531 | acc_train=0.9422\nEpoch  275 | loss=0.1605 | acc_train=0.9422\nEpoch  300 | loss=0.1393 | acc_train=0.9397\n</pre> In\u00a0[12]: Copied! <pre># Predi\u00e7\u00f5es no teste\nyprob_test, _ = forward(X_test)\nacc_te = accuracy(y_test, yprob_test)\ncm = confusion_matrix_manual(y_test, yprob_test)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\")\nprint(cm)\n</pre> # Predi\u00e7\u00f5es no teste yprob_test, _ = forward(X_test) acc_te = accuracy(y_test, yprob_test) cm = confusion_matrix_manual(y_test, yprob_test)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\") print(cm) <pre>Acur\u00e1cia (teste): 0.9600\nMatriz de confus\u00e3o [ [TN, FP], [FN, TP] ]:\n[[47  3]\n [ 1 49]]\n</pre> In\u00a0[13]: Copied! <pre># Grade para visualizar a fronteira\nx_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0\ny_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                     np.linspace(y_min, y_max, 300))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Lembre: padronizamos com 'scaler'\ngrid_std = scaler.transform(grid)\nprobs, _ = forward(grid_std)\nZZ = probs.reshape(xx.shape)\n\nplt.figure()\nplt.contourf(xx, yy, ZZ, levels=50, alpha=0.6)\nplt.colorbar(label=\"p(y=1)\")\nplt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8)\nplt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8)\nplt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Grade para visualizar a fronteira x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0 y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                      np.linspace(y_min, y_max, 300)) grid = np.c_[xx.ravel(), yy.ravel()]  # Lembre: padronizamos com 'scaler' grid_std = scaler.transform(grid) probs, _ = forward(grid_std) ZZ = probs.reshape(xx.shape)  plt.figure() plt.contourf(xx, yy, ZZ, levels=50, alpha=0.6) plt.colorbar(label=\"p(y=1)\") plt.scatter(X[y==0,0], X[y==0,1], s=10, label=\"Classe 0\", edgecolor=\"k\", alpha=0.8) plt.scatter(X[y==1,0], X[y==1,1], s=10, label=\"Classe 1 (2 clusters)\", edgecolor=\"k\", alpha=0.8) plt.title(\"Fronteira de decis\u00e3o (probabilidade y=1)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Synthetic Data and Reusable MLP</p> <p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol> In\u00a0[14]: Copied! <pre>from sklearn.decomposition import PCA\n\nnp.random.seed(42)\nplt.rcParams[\"figure.figsize\"] = (6, 5)\n# --- Ativa\u00e7\u00f5es ---\ndef softmax(z):\n    # z: (m, K)\n    z_shift = z - np.max(z, axis=1, keepdims=True)\n    e = np.exp(z_shift)\n    return e / np.sum(e, axis=1, keepdims=True)\n\n# --- Loss e m\u00e9tricas (multi-classe) ---\ndef cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):\n    # y_true_onehot, y_prob: (m, K)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))\n\ndef accuracy_multiclass(y_true, y_prob):\n    # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}\n    y_pred = np.argmax(y_prob, axis=1)\n    return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()\n\ndef confusion_matrix_k(y_true, y_prob, K):\n    y_pred = np.argmax(y_prob, axis=1)\n    cm = np.zeros((K, K), dtype=int)\n    for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):\n        cm[t, p] += 1\n    return cm\n</pre> from sklearn.decomposition import PCA  np.random.seed(42) plt.rcParams[\"figure.figsize\"] = (6, 5) # --- Ativa\u00e7\u00f5es --- def softmax(z):     # z: (m, K)     z_shift = z - np.max(z, axis=1, keepdims=True)     e = np.exp(z_shift)     return e / np.sum(e, axis=1, keepdims=True)  # --- Loss e m\u00e9tricas (multi-classe) --- def cross_entropy_onehot(y_true_onehot, y_prob, eps=1e-12):     # y_true_onehot, y_prob: (m, K)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -np.mean(np.sum(y_true_onehot * np.log(y_prob), axis=1))  def accuracy_multiclass(y_true, y_prob):     # y_true: (m,1) ou (m,), r\u00f3tulos {0..K-1}     y_pred = np.argmax(y_prob, axis=1)     return (y_pred.reshape(-1) == y_true.reshape(-1)).mean()  def confusion_matrix_k(y_true, y_prob, K):     y_pred = np.argmax(y_prob, axis=1)     cm = np.zeros((K, K), dtype=int)     for t, p in zip(y_true.reshape(-1), y_pred.reshape(-1)):         cm[t, p] += 1     return cm In\u00a0[15]: Copied! <pre>N_total = 1500\nK = 3\nn_features = 4\nclass_sep = 1.8\nflip_y = 0.00\n\n# Queremos tamanhos semelhantes por classe\ntarget_per_class = N_total // K  # 500\n\ndef gen_class_subset(target, n_clusters, rs):\n    # como vamos filtrar apenas a classe positiva, geramos o dobro\n    n_samples_tmp = target * 2\n    X_tmp, y_tmp = make_classification(\n        n_samples=n_samples_tmp,\n        n_features=n_features,\n        n_informative=n_features,\n        n_redundant=0,\n        n_classes=2,\n        n_clusters_per_class=n_clusters,\n        class_sep=class_sep,\n        flip_y=flip_y,\n        random_state=rs\n    )\n    X_pos = X_tmp[y_tmp == 1]\n    if len(X_pos) &lt; target:\n        # se por algum motivo veio menos, reamostrar com rs+1\n        X_more, y_more = make_classification(\n            n_samples=n_samples_tmp,\n            n_features=n_features,\n            n_informative=n_features,\n            n_redundant=0,\n            n_classes=2,\n            n_clusters_per_class=n_clusters,\n            class_sep=class_sep,\n            flip_y=flip_y,\n            random_state=rs+101\n        )\n        X_pos = np.vstack([X_pos, X_more[y_more == 1]])\n    return X_pos[:target]\n\n# Classe 0 -&gt; 2 clusters\nX0 = gen_class_subset(target_per_class, n_clusters=2, rs=42)\ny0 = np.zeros((X0.shape[0],), dtype=int)\n\n# Classe 1 -&gt; 3 clusters\nX1 = gen_class_subset(target_per_class, n_clusters=3, rs=43)\ny1 = np.ones((X1.shape[0],), dtype=int)\n\n# Classe 2 -&gt; 4 clusters\nX2 = gen_class_subset(target_per_class, n_clusters=4, rs=44)\ny2 = np.full((X2.shape[0],), 2, dtype=int)\n\n# Combine e embaralhe\nX = np.vstack([X0, X1, X2])\ny = np.concatenate([y0, y1, y2])\n\nperm = np.random.permutation(len(X))\nX = X[perm]\ny = y[perm]\n\nprint(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])\n\n# Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar\npca = PCA(n_components=2, random_state=0)\nX_2d = pca.fit_transform(X)\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c)\nplt.title(\"Dados (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> N_total = 1500 K = 3 n_features = 4 class_sep = 1.8 flip_y = 0.00  # Queremos tamanhos semelhantes por classe target_per_class = N_total // K  # 500  def gen_class_subset(target, n_clusters, rs):     # como vamos filtrar apenas a classe positiva, geramos o dobro     n_samples_tmp = target * 2     X_tmp, y_tmp = make_classification(         n_samples=n_samples_tmp,         n_features=n_features,         n_informative=n_features,         n_redundant=0,         n_classes=2,         n_clusters_per_class=n_clusters,         class_sep=class_sep,         flip_y=flip_y,         random_state=rs     )     X_pos = X_tmp[y_tmp == 1]     if len(X_pos) &lt; target:         # se por algum motivo veio menos, reamostrar com rs+1         X_more, y_more = make_classification(             n_samples=n_samples_tmp,             n_features=n_features,             n_informative=n_features,             n_redundant=0,             n_classes=2,             n_clusters_per_class=n_clusters,             class_sep=class_sep,             flip_y=flip_y,             random_state=rs+101         )         X_pos = np.vstack([X_pos, X_more[y_more == 1]])     return X_pos[:target]  # Classe 0 -&gt; 2 clusters X0 = gen_class_subset(target_per_class, n_clusters=2, rs=42) y0 = np.zeros((X0.shape[0],), dtype=int)  # Classe 1 -&gt; 3 clusters X1 = gen_class_subset(target_per_class, n_clusters=3, rs=43) y1 = np.ones((X1.shape[0],), dtype=int)  # Classe 2 -&gt; 4 clusters X2 = gen_class_subset(target_per_class, n_clusters=4, rs=44) y2 = np.full((X2.shape[0],), 2, dtype=int)  # Combine e embaralhe X = np.vstack([X0, X1, X2]) y = np.concatenate([y0, y1, y2])  perm = np.random.permutation(len(X)) X = X[perm] y = y[perm]  print(\"Shapes:\", X.shape, y.shape, \"| classes:\", [np.sum(y==i) for i in range(K)])  # Visualiza\u00e7\u00e3o r\u00e1pida em 2D via PCA s\u00f3 para inspecionar pca = PCA(n_components=2, random_state=0) X_2d = pca.fit_transform(X) plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(X_2d[y==lbl,0], X_2d[y==lbl,1], s=10, alpha=0.8, label=f\"Classe {lbl}\", c=c) plt.title(\"Dados (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show() <pre>Shapes: (1500, 4) (1500,) | classes: [np.int64(500), np.int64(500), np.int64(500)]\n</pre> In\u00a0[16]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[17]: Copied! <pre># Arquitetura\nn_in = n_features\nn_hidden = 32\nn_out = K\n\n# Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden))\nb1 = np.zeros((1, n_hidden))\n\nlimit2 = np.sqrt(6.0 / (n_hidden + n_out))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out))\nb2 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    # y: (m,1)  -&gt; (m,K)\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    z1 = Xb @ W1 + b1         # (m, hidden)\n    a1 = tanh(z1)             # (m, hidden)\n    z2 = a1 @ W2 + b2         # (m, K)\n    a2 = softmax(z2)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2)\n    return a2, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m\n    dz2 = (a2 - yb_onehot) / m          # (m,K)\n    dW2 = a1.T @ dz2                    # (hidden,K)\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    da1 = dz2 @ W2.T                    # (m,hidden)\n    dz1 = da1 * dtanh(a1)               # (m,hidden)\n    dW1 = Xb.T @ dz1                    # (n_in,hidden)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)\n\n    return dW1, db1_, dW2, db2\n\ndef update_params(dW1, db1_, dW2, db2_, lr):\n    global W1, b1, W2, b2\n    W1 -= lr * dW1\n    b1 -= lr * db1_\n    W2 -= lr * dW2\n    b2 -= lr * db2_\n</pre> # Arquitetura n_in = n_features n_hidden = 32 n_out = K  # Inicializa\u00e7\u00e3o Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden)) b1 = np.zeros((1, n_hidden))  limit2 = np.sqrt(6.0 / (n_hidden + n_out)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden, n_out)) b2 = np.zeros((1, n_out))  def onehot(y, K):     # y: (m,1)  -&gt; (m,K)     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     z1 = Xb @ W1 + b1         # (m, hidden)     a1 = tanh(z1)             # (m, hidden)     z2 = a1 @ W2 + b2         # (m, K)     a2 = softmax(z2)          # (m, K)     cache = (Xb, z1, a1, z2, a2)     return a2, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2 = cache     m = Xb.shape[0]      # Softmax + CE: grad da sa\u00edda \u00e9 (a2 - y) / m     dz2 = (a2 - yb_onehot) / m          # (m,K)     dW2 = a1.T @ dz2                    # (hidden,K)     db2 = np.sum(dz2, axis=0, keepdims=True)      da1 = dz2 @ W2.T                    # (m,hidden)     dz1 = da1 * dtanh(a1)               # (m,hidden)     dW1 = Xb.T @ dz1                    # (n_in,hidden)     db1_ = np.sum(dz1, axis=0, keepdims=True)      return dW1, db1_, dW2, db2  def update_params(dW1, db1_, dW2, db2_, lr):     global W1, b1, W2, b2     W1 -= lr * dW1     b1 -= lr * db1_     W2 -= lr * dW2     b2 -= lr * db2_  In\u00a0[18]: Copied! <pre>epochs = 350\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\n\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 350 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m)  Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9847 | acc_train=0.5717\nEpoch   25 | loss=0.7307 | acc_train=0.6967\nEpoch   50 | loss=0.5286 | acc_train=0.8008\nEpoch   75 | loss=0.4153 | acc_train=0.8483\n</pre> <pre>Epoch  100 | loss=0.3526 | acc_train=0.8850\nEpoch  125 | loss=0.3169 | acc_train=0.8983\nEpoch  150 | loss=0.2904 | acc_train=0.9025\nEpoch  175 | loss=0.2701 | acc_train=0.9025\n</pre> <pre>Epoch  200 | loss=0.2559 | acc_train=0.9117\nEpoch  225 | loss=0.2439 | acc_train=0.9167\nEpoch  250 | loss=0.2333 | acc_train=0.9150\nEpoch  275 | loss=0.2225 | acc_train=0.9192\n</pre> <pre>Epoch  300 | loss=0.2137 | acc_train=0.9208\nEpoch  325 | loss=0.2055 | acc_train=0.9283\nEpoch  350 | loss=0.1999 | acc_train=0.9267\n</pre> In\u00a0[19]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9200\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[93  7  0]\n [ 6 90  4]\n [ 6  1 93]]\n</pre> In\u00a0[20]: Copied! <pre># Projeta teste em 2D para visualizar predi\u00e7\u00f5es\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Projeta teste em 2D para visualizar predi\u00e7\u00f5es pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend() plt.grid(True, alpha=0.3) plt.show()  <p>Multi-Class Classification with Deeper MLP Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p> In\u00a0[21]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_train = y_train.reshape(-1, 1)\ny_test  = y_test.reshape(-1, 1)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.20, random_state=42, stratify=y )  scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  y_train = y_train.reshape(-1, 1) y_test  = y_test.reshape(-1, 1)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  <pre>(1200, 4) (1200, 1) (300, 4) (300, 1)\n</pre> In\u00a0[22]: Copied! <pre># Arquitetura (duas ocultas)\nK = 3                 # n\u00famero de classes\nn_in = X_train.shape[1]\nn_hidden1 = 64\nn_hidden2 = 32\nn_out = K\n\n# Xavier/Glorot p/ tanh/softmax\nlimit1 = np.sqrt(6.0 / (n_in + n_hidden1))\nW1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1))\nb1 = np.zeros((1, n_hidden1))\n\nlimit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2))\nW2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2))\nb2 = np.zeros((1, n_hidden2))\n\nlimit3 = np.sqrt(6.0 / (n_hidden2 + n_out))\nW3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out))\nb3 = np.zeros((1, n_out))\n\ndef onehot(y, K):\n    m = y.shape[0]\n    out = np.zeros((m, K), dtype=float)\n    out[np.arange(m), y.reshape(-1)] = 1.0\n    return out\n\ndef forward(Xb):\n    # camada 1\n    z1 = Xb @ W1 + b1         # (m, h1)\n    a1 = tanh(z1)             # (m, h1)\n    # camada 2\n    z2 = a1 @ W2 + b2         # (m, h2)\n    a2 = tanh(z2)             # (m, h2)\n    # sa\u00edda\n    z3 = a2 @ W3 + b3         # (m, K)\n    a3 = softmax(z3)          # (m, K)\n    cache = (Xb, z1, a1, z2, a2, z3, a3)\n    return a3, cache\n\ndef backward(cache, yb_onehot):\n    Xb, z1, a1, z2, a2, z3, a3 = cache\n    m = Xb.shape[0]\n\n    # Softmax + CE\n    dz3 = (a3 - yb_onehot) / m               # (m,K)\n    dW3 = a2.T @ dz3                         # (h2,K)\n    db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)\n\n    da2 = dz3 @ W3.T                         # (m,h2)\n    dz2 = da2 * dtanh(a2)                    # (m,h2)\n    dW2 = a1.T @ dz2                         # (h1,h2)\n    db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)\n\n    da1 = dz2 @ W2.T                         # (m,h1)\n    dz1 = da1 * dtanh(a1)                    # (m,h1)\n    dW1 = Xb.T @ dz1                         # (n_in,h1)\n    db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)\n\n    return dW1, db1_, dW2, db2_, dW3, db3\n\ndef update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):\n    global W1, b1, W2, b2, W3, b3\n    W1 -= lr * dW1; b1 -= lr * db1_\n    W2 -= lr * dW2; b2 -= lr * db2_\n    W3 -= lr * dW3; b3 -= lr * db3_\n</pre> # Arquitetura (duas ocultas) K = 3                 # n\u00famero de classes n_in = X_train.shape[1] n_hidden1 = 64 n_hidden2 = 32 n_out = K  # Xavier/Glorot p/ tanh/softmax limit1 = np.sqrt(6.0 / (n_in + n_hidden1)) W1 = np.random.uniform(-limit1, limit1, size=(n_in, n_hidden1)) b1 = np.zeros((1, n_hidden1))  limit2 = np.sqrt(6.0 / (n_hidden1 + n_hidden2)) W2 = np.random.uniform(-limit2, limit2, size=(n_hidden1, n_hidden2)) b2 = np.zeros((1, n_hidden2))  limit3 = np.sqrt(6.0 / (n_hidden2 + n_out)) W3 = np.random.uniform(-limit3, limit3, size=(n_hidden2, n_out)) b3 = np.zeros((1, n_out))  def onehot(y, K):     m = y.shape[0]     out = np.zeros((m, K), dtype=float)     out[np.arange(m), y.reshape(-1)] = 1.0     return out  def forward(Xb):     # camada 1     z1 = Xb @ W1 + b1         # (m, h1)     a1 = tanh(z1)             # (m, h1)     # camada 2     z2 = a1 @ W2 + b2         # (m, h2)     a2 = tanh(z2)             # (m, h2)     # sa\u00edda     z3 = a2 @ W3 + b3         # (m, K)     a3 = softmax(z3)          # (m, K)     cache = (Xb, z1, a1, z2, a2, z3, a3)     return a3, cache  def backward(cache, yb_onehot):     Xb, z1, a1, z2, a2, z3, a3 = cache     m = Xb.shape[0]      # Softmax + CE     dz3 = (a3 - yb_onehot) / m               # (m,K)     dW3 = a2.T @ dz3                         # (h2,K)     db3 = np.sum(dz3, axis=0, keepdims=True) # (1,K)      da2 = dz3 @ W3.T                         # (m,h2)     dz2 = da2 * dtanh(a2)                    # (m,h2)     dW2 = a1.T @ dz2                         # (h1,h2)     db2_ = np.sum(dz2, axis=0, keepdims=True)# (1,h2)      da1 = dz2 @ W2.T                         # (m,h1)     dz1 = da1 * dtanh(a1)                    # (m,h1)     dW1 = Xb.T @ dz1                         # (n_in,h1)     db1_ = np.sum(dz1, axis=0, keepdims=True)# (1,h1)      return dW1, db1_, dW2, db2_, dW3, db3  def update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr):     global W1, b1, W2, b2, W3, b3     W1 -= lr * dW1; b1 -= lr * db1_     W2 -= lr * dW2; b2 -= lr * db2_     W3 -= lr * dW3; b3 -= lr * db3_  In\u00a0[23]: Copied! <pre>epochs = 400\nbatch_size = 64\nlr = 0.05\nloss_hist = []\n\nm = X_train.shape[0]\nidxs = np.arange(m)\nYtr_1h = onehot(y_train, K)\n\nfor ep in range(1, epochs+1):\n    np.random.shuffle(idxs)\n    Xtr = X_train[idxs]\n    Ytr = Ytr_1h[idxs]\n\n    ep_loss = 0.0\n    n_batches = 0\n\n    for start in range(0, m, batch_size):\n        end = start + batch_size\n        Xb = Xtr[start:end]\n        Yb = Ytr[start:end]\n\n        yhat, cache = forward(Xb)\n        loss = cross_entropy_onehot(Yb, yhat)\n        ep_loss += loss\n        n_batches += 1\n\n        dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)\n        update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)\n\n    loss_hist.append(ep_loss / n_batches)\n\n    if ep % 25 == 0 or ep == 1:\n        yhat_tr, _ = forward(X_train)\n        acc_tr = accuracy_multiclass(y_train, yhat_tr)\n        print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")\n\nplt.figure()\nplt.plot(range(1, epochs+1), loss_hist)\nplt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> epochs = 400 batch_size = 64 lr = 0.05 loss_hist = []  m = X_train.shape[0] idxs = np.arange(m) Ytr_1h = onehot(y_train, K)  for ep in range(1, epochs+1):     np.random.shuffle(idxs)     Xtr = X_train[idxs]     Ytr = Ytr_1h[idxs]      ep_loss = 0.0     n_batches = 0      for start in range(0, m, batch_size):         end = start + batch_size         Xb = Xtr[start:end]         Yb = Ytr[start:end]          yhat, cache = forward(Xb)         loss = cross_entropy_onehot(Yb, yhat)         ep_loss += loss         n_batches += 1          dW1, db1_, dW2, db2_, dW3, db3_ = backward(cache, Yb)         update_params(dW1, db1_, dW2, db2_, dW3, db3_, lr)      loss_hist.append(ep_loss / n_batches)      if ep % 25 == 0 or ep == 1:         yhat_tr, _ = forward(X_train)         acc_tr = accuracy_multiclass(y_train, yhat_tr)         print(f\"Epoch {ep:4d} | loss={loss_hist[-1]:.4f} | acc_train={acc_tr:.4f}\")  plt.figure() plt.plot(range(1, epochs+1), loss_hist) plt.title(\"Loss (treino) por \u00e9poca \u2014 CCE (2 ocultas)\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Cross-Entropy\") plt.grid(True, alpha=0.3) plt.show()  <pre>Epoch    1 | loss=0.9520 | acc_train=0.5642\nEpoch   25 | loss=0.5414 | acc_train=0.7717\n</pre> <pre>Epoch   50 | loss=0.3694 | acc_train=0.8708\nEpoch   75 | loss=0.3003 | acc_train=0.8975\n</pre> <pre>Epoch  100 | loss=0.2528 | acc_train=0.9058\nEpoch  125 | loss=0.2266 | acc_train=0.9233\n</pre> <pre>Epoch  150 | loss=0.2025 | acc_train=0.9308\nEpoch  175 | loss=0.1887 | acc_train=0.9367\n</pre> <pre>Epoch  200 | loss=0.1764 | acc_train=0.9250\nEpoch  225 | loss=0.1619 | acc_train=0.9408\n</pre> <pre>Epoch  250 | loss=0.1524 | acc_train=0.9467\nEpoch  275 | loss=0.1475 | acc_train=0.9308\n</pre> <pre>Epoch  300 | loss=0.1390 | acc_train=0.9467\nEpoch  325 | loss=0.1300 | acc_train=0.9408\n</pre> <pre>Epoch  350 | loss=0.1282 | acc_train=0.9467\nEpoch  375 | loss=0.1261 | acc_train=0.9483\n</pre> <pre>Epoch  400 | loss=0.1212 | acc_train=0.9533\n</pre> In\u00a0[24]: Copied! <pre>yprob_te, _ = forward(X_test)\nacc_te = accuracy_multiclass(y_test, yprob_te)\ncm = confusion_matrix_k(y_test, yprob_te, K)\n\nprint(f\"Acur\u00e1cia (teste): {acc_te:.4f}\")\nprint(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\")\nprint(cm)\n</pre> yprob_te, _ = forward(X_test) acc_te = accuracy_multiclass(y_test, yprob_te) cm = confusion_matrix_k(y_test, yprob_te, K)  print(f\"Acur\u00e1cia (teste): {acc_te:.4f}\") print(\"Matriz de confus\u00e3o (linhas = verdade, colunas = predito):\") print(cm)  <pre>Acur\u00e1cia (teste): 0.9267\nMatriz de confus\u00e3o (linhas = verdade, colunas = predito):\n[[96  4  0]\n [10 87  3]\n [ 5  0 95]]\n</pre> In\u00a0[25]: Copied! <pre># Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste\npca_viz = PCA(n_components=2, random_state=0)\nX_all_std = np.vstack([X_train, X_test])\npca_viz.fit(X_all_std)\n\nXte_2d = pca_viz.transform(X_test)\ny_pred = np.argmax(yprob_te, axis=1)\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c)\nplt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n\nplt.figure()\nfor c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):\n    plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c)\nplt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\")\nplt.legend(); plt.grid(True, alpha=0.3); plt.show()\n</pre> # Projeta treino+teste para PCA consistente e visualiza predi\u00e7\u00f5es no teste pca_viz = PCA(n_components=2, random_state=0) X_all_std = np.vstack([X_train, X_test]) pca_viz.fit(X_all_std)  Xte_2d = pca_viz.transform(X_test) y_pred = np.argmax(yprob_te, axis=1)  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_pred==lbl,0], Xte_2d[y_pred==lbl,1], s=12, alpha=0.85, label=f\"Pred {lbl}\", c=c) plt.title(\"Teste \u2014 Predi\u00e7\u00f5es (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  plt.figure() for c, lbl in zip([\"tab:blue\",\"tab:orange\",\"tab:green\"], [0,1,2]):     plt.scatter(Xte_2d[y_test.reshape(-1)==lbl,0], Xte_2d[y_test.reshape(-1)==lbl,1], s=12, alpha=0.85, label=f\"True {lbl}\", c=c) plt.title(\"Teste \u2014 R\u00f3tulos verdadeiros (PCA 2D)\") plt.legend(); plt.grid(True, alpha=0.3); plt.show()  In\u00a0[26]: Copied! <pre># === Avalia\u00e7\u00e3o treino vs teste ===\nyprob_tr, _ = forward(X_train)\nyprob_te, _ = forward(X_test)\n\nacc_tr = accuracy_multiclass(y_train, yprob_tr)\nacc_te = accuracy_multiclass(y_test, yprob_te)\n\nprint(f\"Acc TREINO: {acc_tr:.4f}\")\nprint(f\"Acc TESTE : {acc_te:.4f}\")\n\nloss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr)\nloss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te)\nprint(f\"Loss TREINO: {loss_tr:.4f}\")\nprint(f\"Loss TESTE : {loss_te:.4f}\")\n</pre> # === Avalia\u00e7\u00e3o treino vs teste === yprob_tr, _ = forward(X_train) yprob_te, _ = forward(X_test)  acc_tr = accuracy_multiclass(y_train, yprob_tr) acc_te = accuracy_multiclass(y_test, yprob_te)  print(f\"Acc TREINO: {acc_tr:.4f}\") print(f\"Acc TESTE : {acc_te:.4f}\")  loss_tr = cross_entropy_onehot(onehot(y_train, K), yprob_tr) loss_te = cross_entropy_onehot(onehot(y_test,  K), yprob_te) print(f\"Loss TREINO: {loss_tr:.4f}\") print(f\"Loss TESTE : {loss_te:.4f}\") <pre>Acc TREINO: 0.9533\nAcc TESTE : 0.9267\nLoss TREINO: 0.1145\nLoss TESTE : 0.1905\n</pre> <p><code>Accuracy Train</code> foi maior que <code>Accuracy Test</code>, indicando que o modelo est\u00e1 sofrendo de overfitting.</p>"},{"location":"Exercicios/EX3/MLP/#3-mlp","title":"3. MLP\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#activity-understanding-multi-layer-perceptrons-mlps","title":"Activity: Understanding Multi-Layer Perceptrons (MLPs)\u00b6","text":"<p>This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs).</p>"},{"location":"Exercicios/EX3/MLP/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#exercise-4","title":"Exercise 4\u00b6","text":""},{"location":"Exercicios/EX3/MLP/#verficacao-de-overfitting-do-ex4","title":"Verfica\u00e7\u00e3o de Overfitting do EX4\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/","title":"Multi-Layer Perceptron (MLP) Classification","text":"In\u00a0[1]: Copied! <pre># Basic imports &amp; reproducibility\nimport os\nimport random\nfrom pathlib import Path\nimport warnings\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport copy\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n# stats / util\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# preprocessing &amp; modeling\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score, classification_report\n)\n\n# notebook display\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\nwarnings.filterwarnings('ignore')\n\n# plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\n# seeds for reproducibility (use consistently across notebook)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n</pre> # Basic imports &amp; reproducibility import os import random from pathlib import Path import warnings  import math import numpy as np import pandas as pd import itertools import copy  # plotting import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import MaxNLocator  # stats / util from scipy import stats from tqdm.auto import tqdm  # preprocessing &amp; modeling from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.utils.class_weight import compute_class_weight  # metrics from sklearn.metrics import (     accuracy_score, precision_score, recall_score, f1_score,     confusion_matrix, roc_auc_score, roc_curve,     precision_recall_curve, average_precision_score, classification_report )  # notebook display %matplotlib inline pd.set_option('display.max_columns', None) pd.set_option('display.width', 200) warnings.filterwarnings('ignore')  # plotting style plt.style.use('seaborn-v0_8-whitegrid') sns.set_context('notebook')  # seeds for reproducibility (use consistently across notebook) SEED = 42 random.seed(SEED) np.random.seed(SEED) os.environ['PYTHONHASHSEED'] = str(SEED) <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 20\n     18 # stats / util\n     19 from scipy import stats\n---&gt; 20 from tqdm.auto import tqdm\n     22 # preprocessing &amp; modeling\n     23 from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nModuleNotFoundError: No module named 'tqdm'</pre> In\u00a0[2]: Copied! <pre># Load dataset\nDATA_DIR = Path(\"data\")\ntrain = pd.read_csv(DATA_DIR / \"train.csv\")\ntest = pd.read_csv(DATA_DIR / \"test.csv\")\n\nprint(\"Dataset Shape:\")\nprint(f\"  Training: {train.shape}\")\nprint(f\"  Test: {test.shape}\")\n\nprint(\"\\nFirst 5 rows:\")\ndisplay(train.head())\n\nprint(\"\\nData Types:\")\nprint(train.dtypes)\n</pre> # Load dataset DATA_DIR = Path(\"data\") train = pd.read_csv(DATA_DIR / \"train.csv\") test = pd.read_csv(DATA_DIR / \"test.csv\")  print(\"Dataset Shape:\") print(f\"  Training: {train.shape}\") print(f\"  Test: {test.shape}\")  print(\"\\nFirst 5 rows:\") display(train.head())  print(\"\\nData Types:\") print(train.dtypes) <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 3\n      1 # Load dataset\n      2 DATA_DIR = Path(\"data\")\n----&gt; 3 train = pd.read_csv(DATA_DIR / \"train.csv\")\n      4 test = pd.read_csv(DATA_DIR / \"test.csv\")\n      6 print(\"Dataset Shape:\")\n\nFile ~\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~\\Documents\\Semestre_9\\Redes Neurais\\Neural Network\\env\\Lib\\site-packages\\pandas\\io\\common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'data\\\\train.csv'</pre> In\u00a0[3]: Copied! <pre># Identify feature types\ntarget_col = 'y'\nid_col = 'id'\n\nnumerical_features = train.select_dtypes(include=['number']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n</pre> # Identify feature types target_col = 'y' id_col = 'id'  numerical_features = train.select_dtypes(include=['number']).columns.tolist() numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]  categorical_features = train.select_dtypes(include=['object']).columns.tolist()  print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\") print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 5\n      2 target_col = 'y'\n      3 id_col = 'id'\n----&gt; 5 numerical_features = train.select_dtypes(include=['number']).columns.tolist()\n      6 numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n      8 categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nNameError: name 'train' is not defined</pre> In\u00a0[4]: Copied! <pre># Summary statistics\ndisplay(train[numerical_features].describe())\n\nprint(\"\\nTarget Variable Distribution:\")\nprint(train[target_col].value_counts())\nprint(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\")\n</pre> # Summary statistics display(train[numerical_features].describe())  print(\"\\nTarget Variable Distribution:\") print(train[target_col].value_counts()) print(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 # Summary statistics\n----&gt; 2 display(train[numerical_features].describe())\n      4 print(\"\\nTarget Variable Distribution:\")\n      5 print(train[target_col].value_counts())\n\nNameError: name 'train' is not defined</pre> In\u00a0[5]: Copied! <pre># Check for missing values\nprint(\"Missing Values:\")\nprint(train.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n</pre> # Check for missing values print(\"Missing Values:\") print(train.isnull().sum())  # Check for duplicates print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\") <pre>Missing Values:\n</pre> <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 3\n      1 # Check for missing values\n      2 print(\"Missing Values:\")\n----&gt; 3 print(train.isnull().sum())\n      5 # Check for duplicates\n      6 print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n\nNameError: name 'train' is not defined</pre> In\u00a0[6]: Copied! <pre># Check for outliers (beyond 3 standard deviations)\noutlier_summary = {}\nfor col in numerical_features:\n    z = np.abs(stats.zscore(train[col]))\n    outlier_summary[col] = (z &gt; 3).mean() * 100\n\noutlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers'])\ndisplay(outlier_df.sort_values('% Outliers', ascending=False))\n</pre> # Check for outliers (beyond 3 standard deviations) outlier_summary = {} for col in numerical_features:     z = np.abs(stats.zscore(train[col]))     outlier_summary[col] = (z &gt; 3).mean() * 100  outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers']) display(outlier_df.sort_values('% Outliers', ascending=False)) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 3\n      1 # Check for outliers (beyond 3 standard deviations)\n      2 outlier_summary = {}\n----&gt; 3 for col in numerical_features:\n      4     z = np.abs(stats.zscore(train[col]))\n      5     outlier_summary[col] = (z &gt; 3).mean() * 100\n\nNameError: name 'numerical_features' is not defined</pre> In\u00a0[7]: Copied! <pre># Feature skewness\nskewness = train[numerical_features].skew().sort_values(ascending=False)\nprint(\"Feature Skewness:\")\ndisplay(skewness)\n</pre> # Feature skewness skewness = train[numerical_features].skew().sort_values(ascending=False) print(\"Feature Skewness:\") display(skewness) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 2\n      1 # Feature skewness\n----&gt; 2 skewness = train[numerical_features].skew().sort_values(ascending=False)\n      3 print(\"Feature Skewness:\")\n      4 display(skewness)\n\nNameError: name 'train' is not defined</pre> In\u00a0[8]: Copied! <pre>fig, axes = plt.subplots(3, 3, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_features):\n    data = train[col].dropna()\n    # histogram with density overlay (optional)\n    axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)\n    axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)\n    axes[i].grid(alpha=0.3)\n    if data.max() &gt; 5000:\n        axes[i].set_xscale(\"log\")\n    axes[i].set_xlabel(\"\")\n    axes[i].set_ylabel(\"\")\n\n# hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(3, 3, figsize=(15, 12)) axes = axes.flatten()  for i, col in enumerate(numerical_features):     data = train[col].dropna()     # histogram with density overlay (optional)     axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)     axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)     axes[i].grid(alpha=0.3)     if data.max() &gt; 5000:         axes[i].set_xscale(\"log\")     axes[i].set_xlabel(\"\")     axes[i].set_ylabel(\"\")  # hide any unused subplots for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 4\n      1 fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n      2 axes = axes.flatten()\n----&gt; 4 for i, col in enumerate(numerical_features):\n      5     data = train[col].dropna()\n      6     # histogram with density overlay (optional)\n\nNameError: name 'numerical_features' is not defined</pre> In\u00a0[9]: Copied! <pre># Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation = train[numerical_features + [target_col]].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\nplt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost correlated features with target:\")\nprint(correlation[target_col].sort_values(ascending=False))\n</pre> # Correlation matrix plt.figure(figsize=(10, 8)) correlation = train[numerical_features + [target_col]].corr() sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True) plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()  print(\"\\nMost correlated features with target:\") print(correlation[target_col].sort_values(ascending=False)) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 # Correlation matrix\n      2 plt.figure(figsize=(10, 8))\n----&gt; 3 correlation = train[numerical_features + [target_col]].corr()\n      4 sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\n      5 plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n\nNameError: name 'train' is not defined</pre> <pre>&lt;Figure size 1000x800 with 0 Axes&gt;</pre> In\u00a0[10]: Copied! <pre>detailed = []\nfor c in categorical_features:\n    vc = train[c].value_counts(dropna=False)\n    top1 = f\"{vc.index[0]}\"\n    detailed.append({\n        'feature': c,\n        'n_unique': train[c].nunique(dropna=False),\n        'most_freq': top1,\n        'total_rows': len(train)\n    })\ncat_summary_full = pd.DataFrame(detailed).set_index('feature')\ndisplay(cat_summary_full)\n</pre> detailed = [] for c in categorical_features:     vc = train[c].value_counts(dropna=False)     top1 = f\"{vc.index[0]}\"     detailed.append({         'feature': c,         'n_unique': train[c].nunique(dropna=False),         'most_freq': top1,         'total_rows': len(train)     }) cat_summary_full = pd.DataFrame(detailed).set_index('feature') display(cat_summary_full) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 detailed = []\n----&gt; 2 for c in categorical_features:\n      3     vc = train[c].value_counts(dropna=False)\n      4     top1 = f\"{vc.index[0]}\"\n\nNameError: name 'categorical_features' is not defined</pre> In\u00a0[11]: Copied! <pre>cat_cols = categorical_features\nn_features = len(cat_cols)\n\n# Choose subplot grid layout (3 columns works well)\ncols = 3\nrows = math.ceil(n_features / cols)\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4))\naxes = axes.flatten()\n\nfor i, c in enumerate(cat_cols):\n    ax = axes[i]\n    vals = train[c].value_counts(dropna=False)\n    categories = vals.index.astype(str).tolist()\n    counts = vals.values\n\n    # Compute target rate per category (align with counts)\n    rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values\n\n    # Plot count bars\n    ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")\n\n    ax.set_xticks(range(len(categories)))\n    ax.set_xticklabels(categories, rotation=90, fontsize=7)\n    ax.set_ylabel(\"count\")\n    ax.set_title(f'{c} (n={len(categories)})', fontsize=10)\n    ax.grid(alpha=0.2)\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n# Hide unused subplots if any\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> cat_cols = categorical_features n_features = len(cat_cols)  # Choose subplot grid layout (3 columns works well) cols = 3 rows = math.ceil(n_features / cols)  fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4)) axes = axes.flatten()  for i, c in enumerate(cat_cols):     ax = axes[i]     vals = train[c].value_counts(dropna=False)     categories = vals.index.astype(str).tolist()     counts = vals.values      # Compute target rate per category (align with counts)     rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values      # Plot count bars     ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")      ax.set_xticks(range(len(categories)))     ax.set_xticklabels(categories, rotation=90, fontsize=7)     ax.set_ylabel(\"count\")     ax.set_title(f'{c} (n={len(categories)})', fontsize=10)     ax.grid(alpha=0.2)     ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # Hide unused subplots if any for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 cat_cols = categorical_features\n      2 n_features = len(cat_cols)\n      4 # Choose subplot grid layout (3 columns works well)\n\nNameError: name 'categorical_features' is not defined</pre> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(5,4))\nsns.barplot(x=train[target_col].value_counts().index, \n            y=train[target_col].value_counts().values, palette='pastel')\nplt.title('Target Variable Distribution')\nplt.xlabel('Subscribed (y)')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No (0)', 'Yes (1)'])\nplt.grid(alpha=0.2)\nplt.show()\n</pre> plt.figure(figsize=(5,4)) sns.barplot(x=train[target_col].value_counts().index,              y=train[target_col].value_counts().values, palette='pastel') plt.title('Target Variable Distribution') plt.xlabel('Subscribed (y)') plt.ylabel('Count') plt.xticks([0, 1], ['No (0)', 'Yes (1)']) plt.grid(alpha=0.2) plt.show() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 2\n      1 plt.figure(figsize=(5,4))\n----&gt; 2 sns.barplot(x=train[target_col].value_counts().index, \n      3             y=train[target_col].value_counts().values, palette='pastel')\n      4 plt.title('Target Variable Distribution')\n      5 plt.xlabel('Subscribed (y)')\n\nNameError: name 'train' is not defined</pre> <pre>&lt;Figure size 500x400 with 0 Axes&gt;</pre> In\u00a0[13]: Copied! <pre># Cap outliers (1st/99th percentile)\ntrain_work = train.copy()\n\nfor col in numerical_features:\n    lower, upper = np.percentile(train_work[col], [1, 99])\n    train_work[col] = np.clip(train_work[col], lower, upper)\n</pre> # Cap outliers (1st/99th percentile) train_work = train.copy()  for col in numerical_features:     lower, upper = np.percentile(train_work[col], [1, 99])     train_work[col] = np.clip(train_work[col], lower, upper) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 # Cap outliers (1st/99th percentile)\n----&gt; 2 train_work = train.copy()\n      4 for col in numerical_features:\n      5     lower, upper = np.percentile(train_work[col], [1, 99])\n\nNameError: name 'train' is not defined</pre> In\u00a0[14]: Copied! <pre># Handle skewness with log1p\n# Replace -1 with 0 for pdays (no previous contact)\nif \"pdays\" in train_work.columns:\n    train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)\n\n# Automatically detect skewed columns (|skew| \u2265 1)\nskewness = train_work[numerical_features].skew()\nskewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist()\ntrain_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))\n\nprint(f\"Highly skewed features transformed: {skewed_cols}\")\n</pre> # Handle skewness with log1p # Replace -1 with 0 for pdays (no previous contact) if \"pdays\" in train_work.columns:     train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)  # Automatically detect skewed columns (|skew| \u2265 1) skewness = train_work[numerical_features].skew() skewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist() train_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))  print(f\"Highly skewed features transformed: {skewed_cols}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 3\n      1 # Handle skewness with log1p\n      2 # Replace -1 with 0 for pdays (no previous contact)\n----&gt; 3 if \"pdays\" in train_work.columns:\n      4     train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)\n      6 # Automatically detect skewed columns (|skew| \u2265 1)\n\nNameError: name 'train_work' is not defined</pre> In\u00a0[15]: Copied! <pre># Define features and target\nX_full = train_work.drop(columns=[target_col, id_col])\ny_full = train_work[target_col].values\n</pre> # Define features and target X_full = train_work.drop(columns=[target_col, id_col]) y_full = train_work[target_col].values <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 2\n      1 # Define features and target\n----&gt; 2 X_full = train_work.drop(columns=[target_col, id_col])\n      3 y_full = train_work[target_col].values\n\nNameError: name 'train_work' is not defined</pre> In\u00a0[16]: Copied! <pre># Split into train / val / test\n# First split train+val vs test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED\n)\n# Then split train vs val (from the remaining 85%)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED\n)  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall\n\nprint(\"Final splits:\")\nprint(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\nprint(\" Class balance (train):\", np.bincount(y_train))\nprint(\" Class balance (val):  \", np.bincount(y_val))\nprint(\" Class balance (test): \", np.bincount(y_test))\n</pre> # Split into train / val / test # First split train+val vs test X_temp, X_test, y_temp, y_test = train_test_split(     X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED ) # Then split train vs val (from the remaining 85%) X_train, X_val, y_train, y_val = train_test_split(     X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED )  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall  print(\"Final splits:\") print(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\") print(\" Class balance (train):\", np.bincount(y_train)) print(\" Class balance (val):  \", np.bincount(y_val)) print(\" Class balance (test): \", np.bincount(y_test)) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 3\n      1 # Split into train / val / test\n      2 # First split train+val vs test\n----&gt; 3 X_temp, X_test, y_temp, y_test = train_test_split(\n      4     X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED\n      5 )\n      6 # Then split train vs val (from the remaining 85%)\n      7 X_train, X_val, y_train, y_val = train_test_split(\n      8     X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED\n      9 )  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall\n\nNameError: name 'train_test_split' is not defined</pre> In\u00a0[17]: Copied! <pre># Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numerical_features),\n        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\n# Fit only on training data\npreprocessor.fit(X_train)\n\n# Transform splits\nX_train_proc = preprocessor.transform(X_train)\nX_val_proc   = preprocessor.transform(X_val)\nX_test_proc  = preprocessor.transform(X_test)\n\nprint(\"\\nProcessed feature dimensions:\")\nprint(f\" Train: {X_train_proc.shape}\")\nprint(f\" Val:   {X_val_proc.shape}\")\nprint(f\" Test:  {X_test_proc.shape}\")\n</pre> # Preprocessing pipeline preprocessor = ColumnTransformer(     transformers=[         (\"num\", StandardScaler(), numerical_features),         (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),     ],     remainder=\"drop\", )  # Fit only on training data preprocessor.fit(X_train)  # Transform splits X_train_proc = preprocessor.transform(X_train) X_val_proc   = preprocessor.transform(X_val) X_test_proc  = preprocessor.transform(X_test)  print(\"\\nProcessed feature dimensions:\") print(f\" Train: {X_train_proc.shape}\") print(f\" Val:   {X_val_proc.shape}\") print(f\" Test:  {X_test_proc.shape}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 2\n      1 # Preprocessing pipeline\n----&gt; 2 preprocessor = ColumnTransformer(\n      3     transformers=[\n      4         (\"num\", StandardScaler(), numerical_features),\n      5         (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),\n      6     ],\n      7     remainder=\"drop\",\n      8 )\n     10 # Fit only on training data\n     11 preprocessor.fit(X_train)\n\nNameError: name 'ColumnTransformer' is not defined</pre> In\u00a0[18]: Copied! <pre># Class weights (for imbalance)\ncw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weight = {0: cw[0], 1: cw[1]}\nprint(\"\\nClass weights:\", class_weight)\n</pre> # Class weights (for imbalance) cw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train) class_weight = {0: cw[0], 1: cw[1]} print(\"\\nClass weights:\", class_weight) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 2\n      1 # Class weights (for imbalance)\n----&gt; 2 cw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n      3 class_weight = {0: cw[0], 1: cw[1]}\n      4 print(\"\\nClass weights:\", class_weight)\n\nNameError: name 'compute_class_weight' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[19]: Copied! <pre>class MLP:\n    \"\"\"\n    Multi-Layer Perceptron (NumPy) for binary classification.\n    Architecture: Input \u2192 Hidden Layer(s) \u2192 Output\n    Activation: ReLU (hidden), Sigmoid (output)\n    Loss: Binary Cross-Entropy with optional L2 regularization\n    Optimizer: SGD (parameter updates handled externally)\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):\n        np.random.seed(random_state)\n        self.l2_lambda = l2_lambda\n        \n        # Define layer sizes: input, hidden(s), output\n        layer_sizes = [input_dim] + hidden_sizes + [1]\n        self.num_layers = len(layer_sizes) - 1\n        \n        # Initialize parameters with He initialization (good for ReLU)\n        self.weights = []\n        self.biases = []\n        for i in range(self.num_layers):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            std = np.sqrt(2.0 / n_in)\n            self.weights.append(np.random.randn(n_in, n_out) * std)\n            self.biases.append(np.zeros((1, n_out)))\n\n    # Activation functions\n    def relu(self, x):\n        \"\"\"ReLU activation\"\"\"\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (x &gt; 0).astype(float)\n\n    def sigmoid(self, x):\n        \"\"\"Numerically stable sigmoid\"\"\"\n        out = np.empty_like(x)\n        pos = x &gt;= 0\n        out[pos] = 1 / (1 + np.exp(-x[pos]))\n        neg = ~pos\n        exp_x = np.exp(x[neg])\n        out[neg] = exp_x / (1 + exp_x)\n        return out\n\n    # Forward propagation\n    def forward(self, X):\n        \"\"\"\n        Forward pass through all layers.\n        Returns:\n            y_pred: predicted probabilities\n            cache: activations for backpropagation\n        \"\"\"\n        cache = {'A0': X}\n        A = X\n        for i in range(self.num_layers - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            cache[f'Z{i+1}'] = Z\n            cache[f'A{i+1}'] = A\n        Z_out = A @ self.weights[-1] + self.biases[-1]\n        A_out = self.sigmoid(Z_out)\n        cache[f'Z{self.num_layers}'] = Z_out\n        cache[f'A{self.num_layers}'] = A_out\n        return A_out, cache\n\n    # Loss computation\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Binary Cross-Entropy + optional L2 penalty.\n        \"\"\"\n        m = y_true.shape[0]\n        y_true = y_true.reshape(-1, 1)\n        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n        bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))\n        return bce + l2\n\n    # Backpropagation\n    def backward(self, cache, y_true):\n        \"\"\"\n        Compute gradients for all parameters using backpropagation.\n        \"\"\"\n        y_true = y_true.reshape(-1, 1)\n        m = y_true.shape[0]\n        grads_w, grads_b = [], []\n\n        # Gradient for output layer\n        y_pred = cache[f'A{self.num_layers}']\n        dZ = y_pred - y_true\n\n        for i in range(self.num_layers - 1, -1, -1):\n            A_prev = cache[f'A{i}']\n            dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]\n            db = np.mean(dZ, axis=0, keepdims=True)\n            grads_w.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = dZ @ self.weights[i].T\n                dZ = dA * self.relu_derivative(cache[f'Z{i}'])\n\n        return grads_w, grads_b\n\n    # Parameter update\n    def update_parameters(self, grads_w, grads_b, learning_rate):\n        \"\"\"Apply gradient descent step.\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= learning_rate * grads_w[i]\n            self.biases[i]  -= learning_rate * grads_b[i]\n\n        # Prediction helpers\n    def predict_proba(self, X):\n        \"\"\"\n        Compute output probabilities for given inputs.\n        \"\"\"\n        y_pred, _ = self.forward(X)\n        return y_pred.reshape(-1)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"\n        Predict binary class labels (0/1).\n        \"\"\"\n        return (self.predict_proba(X) &gt;= threshold).astype(int)\n\n\nprint(\"MLP class implemented successfully.\")\n</pre> class MLP:     \"\"\"     Multi-Layer Perceptron (NumPy) for binary classification.     Architecture: Input \u2192 Hidden Layer(s) \u2192 Output     Activation: ReLU (hidden), Sigmoid (output)     Loss: Binary Cross-Entropy with optional L2 regularization     Optimizer: SGD (parameter updates handled externally)     \"\"\"          def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):         np.random.seed(random_state)         self.l2_lambda = l2_lambda                  # Define layer sizes: input, hidden(s), output         layer_sizes = [input_dim] + hidden_sizes + [1]         self.num_layers = len(layer_sizes) - 1                  # Initialize parameters with He initialization (good for ReLU)         self.weights = []         self.biases = []         for i in range(self.num_layers):             n_in, n_out = layer_sizes[i], layer_sizes[i+1]             std = np.sqrt(2.0 / n_in)             self.weights.append(np.random.randn(n_in, n_out) * std)             self.biases.append(np.zeros((1, n_out)))      # Activation functions     def relu(self, x):         \"\"\"ReLU activation\"\"\"         return np.maximum(0, x)      def relu_derivative(self, x):         \"\"\"Derivative of ReLU\"\"\"         return (x &gt; 0).astype(float)      def sigmoid(self, x):         \"\"\"Numerically stable sigmoid\"\"\"         out = np.empty_like(x)         pos = x &gt;= 0         out[pos] = 1 / (1 + np.exp(-x[pos]))         neg = ~pos         exp_x = np.exp(x[neg])         out[neg] = exp_x / (1 + exp_x)         return out      # Forward propagation     def forward(self, X):         \"\"\"         Forward pass through all layers.         Returns:             y_pred: predicted probabilities             cache: activations for backpropagation         \"\"\"         cache = {'A0': X}         A = X         for i in range(self.num_layers - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             cache[f'Z{i+1}'] = Z             cache[f'A{i+1}'] = A         Z_out = A @ self.weights[-1] + self.biases[-1]         A_out = self.sigmoid(Z_out)         cache[f'Z{self.num_layers}'] = Z_out         cache[f'A{self.num_layers}'] = A_out         return A_out, cache      # Loss computation     def compute_loss(self, y_true, y_pred):         \"\"\"         Binary Cross-Entropy + optional L2 penalty.         \"\"\"         m = y_true.shape[0]         y_true = y_true.reshape(-1, 1)         y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)         bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))         l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))         return bce + l2      # Backpropagation     def backward(self, cache, y_true):         \"\"\"         Compute gradients for all parameters using backpropagation.         \"\"\"         y_true = y_true.reshape(-1, 1)         m = y_true.shape[0]         grads_w, grads_b = [], []          # Gradient for output layer         y_pred = cache[f'A{self.num_layers}']         dZ = y_pred - y_true          for i in range(self.num_layers - 1, -1, -1):             A_prev = cache[f'A{i}']             dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]             db = np.mean(dZ, axis=0, keepdims=True)             grads_w.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = dZ @ self.weights[i].T                 dZ = dA * self.relu_derivative(cache[f'Z{i}'])          return grads_w, grads_b      # Parameter update     def update_parameters(self, grads_w, grads_b, learning_rate):         \"\"\"Apply gradient descent step.\"\"\"         for i in range(self.num_layers):             self.weights[i] -= learning_rate * grads_w[i]             self.biases[i]  -= learning_rate * grads_b[i]          # Prediction helpers     def predict_proba(self, X):         \"\"\"         Compute output probabilities for given inputs.         \"\"\"         y_pred, _ = self.forward(X)         return y_pred.reshape(-1)      def predict(self, X, threshold=0.5):         \"\"\"         Predict binary class labels (0/1).         \"\"\"         return (self.predict_proba(X) &gt;= threshold).astype(int)   print(\"MLP class implemented successfully.\") <pre>MLP class implemented successfully.\n</pre> In\u00a0[20]: Copied! <pre>def train_mlp(model, X_train, y_train, X_val, y_val,\n              epochs=50, batch_size=512, learning_rate=1e-2,\n              early_stopping=5, verbose=True):\n    \"\"\"\n    Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.\n    \"\"\"\n    # --- Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed\n    import pandas as pd\n    if isinstance(X_train, pd.DataFrame):\n        X_train = X_train.values\n    if isinstance(X_val, pd.DataFrame):\n        X_val = X_val.values\n\n    if hasattr(X_train, \"toarray\"):\n        X_train = X_train.toarray()\n    if hasattr(X_val, \"toarray\"):\n        X_val = X_val.toarray()\n\n    y_train = np.asarray(y_train).reshape(-1, 1)\n    y_val   = np.asarray(y_val).reshape(-1, 1)\n\n    n = X_train.shape[0]\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}\n    best_val_loss = np.inf\n    patience = 0\n    best_weights = None\n\n    for epoch in range(1, epochs + 1):\n        # Shuffle training data (use permutation index and apply to numpy arrays)\n        idx = np.random.permutation(n)\n        X_shuf = X_train[idx]\n        y_shuf = y_train[idx]\n\n        epoch_loss = 0.0\n\n        # Mini-batch training\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            Xb = X_shuf[start:end]\n            yb = y_shuf[start:end]\n\n            y_pred, cache = model.forward(Xb)\n            loss = model.compute_loss(yb, y_pred)\n            epoch_loss += loss * (end - start)\n\n            grads_w, grads_b = model.backward(cache, yb)\n            # model.update_parameters may expect learning_rate argument depending on your implementation:\n            # if your model.update_parameters signature accepts learning_rate, call with it; else set lr in model.\n            try:\n                model.update_parameters(grads_w, grads_b, learning_rate)\n            except TypeError:\n                # older signature without learning_rate; assume model has .lr attribute\n                model.update_parameters(grads_w, grads_b)\n\n        epoch_loss /= n\n        history['train_loss'].append(epoch_loss)\n\n        # Validation\n        y_val_pred, _ = model.forward(X_val)\n        val_loss = model.compute_loss(y_val, y_val_pred)\n        try:\n            val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())\n            val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())\n        except Exception:\n            val_auc = np.nan\n            val_ap  = np.nan\n\n        history['val_loss'].append(val_loss)\n        history['val_auc'].append(val_auc)\n        history['val_ap'].append(val_ap)\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"\n                  f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")\n\n        # Early stopping\n        if val_loss &lt; best_val_loss - 1e-6:\n            best_val_loss = val_loss\n            best_weights = ([W.copy() for W in model.weights],\n                            [b.copy() for b in model.biases])\n            patience = 0\n        else:\n            patience += 1\n        if patience &gt;= early_stopping:\n            if verbose: print(\"Early stopping triggered.\")\n            break\n\n    if best_weights is not None:\n        model.weights, model.biases = best_weights\n\n    return history\n</pre> def train_mlp(model, X_train, y_train, X_val, y_val,               epochs=50, batch_size=512, learning_rate=1e-2,               early_stopping=5, verbose=True):     \"\"\"     Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.     \"\"\"     # --- Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed     import pandas as pd     if isinstance(X_train, pd.DataFrame):         X_train = X_train.values     if isinstance(X_val, pd.DataFrame):         X_val = X_val.values      if hasattr(X_train, \"toarray\"):         X_train = X_train.toarray()     if hasattr(X_val, \"toarray\"):         X_val = X_val.toarray()      y_train = np.asarray(y_train).reshape(-1, 1)     y_val   = np.asarray(y_val).reshape(-1, 1)      n = X_train.shape[0]     history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}     best_val_loss = np.inf     patience = 0     best_weights = None      for epoch in range(1, epochs + 1):         # Shuffle training data (use permutation index and apply to numpy arrays)         idx = np.random.permutation(n)         X_shuf = X_train[idx]         y_shuf = y_train[idx]          epoch_loss = 0.0          # Mini-batch training         for start in range(0, n, batch_size):             end = min(start + batch_size, n)             Xb = X_shuf[start:end]             yb = y_shuf[start:end]              y_pred, cache = model.forward(Xb)             loss = model.compute_loss(yb, y_pred)             epoch_loss += loss * (end - start)              grads_w, grads_b = model.backward(cache, yb)             # model.update_parameters may expect learning_rate argument depending on your implementation:             # if your model.update_parameters signature accepts learning_rate, call with it; else set lr in model.             try:                 model.update_parameters(grads_w, grads_b, learning_rate)             except TypeError:                 # older signature without learning_rate; assume model has .lr attribute                 model.update_parameters(grads_w, grads_b)          epoch_loss /= n         history['train_loss'].append(epoch_loss)          # Validation         y_val_pred, _ = model.forward(X_val)         val_loss = model.compute_loss(y_val, y_val_pred)         try:             val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())             val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())         except Exception:             val_auc = np.nan             val_ap  = np.nan          history['val_loss'].append(val_loss)         history['val_auc'].append(val_auc)         history['val_ap'].append(val_ap)          if verbose:             print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"                   f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")          # Early stopping         if val_loss &lt; best_val_loss - 1e-6:             best_val_loss = val_loss             best_weights = ([W.copy() for W in model.weights],                             [b.copy() for b in model.biases])             patience = 0         else:             patience += 1         if patience &gt;= early_stopping:             if verbose: print(\"Early stopping triggered.\")             break      if best_weights is not None:         model.weights, model.biases = best_weights      return history In\u00a0[21]: Copied! <pre># Initialize and train MLP\nmlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5)\nhistory = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,\n                    epochs=100, batch_size=256, learning_rate=1e-2,\n                    early_stopping=7, verbose=False)\n</pre> # Initialize and train MLP mlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5) history = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,                     epochs=100, batch_size=256, learning_rate=1e-2,                     early_stopping=7, verbose=False) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 2\n      1 # Initialize and train MLP\n----&gt; 2 mlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5)\n      3 history = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,\n      4                     epochs=100, batch_size=256, learning_rate=1e-2,\n      5                     early_stopping=7, verbose=False)\n\nNameError: name 'X_train_proc' is not defined</pre> In\u00a0[22]: Copied! <pre># Plot training history\nepochs = np.arange(1, len(history['train_loss']) + 1)\n\nplt.figure(figsize=(12,4))\n\n# Loss plot\nplt.subplot(1,2,1)\nplt.plot(epochs, history['train_loss'], label='train_loss')\nplt.plot(epochs, history['val_loss'],   label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\n# Validation metrics plot (AUC and Average Precision)\nplt.subplot(1,2,2)\nif 'val_auc' in history and any(~np.isnan(history['val_auc'])):\n    plt.plot(epochs, history['val_auc'], label='val_ROC_AUC')\nif 'val_ap' in history and any(~np.isnan(history['val_ap'])):\n    plt.plot(epochs, history['val_ap'], label='val_PR_AUC')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.ylim(0.0, 1.0)\nplt.title('Validation AUC / PR-AUC vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot training history epochs = np.arange(1, len(history['train_loss']) + 1)  plt.figure(figsize=(12,4))  # Loss plot plt.subplot(1,2,1) plt.plot(epochs, history['train_loss'], label='train_loss') plt.plot(epochs, history['val_loss'],   label='val_loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Loss vs Epochs') plt.legend() plt.grid(alpha=0.2)  # Validation metrics plot (AUC and Average Precision) plt.subplot(1,2,2) if 'val_auc' in history and any(~np.isnan(history['val_auc'])):     plt.plot(epochs, history['val_auc'], label='val_ROC_AUC') if 'val_ap' in history and any(~np.isnan(history['val_ap'])):     plt.plot(epochs, history['val_ap'], label='val_PR_AUC') plt.xlabel('Epoch') plt.ylabel('Score') plt.ylim(0.0, 1.0) plt.title('Validation AUC / PR-AUC vs Epochs') plt.legend() plt.grid(alpha=0.2)  plt.tight_layout() plt.show()  <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 2\n      1 # Plot training history\n----&gt; 2 epochs = np.arange(1, len(history['train_loss']) + 1)\n      4 plt.figure(figsize=(12,4))\n      6 # Loss plot\n\nNameError: name 'history' is not defined</pre> <p>Analysis of Training Curves</p> <p>Both training and validation losses decrease steadily and plateau after ~80 epochs, indicating smooth convergence without instability. The small gap between the two curves suggests minimal overfitting.</p> <p>Validation ROC-AUC remains consistently high (~0.96) throughout training, while PR-AUC improves gradually before stabilizing\u2014showing the model learns to balance precision and recall effectively. Overall, the curves demonstrate a well-behaved optimization process and good generalization to unseen data.</p> In\u00a0[23]: Copied! <pre># Convert test data to dense if sparse\nX_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc\n\n# Predict probabilities and labels\ny_test_probs = mlp_model.predict_proba(X_test_eval)\ny_test_pred = (y_test_probs &gt;= 0.5).astype(int)\n\n# Metrics\nacc  = accuracy_score(y_test, y_test_pred)\nprec = precision_score(y_test, y_test_pred, zero_division=0)\nrec  = recall_score(y_test, y_test_pred, zero_division=0)\nf1   = f1_score(y_test, y_test_pred, zero_division=0)\nroc  = roc_auc_score(y_test, y_test_probs)\nap   = average_precision_score(y_test, y_test_probs)\n\nprint(\"=== Test Set Performance ===\")\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc:.4f}\")\nprint(f\"PR-AUC (AP): {ap:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})')\nplt.plot([0,1],[0,1],'--',color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n# Precision-Recall Curve\nprec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n</pre> # Convert test data to dense if sparse X_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc  # Predict probabilities and labels y_test_probs = mlp_model.predict_proba(X_test_eval) y_test_pred = (y_test_probs &gt;= 0.5).astype(int)  # Metrics acc  = accuracy_score(y_test, y_test_pred) prec = precision_score(y_test, y_test_pred, zero_division=0) rec  = recall_score(y_test, y_test_pred, zero_division=0) f1   = f1_score(y_test, y_test_pred, zero_division=0) roc  = roc_auc_score(y_test, y_test_probs) ap   = average_precision_score(y_test, y_test_probs)  print(\"=== Test Set Performance ===\") print(f\"Accuracy: {acc:.4f}\") print(f\"Precision: {prec:.4f}\") print(f\"Recall: {rec:.4f}\") print(f\"F1-score: {f1:.4f}\") print(f\"ROC-AUC: {roc:.4f}\") print(f\"PR-AUC (AP): {ap:.4f}\")  # Confusion matrix cm = confusion_matrix(y_test, y_test_pred) plt.figure(figsize=(5,4)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.title(\"Confusion Matrix (Test Set)\") plt.xlabel(\"Predicted\") plt.ylabel(\"Actual\") plt.show()  # ROC Curve fpr, tpr, _ = roc_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})') plt.plot([0,1],[0,1],'--',color='gray') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC Curve') plt.legend() plt.grid(alpha=0.2) plt.show()  # Precision-Recall Curve prec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})') plt.xlabel('Recall') plt.ylabel('Precision') plt.title('Precision-Recall Curve') plt.legend() plt.grid(alpha=0.2) plt.show() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 2\n      1 # Convert test data to dense if sparse\n----&gt; 2 X_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc\n      4 # Predict probabilities and labels\n      5 y_test_probs = mlp_model.predict_proba(X_test_eval)\n\nNameError: name 'X_test_proc' is not defined</pre> <p>Precision: 0.7276 Recall: 0.6601 F1-score: 0.6922 ROC-AUC: 0.9606 PR-AUC (AP): 0.7663</p> <p>On the test set, the MLP achieved 92.92% accuracy, 0.7276 precision, 0.6601 recall, and 0.6922 F1-score, with a strong ROC-AUC = 0.9606 and PR-AUC = 0.7663. These results confirm that the network discriminates well between classes while maintaining reasonable balance between false positives and false negatives. The high ROC-AUC indicates excellent overall separability, while the moderate PR-AUC reflects the challenge of the class imbalance. Overall, the model generalizes effectively to unseen data without significant overfitting, demonstrating a robust fit to this binary classification task.</p> In\u00a0[24]: Copied! <pre># Prepare test set for submission\n# Apply the EXACT same preprocessing as training data\ntest_work = test.copy()\n\n# Cap outliers (1st/99th percentile)\nfor col in numerical_features:\n    lower, upper = np.percentile(train[col], [1, 99])\n    test_work[col] = np.clip(test_work[col], lower, upper)\n\n# Handle pdays special case (replace -1 with 0)\nif \"pdays\" in test_work.columns:\n    test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)\n\n# Apply log transform to skewed features\ntest_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))\n\n# Apply the fitted preprocessor (StandardScaler + OneHotEncoder)\ntest_work_proc = preprocessor.transform(test_work)\n\n# Convert to dense if sparse\ntest_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc\n\n# Predict probabilities\ntest_probs = mlp_model.predict_proba(test_work_proc)\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    'id': test[id_col],\n    'y': test_probs\n})\nsubmission.to_csv(\"mlp_submission.csv\", index=False)\n</pre> # Prepare test set for submission # Apply the EXACT same preprocessing as training data test_work = test.copy()  # Cap outliers (1st/99th percentile) for col in numerical_features:     lower, upper = np.percentile(train[col], [1, 99])     test_work[col] = np.clip(test_work[col], lower, upper)  # Handle pdays special case (replace -1 with 0) if \"pdays\" in test_work.columns:     test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)  # Apply log transform to skewed features test_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))  # Apply the fitted preprocessor (StandardScaler + OneHotEncoder) test_work_proc = preprocessor.transform(test_work)  # Convert to dense if sparse test_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc  # Predict probabilities test_probs = mlp_model.predict_proba(test_work_proc)  # Prepare submission submission = pd.DataFrame({     'id': test[id_col],     'y': test_probs }) submission.to_csv(\"mlp_submission.csv\", index=False) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 3\n      1 # Prepare test set for submission\n      2 # Apply the EXACT same preprocessing as training data\n----&gt; 3 test_work = test.copy()\n      5 # Cap outliers (1st/99th percentile)\n      6 for col in numerical_features:\n\nNameError: name 'test' is not defined</pre> In\u00a0[25]: Copied! <pre># Submitting to Kaggle\n\n!kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\"\n</pre> # Submitting to Kaggle  !kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\" <pre>'kaggle' n\u00c6o \u201a reconhecido como um comando interno\nou externo, um programa oper\u00a0vel ou um arquivo em lotes.\n</pre> <p>Kaggle score: 0.96158 (public leaderboard)</p> <ul> <li>Leaderboard position: Within the top 68% of all public submissions.</li> <li>Reason for not reaching top 50%:<ul> <li>The MLP was implemented from scratch using only NumPy, without high-level optimization libraries like PyTorch or TensorFlow.</li> <li>State-of-the-art Kaggle submissions typically rely on ensemble methods (e.g., LightGBM, CatBoost) and extensive feature engineering, giving them a natural advantage on tabular datasets.</li> </ul> </li> </ul> <p></p>"},{"location":"Projetos/Projeto%201/notebook/#multi-layer-perceptron-mlp-classification","title":"Multi-Layer Perceptron (MLP) Classification\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#project-overview","title":"Project Overview\u00b6","text":"<p>This notebook implements a Multi-Layer Perceptron (MLP) neural network from scratch for binary classification on a real-world banking dataset.</p> <p>Authors: Rodrigo Medeiros, Matheus Castellucci, Jo\u00e3o Pedro Rodrigues</p>"},{"location":"Projetos/Projeto%201/notebook/#1-dataset-selection","title":"1. Dataset Selection\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#dataset-information","title":"Dataset Information\u00b6","text":"<p>Name: Binary Classification with a Bank Dataset Source: Kaggle Playground Series S5E8 Original Dataset: Bank Marketing Dataset</p> <p>Size:</p> <ul> <li>Training set: 750,000 rows \u00d7 18 columns</li> <li>Test set: 250,000 rows \u00d7 17 columns</li> <li>Features: 16 (7 numerical + 9 categorical)</li> <li>Target: Binary (subscription to term deposit: yes/no)</li> </ul> <p>Why this dataset?</p> <ul> <li>Real-world banking application (predicting term deposit subscriptions)</li> <li>Sufficient complexity with mixed feature types</li> <li>Class imbalance - realistic scenario</li> <li>Relevant to marketing and customer behavior prediction</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#2-dataset-explanation","title":"2. Dataset Explanation\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Numerical Features (7):</p> <ul> <li><code>age</code>: Client's age</li> <li><code>balance</code>: Average yearly balance (euros)</li> <li><code>day</code>: Last contact day of month</li> <li><code>duration</code>: Last contact duration (seconds)</li> <li><code>campaign</code>: Number of contacts during this campaign</li> <li><code>pdays</code>: Days since last contact from previous campaign (-1 = not contacted)</li> <li><code>previous</code>: Number of contacts before this campaign</li> </ul> <p>Categorical Features (9):</p> <ul> <li><code>job</code>: Type of job</li> <li><code>marital</code>: Marital status</li> <li><code>education</code>: Education level</li> <li><code>default</code>: Has credit in default?</li> <li><code>housing</code>: Has housing loan?</li> <li><code>loan</code>: Has personal loan?</li> <li><code>contact</code>: Contact communication type</li> <li><code>month</code>: Last contact month</li> <li><code>poutcome</code>: Outcome of previous campaign</li> </ul> <p>Target Variable:</p> <ul> <li><code>y</code>: Subscribed to term deposit? (0 = no, 1 = yes)</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#target-variable-distribution","title":"Target Variable Distribution\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#potential-issues-identified","title":"Potential Issues Identified\u00b6","text":"<ul> <li><p>Class Imbalance: ~88% negative class, ~12% positive class</p> </li> <li><p>Outliers: Several numerical features have extreme values (detected via histograms)</p> </li> <li><p>Skewed Distributions: Most numerical features are right-skewed</p> </li> <li><p>Mixed Feature Types: Requires encoding for categorical variables</p> </li> <li><p>Missing Values: No missing values detected</p> </li> <li><p>Duplicates: No duplicate rows detected</p> </li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#preprocessing-justification","title":"Preprocessing Justification\u00b6","text":"<ul> <li><p>Outlier Capping (1st\u201399th Percentile): Limits the influence of extreme values while preserving all rows, avoiding bias and data loss.</p> </li> <li><p>Log Transform (|skew| \u2265 1): Reduces heavy right skew in numerical features for more stable gradient updates.</p> </li> <li><p>StandardScaler: Centers and scales numeric features to improve MLP convergence.</p> </li> <li><p>OneHotEncoder: Encodes categorical variables as binary vectors; drop='first' prevents redundancy and handle_unknown='ignore' ensures consistency on new data.</p> </li> <li><p>Sparse Output: Saves memory with large one-hot matrices, converted to dense only if needed for training.</p> </li> <li><p>Stratified 70/15/15 Split: Preserves class proportions across train, validation, and test sets for unbiased evaluation.</p> </li> <li><p>Class Weights: Counteracts class imbalance (~12% positive) by adjusting loss contributions between classes.</p> </li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#4-mlp-implementation","title":"4. MLP Implementation\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#hyperparameters","title":"Hyperparameters\u00b6","text":"<ul> <li><p>Input Layer: matches the number of preprocessed features.</p> </li> <li><p>Hidden Layers: three fully connected layers with 128 and 64 neurons, each using ReLU activation to introduce non-linearity and mitigate vanishing gradients.</p> </li> <li><p>Output Layer: a single neuron with sigmoid activation to output probabilities for the binary target.</p> </li> <li><p>Initialization: weights are initialized with He initialization (N(0, sqrt(2/n_in))) to maintain stable activation variance across layers.</p> </li> <li><p>Loss Function: Binary Cross-Entropy (BCE) with optional L2 regularization, penalizing large weights to improve generalization.</p> </li> <li><p>Optimizer: standard Stochastic Gradient Descent (SGD) is used for parameter updates; gradient descent steps are handled in the training loop.</p> </li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#5-model-training","title":"5. Model Training\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#training-procedure","title":"Training Procedure\u00b6","text":"<p>The MLP was trained using mini-batch Stochastic Gradient Descent (SGD) implemented from scratch. Each epoch consists of the following steps:</p> <ol> <li><p>Data Shuffling and Mini-Batches \u2014 At the start of every epoch, the training data are randomly shuffled and divided into batches to improve gradient estimation and generalization.</p> </li> <li><p>Forward Propagation \u2014 For each batch, the model performs matrix multiplications and ReLU activations across layers to compute predicted probabilities.</p> </li> <li><p>Loss Computation \u2014 The Binary Cross-Entropy (BCE) loss is computed between predictions and true labels, with an additional L2 penalty on the weights to discourage overfitting.</p> </li> <li><p>Backpropagation \u2014 Gradients of the BCE + L2 loss with respect to every weight and bias are obtained via the chain rule.</p> <ul> <li>The ReLU derivative (<code>1 if z &gt; 0 else 0</code>) prevents vanishing gradients common in sigmoid/tanh activations.</li> <li>Intermediate activations are stored in a <code>cache</code> dictionary for reuse during gradient computation.</li> </ul> </li> <li><p>Parameter Update \u2014 Each layer\u2019s weights and biases are updated by</p> <p>$ W \\leftarrow W - \\eta\\,\\nabla_W L,\\qquad b \\leftarrow b - \\eta\\,\\nabla_b L $</p> <p>where the learning rate controls the step size.</p> </li> <li><p>Validation Evaluation \u2014 After every epoch, the model runs a forward pass on the validation set to track loss, ROC-AUC, and PR-AUC.</p> </li> <li><p>Early Stopping \u2014 If the validation loss fails to improve for a number of epochs, training stops and the best weights (lowest validation loss) are restored.</p> </li> </ol>"},{"location":"Projetos/Projeto%201/notebook/#training-challenges-and-solutions","title":"Training Challenges and Solutions\u00b6","text":"<ul> <li>Vanishing Gradients: Addressed by using ReLU activations and He initialization, which preserve gradient scale across layers.</li> <li>Overfitting: Controlled with L2 regularization and early stopping; validation loss and AUC were monitored each epoch.</li> <li>Instability in Loss: Occasional fluctuations caused by mini-batch noise; mitigated by averaging losses over all batches per epoch.</li> <li>Imbalanced Classes: Since positives represent only \u2248 12 % of the data, performance was evaluated primarily with ROC-AUC and PR-AUC rather than accuracy.</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy\u00b6","text":"<p>The dataset was already divided into train / validation / test sets using a 70 / 15 / 15 split. All splits were stratified to preserve the original class distribution (~12 % positive class).</p>"},{"location":"Projetos/Projeto%201/notebook/#validation-role-in-hyperparameter-tuning","title":"Validation role in hyperparameter tuning\u00b6","text":"<ul> <li>The validation set is crucial for evaluating alternative model configurations such as learning rate, hidden layer size, L2 regularization strength, and batch size.</li> <li>The main selection metric is validation ROC-AUC (<code>val_auc</code>), since accuracy is unreliable under class imbalance. PR-AUC serves as a secondary indicator of precision\u2013recall trade-offs.</li> <li>Typical tuning workflow:<ol> <li>Train each candidate model on the training set only.</li> <li>Evaluate on the validation set after every epoch to track learning progress.</li> <li>Select the configuration that yields the highest validation ROC-AUC.</li> <li>Retrain the best model on the combined training + validation data before the final test evaluation.</li> </ol> </li> <li>Early stopping monitors the same validation metric (<code>val_auc</code>) and halts training when no improvement is observed for a fixed number of epochs (patience = 7), avoiding unnecessary computation and overfitting.</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#8-evaluation-metrics","title":"8. Evaluation Metrics\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#kaggle-submission","title":"Kaggle Submission\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#conclusion-and-references","title":"Conclusion and References\u00b6","text":""},{"location":"Projetos/Projeto%201/notebook/#overall-findings","title":"Overall Findings\u00b6","text":"<p>This project successfully implemented a Multi-Layer Perceptron (MLP) from scratch using NumPy for binary classification on a real-world bank marketing dataset from Kaggle. The model achieved strong generalization with a ROC-AUC of 0.96 and a PR-AUC of 0.77 on the test set \u2014 values that demonstrate reliable separation of the two classes despite significant class imbalance (\u224812% positives).</p> <p>The MLP\u2019s final configuration \u2014 <code>[128, 64, 32]</code> hidden layers, ReLU activations, He initialization, L2 regularization, and early stopping \u2014 provided an effective balance between learning capacity and overfitting control. Training was stable, and validation curves showed smooth convergence without divergence or oscillation. Using validation ROC-AUC as the monitored metric ensured that model selection aligned with the project\u2019s primary goal: robust ranking performance under imbalance.</p>"},{"location":"Projetos/Projeto%201/notebook/#limitations","title":"Limitations\u00b6","text":"<ul> <li>Model simplicity: The MLP, while effective, lacks the representational efficiency of more advanced architectures like gradient-boosted trees (XGBoost/LightGBM) or deep ensembles commonly used for tabular data.</li> <li>Manual optimization: Without adaptive optimizers (e.g., Adam) or learning-rate schedules beyond simple decay, convergence speed and optimality might be limited.</li> <li>Feature interactions: The MLP relied solely on basic preprocessing; engineered interactions or embeddings could further enhance predictive performance.</li> <li>Computation time: Implementing backpropagation purely in NumPy is slower than using vectorized deep learning frameworks (e.g., PyTorch, TensorFlow).</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#references","title":"References\u00b6","text":"<ul> <li>Kaggle: Playground Series S5E8 \u2014 Bank Marketing Dataset</li> <li>UCI Machine Learning Repository: Bank Marketing Data Set</li> </ul>"},{"location":"Projetos/Projeto%201/notebook/#ai-usage","title":"AI Usage\u00b6","text":"<p>AI was used to help organize the notebook and debug code. Everything was verified and corrected by the authors.</p>"}]}