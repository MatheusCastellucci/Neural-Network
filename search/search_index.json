{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ementa","text":"Teste GitHub Pages Teste com Lorem Ipsum <p>Esse \u00e9 um exemplo de p\u00e1gina simples para GitHub Pages.</p> Se\u00e7\u00e3o 1 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet felis in nunc fringilla ullamcorper. Proin non lacus vitae ligula pulvinar facilisis.</p> Se\u00e7\u00e3o 2 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus vitae venenatis ligula. Ut malesuada augue nec mi tempor, eu malesuada libero hendrerit.</p> Se\u00e7\u00e3o 3 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tincidunt, ipsum at sagittis tincidunt, risus ipsum cursus lorem, non dictum ipsum sapien quis elit.</p> <p>Rodap\u00e9 - P\u00e1gina de Teste</p>"},{"location":"Exercicios/EX1/data/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd <p>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class</p> In\u00a0[2]: Copied! <pre>np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.\n\n# M\u00e9dias e desvios para cada classe\nmeans = [(2, 3), (5, 6), (8, 1), (15, 4)]\nstds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]\n\ndata = []\nlabels = []\n\nfor i in range(len(means)):\n    mean = means[i]\n    std = stds[i]\n\n    x = np.random.normal(loc=mean[0], scale=std[0], size=100)\n    y = np.random.normal(loc=mean[1], scale=std[1], size=100)\n\n    points = []\n    for j in range(100):\n        points.append([x[j], y[j]])\n\n    data.extend(points)\n\n    for j in range(100):\n        labels.append(i)\n\ndata = np.array(data)\nlabels = np.array(labels)\n</pre> np.random.seed(42)  # Essa fun\u00e7\u00e3o usa sementes que sempre ir\u00e3o gerar a mesma sequencia randomica.  # M\u00e9dias e desvios para cada classe means = [(2, 3), (5, 6), (8, 1), (15, 4)] stds = [(0.8, 2.5), (1.2, 1.9), (0.9, 0.9), (0.5, 2)]  data = [] labels = []  for i in range(len(means)):     mean = means[i]     std = stds[i]      x = np.random.normal(loc=mean[0], scale=std[0], size=100)     y = np.random.normal(loc=mean[1], scale=std[1], size=100)      points = []     for j in range(100):         points.append([x[j], y[j]])      data.extend(points)      for j in range(100):         labels.append(i)  data = np.array(data) labels = np.array(labels) <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nfor cls in range(4):\n    plt.scatter(data[labels == cls, 0],\n                data[labels == cls, 1],\n                label=f'Classe {cls}',\n                alpha=0.7)\nplt.legend()\nplt.xlabel('Eixo X')\nplt.ylabel('Eixo Y')\nplt.title('Dispers\u00e3o das 4 classes em 2D')\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) for cls in range(4):     plt.scatter(data[labels == cls, 0],                 data[labels == cls, 1],                 label=f'Classe {cls}',                 alpha=0.7) plt.legend() plt.xlabel('Eixo X') plt.ylabel('Eixo Y') plt.title('Dispers\u00e3o das 4 classes em 2D') plt.show() <p>Analyze and Draw Boundaries:</p> <ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> <p>Answers</p> <ol> <li>O scatter plot mostra que as quatro classes est\u00e3o distribuidas de forma relativamente clara, com alguma sobreposi\u00e7\u00e3o entre elas. As classes 0 e 1 est\u00e3o mais pr\u00f3ximas uma da outra, enquanto as classes 2 ainda encosta um pouco na classe 1, e longe de todas as outras temos a classe 3.</li> <li>N\u00e3o, uma fronteira linear simples n\u00e3o seria capaz de separar todas as classes de forma eficaz, especialmente devido \u00e0 sobreposi\u00e7\u00e3o entre as classes 0 e 1.</li> <li>Pode ser visto no gr\u00e1fico abaixo:</li> </ol> <p></p> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B.</p> In\u00a0[4]: Copied! <pre>mu_A = [0, 0, 0, 0, 0]\nSigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1x.0]])\n\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],\n                    [-0.7, 1.5,  0.4, 0.0, 0.0],\n                    [0.2,  0.4,  1.5, 0.6, 0.0],\n                    [0.0,  0.0,  0.6, 1.5, 0.3],\n                    [0.0,  0.0,  0.0, 0.3, 1.5]])\n\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)\n\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)\n</pre> mu_A = [0, 0, 0, 0, 0] Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],                     [0.8, 1.0, 0.3, 0.0, 0.0],                     [0.1, 0.3, 1.0, 0.5, 0.0],                     [0.0, 0.0, 0.5, 1.0, 0.2],                     [0.0, 0.0, 0.0, 0.2, 1x.0]])  mu_B = [1.5, 1.5, 1.5, 1.5, 1.5] Sigma_B = np.array([[1.5, -0.7,  0.2, 0.0, 0.0],                     [-0.7, 1.5,  0.4, 0.0, 0.0],                     [0.2,  0.4,  1.5, 0.6, 0.0],                     [0.0,  0.0,  0.6, 1.5, 0.3],                     [0.0,  0.0,  0.0, 0.3, 1.5]])  XA = np.random.multivariate_normal(mu_A, Sigma_A, size=500) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=500)  X = np.vstack([XA, XB]) y = np.array([0]*500 + [1]*500) <pre>\n  Cell In[4], line 6\n    [0.0, 0.0, 0.0, 0.2, 1x.0]])\n                         ^\nSyntaxError: invalid decimal literal\n</pre> <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>PCA:</p> <ol> <li>Centralizar os dados (tirar a m\u00e9dia).</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Extrair autovalores e autovetores da matriz de covari\u00e2ncia.</li> <li>Ordenar autovetores pelos maiores autovalores.</li> <li>Projetar os dados nos autovetores escolhidos.</li> </ol> In\u00a0[5]: Copied! <pre>def my_pca(X, n_components=None):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered, rowvar=False)\n    autovalores, autovetores = np.linalg.eigh(cov_matrix)\n\n    sorted_idx = np.argsort(autovalores)[::-1]\n    autovalores = autovalores[sorted_idx]\n    autovetores = autovetores[:, sorted_idx]\n\n    total_var = np.sum(autovalores)\n\n    if n_components is not None:\n        autovetores = autovetores[:, :n_components]\n        autovalores = autovalores[:n_components]\n\n    X_pca = np.dot(X_centered, autovetores)\n\n    return X_pca, autovetores, autovalores, total_var\n</pre> def my_pca(X, n_components=None):     X_centered = X - np.mean(X, axis=0)     cov_matrix = np.cov(X_centered, rowvar=False)     autovalores, autovetores = np.linalg.eigh(cov_matrix)      sorted_idx = np.argsort(autovalores)[::-1]     autovalores = autovalores[sorted_idx]     autovetores = autovetores[:, sorted_idx]      total_var = np.sum(autovalores)      if n_components is not None:         autovetores = autovetores[:, :n_components]         autovalores = autovalores[:n_components]      X_pca = np.dot(X_centered, autovetores)      return X_pca, autovetores, autovalores, total_var In\u00a0[6]: Copied! <pre>Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\nplt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B')\nplt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)')\nplt.legend(); plt.show()\n\nprint('Vari\u00e2ncia explicada:', autovalores / total_var)\nprint('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var))\n</pre> Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)  plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A') plt.scatter(Z[y==1,0], Z[y==1,1], alpha=0.7, label='Classe B') plt.xlabel('PC1'); plt.ylabel('PC2'); plt.title('PCA (5D \u2192 2D)') plt.legend(); plt.show()  print('Vari\u00e2ncia explicada:', autovalores / total_var) print('Vari\u00e2ncia Acumulada:', np.sum(autovalores / total_var)) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 Z, autovetores, autovalores, total_var = my_pca(X, n_components=2)\n      3 plt.figure(figsize=(7,6))\n      4 plt.scatter(Z[y==0,0], Z[y==0,1], alpha=0.7, label='Classe A')\n\nNameError: name 'X' is not defined</pre> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> <p>Answers</p> <ol> <li>As duas classes est\u00e3o bem pr\u00f3ximas uma da outra, com uma quantidade significativa de sobreposi\u00e7\u00e3o. Caso n\u00e3o fossem duas classes diferentes, poder\u00edamos considerar que se tratam de uma \u00fanica classe.</li> <li>N\u00e3o \u00e9 possivel tra\u00e7ar uma linha que separa as duas classes de forma eficaz, sempre haver\u00e1 uma \u00e1rea de sobreposi\u00e7\u00e3o. Modelos lineares simples, como um Perceptron ou Regress\u00e3o Log\u00edstica, v\u00e3o ter dificuldade porque s\u00f3 conseguem aprender fronteiras lineares (hiperplanos). Eles errariam bastante nos pontos da regi\u00e3o central de overlap.</li> </ol> <p>2. Describe the Data:</p> <ol> <li>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</li> <li>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ol> <p>Answers:</p> <ol> <li><p>O dataset busca prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da Spaceship Titanic. Isso pode ser visto na coluna \"Transported\", que \u00e9 a coluna-alvo (sendo booleano).</p> </li> <li><p>Features: num\u00e9ricas vs categ\u00f3ricas</p> <p>Num\u00e9ricas</p> <ul> <li><p>Age \u2192 idade do passageiro</p> </li> <li><p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 valores gastos em diferentes servi\u00e7os</p> </li> </ul> <p>Categ\u00f3ricas</p> <ul> <li><p>HomePlanet \u2192 planeta de origem (ex: Earth, Europa, Mars)</p> </li> <li><p>CryoSleep \u2192 booleano (se o passageiro entrou em sono criog\u00eanico)</p> </li> <li><p>Cabin \u2192 cont\u00e9m m\u00faltiplas infos (deck/num/side). Pode ser decomposta em:</p> <ul> <li><p>Deck (categ\u00f3rica)</p> </li> <li><p>Num (num\u00e9rica)</p> </li> <li><p>Side (P/S \u2192 categ\u00f3rica bin\u00e1ria)</p> </li> </ul> </li> <li><p>Destination \u2192 destino da viagem (ex: TRAPPIST-1e, etc.)</p> </li> <li><p>VIP \u2192 booleano (pagou servi\u00e7o VIP)</p> </li> <li><p>Name \u2192 geralmente descartado (n\u00e3o tem valor preditivo direto)</p> </li> <li><p>PassengerId \u2192 identificador \u00fanico (n\u00e3o usado como feature)</p> </li> </ul> </li> <li><p>Colunas com valores faltantes:</p> </li> </ol> In\u00a0[7]: Copied! <pre>csv_path = \"train.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\nprint(\"Colunas:\", list(df.columns))\n</pre> csv_path = \"train.csv\" df = pd.read_csv(csv_path) print(\"Shape:\", df.shape) print(\"Colunas:\", list(df.columns)) <pre>Shape: (8693, 14)\nColunas: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n</pre> In\u00a0[8]: Copied! <pre>missing = df.isnull().sum()\nmissing_percent = 100 * missing / len(df)\nmissing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent})\nprint(missing_df[missing_df[\"Missing Values\"] &gt; 0])\n</pre> missing = df.isnull().sum() missing_percent = 100 * missing / len(df) missing_df = pd.DataFrame({\"Missing Values\": missing, \"Percent\": missing_percent}) print(missing_df[missing_df[\"Missing Values\"] &gt; 0]) <pre>              Missing Values   Percent\nHomePlanet               201  2.312205\nCryoSleep                217  2.496261\nCabin                    199  2.289198\nDestination              182  2.093639\nAge                      179  2.059128\nVIP                      203  2.335212\nRoomService              181  2.082135\nFoodCourt                183  2.105142\nShoppingMall             208  2.392730\nSpa                      183  2.105142\nVRDeck                   188  2.162660\nName                     200  2.300702\n</pre> <p>3. Preprocessing the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ol> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ol> In\u00a0[9]: Copied! <pre>df_processed = df.copy()\n\n\"\"\"Num\u00e9ricas: \nSolu\u00e7\u00e3o: Preencher com mediana, \nJustificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o.\n\"\"\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    median_val = df_processed[col].median()\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(median_val, inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")\n</pre> df_processed = df.copy()  \"\"\"Num\u00e9ricas:  Solu\u00e7\u00e3o: Preencher com mediana,  Justificativa: Precisamos de algum valor que n\u00e3o comprometa a distribui\u00e7\u00e3o dos dados. A m\u00e9dia teria muita influ\u00eancia de outliers, mas a mediana n\u00e3o, assim chegamos \u00e0 conclus\u00e3o de que a mediana \u00e9 a melhor op\u00e7\u00e3o. \"\"\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] for col in num_cols:     median_val = df_processed[col].median()     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(median_val, inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos com {median_val:.2f}\")  <pre>   - Age: 179 valores preenchidos com 27.00\n   - RoomService: 181 valores preenchidos com 0.00\n   - FoodCourt: 183 valores preenchidos com 0.00\n   - ShoppingMall: 208 valores preenchidos com 0.00\n   - Spa: 183 valores preenchidos com 0.00\n   - VRDeck: 188 valores preenchidos com 0.00\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\nC:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\3205331934.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(median_val, inplace=True)\n</pre> In\u00a0[10]: Copied! <pre>\"\"\"Categ\u00f3ricas: \nSolu\u00e7\u00e3o: Preencher com \"Unknown\"\nJustificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios.\n\"\"\"\ncat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]\nfor col in cat_cols:\n    missing_count = df_processed[col].isnull().sum()\n    df_processed[col].fillna(\"Unknown\", inplace=True)\n    print(f\"   - {col}: {missing_count} valores preenchidos\")\n</pre> \"\"\"Categ\u00f3ricas:  Solu\u00e7\u00e3o: Preencher com \"Unknown\" Justificativa: Colocando \"Unknown\" evitamos distorcer a an\u00e1lise dos dados, uma vez que n\u00e3o criamos dados fict\u00edcios. \"\"\" cat_cols = [\"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"] for col in cat_cols:     missing_count = df_processed[col].isnull().sum()     df_processed[col].fillna(\"Unknown\", inplace=True)     print(f\"   - {col}: {missing_count} valores preenchidos\")  <pre>   - HomePlanet: 201 valores preenchidos\n   - Destination: 182 valores preenchidos\n   - CryoSleep: 217 valores preenchidos\n   - VIP: 203 valores preenchidos\n</pre> <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\797796973.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[col].fillna(\"Unknown\", inplace=True)\n</pre> In\u00a0[11]: Copied! <pre>\"\"\"Cabin: \nSolu\u00e7\u00e3o: Separar em 3 colunas\nJustificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes.\n\"\"\"\ncabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True)\ndf_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\")\ndf_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")\n\n# Preencher CabinNum com mediana\ncabin_num_median = df_processed[\"CabinNum\"].median()\ndf_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> \"\"\"Cabin:  Solu\u00e7\u00e3o: Separar em 3 colunas Justificativa: Precisamos de uma forma de representar as informa\u00e7\u00f5es contidas na coluna \"Cabin\" sem perder dados importantes. \"\"\" cabin_split = df_processed[\"Cabin\"].str.split(\"/\", expand=True) df_processed[\"Deck\"] = cabin_split[0].fillna(\"Unknown\") df_processed[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\") df_processed[\"Side\"] = cabin_split[2].fillna(\"Unknown\")  # Preencher CabinNum com mediana cabin_num_median = df_processed[\"CabinNum\"].median() df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True) <pre>C:\\Users\\matra\\AppData\\Local\\Temp\\ipykernel_14192\\2304158112.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_processed[\"CabinNum\"].fillna(cabin_num_median, inplace=True)\n</pre> In\u00a0[12]: Copied! <pre>\"\"\"Name: \nSolu\u00e7\u00e3o: descartar\nJustificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido.\n\"\"\"\ndf_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)    \n\nprint(\"Ap\u00f3s tratamento:\", df_processed.shape)\n</pre> \"\"\"Name:  Solu\u00e7\u00e3o: descartar Justificativa: O nome n\u00e3o \u00e9 uma informa\u00e7\u00e3o relevante para a an\u00e1lise e pode ser removido. \"\"\" df_processed.drop(columns=[\"Cabin\", \"Name\", \"PassengerId\"], inplace=True)      print(\"Ap\u00f3s tratamento:\", df_processed.shape) <pre>Ap\u00f3s tratamento: (8693, 14)\n</pre> In\u00a0[13]: Copied! <pre>df_encoded = df_processed.copy()\n\n# Mapeamento booleano/tri-estado\nboolean_mappings = {\n    \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},\n    \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1}\n}\nfor col, mapping in boolean_mappings.items():\n    if col in df_encoded.columns:\n        df_encoded[col] = df_encoded[col].astype(str).map(mapping)\n        print(f\"   - {col}: {mapping}\")\n\n# One-hot para categ\u00f3ricas\ncategorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns]\nfor col in categorical_cols:\n    unique_values = df_encoded[col].astype(str).unique()\n    print(f\"   - {col}: {len(unique_values)} categorias\")\n\ndf_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False)\nprint(\"Ap\u00f3s encoding:\", df_encoded.shape)\n</pre> df_encoded = df_processed.copy()  # Mapeamento booleano/tri-estado boolean_mappings = {     \"CryoSleep\": {\"True\": 1, \"False\": 0, \"Unknown\": -1},     \"VIP\": {\"True\": 1, \"False\": 0, \"Unknown\": -1} } for col, mapping in boolean_mappings.items():     if col in df_encoded.columns:         df_encoded[col] = df_encoded[col].astype(str).map(mapping)         print(f\"   - {col}: {mapping}\")  # One-hot para categ\u00f3ricas categorical_cols = [c for c in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"] if c in df_encoded.columns] for col in categorical_cols:     unique_values = df_encoded[col].astype(str).unique()     print(f\"   - {col}: {len(unique_values)} categorias\")  df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=False) print(\"Ap\u00f3s encoding:\", df_encoded.shape) <pre>   - CryoSleep: {'True': 1, 'False': 0, 'Unknown': -1}\n   - VIP: {'True': 1, 'False': 0, 'Unknown': -1}\n   - HomePlanet: 4 categorias\n   - Destination: 4 categorias\n   - Deck: 9 categorias\n   - Side: 3 categorias\nAp\u00f3s encoding: (8693, 30)\n</pre> In\u00a0[14]: Copied! <pre>print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\")\nprint(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")\n\ndef minmax_scale_to_neg1_pos1(series):\n    return 2 * ((series - series.min()) / (series.max() - series.min())) - 1\n\ndf_normalized = df_encoded.copy()\noriginal_data = {}\n\nscaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns]\nfor col in scaling_cols:\n    original_data[col] = df_normalized[col].copy()\n    original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"\n    df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])\n    normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"\n    print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")\n\nprint(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape)\n</pre> print(\"\\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\") print(\"M\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\")  def minmax_scale_to_neg1_pos1(series):     return 2 * ((series - series.min()) / (series.max() - series.min())) - 1  df_normalized = df_encoded.copy() original_data = {}  scaling_cols = [c for c in [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"CabinNum\"] if c in df_normalized.columns] for col in scaling_cols:     original_data[col] = df_normalized[col].copy()     original_range = f\"[{df_normalized[col].min():.1f}, {df_normalized[col].max():.1f}]\"     df_normalized[col] = minmax_scale_to_neg1_pos1(df_normalized[col])     normalized_range = f\"[{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]\"     print(f\"   - {col}: {original_range} \u2192 {normalized_range}\")  print(\"Ap\u00f3s normaliza\u00e7\u00e3o:\", df_normalized.shape) <pre>\n=== NORMALIZA\u00c7\u00c3O DAS FEATURES NUM\u00c9RICAS ===\nM\u00e9todo: Min-Max para range [-1, 1] \u2014 bom p/ tanh\n   - Age: [0.0, 79.0] \u2192 [-1.000, 1.000]\n   - RoomService: [0.0, 14327.0] \u2192 [-1.000, 1.000]\n   - FoodCourt: [0.0, 29813.0] \u2192 [-1.000, 1.000]\n   - ShoppingMall: [0.0, 23492.0] \u2192 [-1.000, 1.000]\n   - Spa: [0.0, 22408.0] \u2192 [-1.000, 1.000]\n   - VRDeck: [0.0, 24133.0] \u2192 [-1.000, 1.000]\n   - CabinNum: [0.0, 1894.0] \u2192 [-1.000, 1.000]\nAp\u00f3s normaliza\u00e7\u00e3o: (8693, 30)\n</pre> In\u00a0[15]: Copied! <pre>target_col = \"Transported\"\nX = df_normalized.drop(columns=[target_col]).values\ny = df_normalized[target_col].map({True: 1, False: 0}).values\nprint(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n</pre> target_col = \"Transported\" X = df_normalized.drop(columns=[target_col]).values y = df_normalized[target_col].map({True: 1, False: 0}).values print(\"X shape:\", X.shape, \"| y shape:\", y.shape) <pre>X shape: (8693, 29) | y shape: (8693,)\n</pre> <p>4. Visualize the Data:</p> <ul> <li>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation</li> </ul> In\u00a0[16]: Copied! <pre>for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:\n    plt.figure(figsize=(12, 5))\n\n    # Antes\n    plt.subplot(1, 2, 1)\n    plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    # Depois\n    plt.subplot(1, 2, 2)\n    plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")\n    plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")\n    plt.xlabel(f\"{col} (normalizado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n\n    plt.tight_layout()\n    plt.show()\n</pre> for col in [c for c in [\"Age\", \"FoodCourt\"] if c in original_data]:     plt.figure(figsize=(12, 5))      # Antes     plt.subplot(1, 2, 1)     plt.hist(original_data[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Antes da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (original)\")     plt.ylabel(\"Frequ\u00eancia\")      # Depois     plt.subplot(1, 2, 2)     plt.hist(df_normalized[col].values, bins=30, alpha=0.7, edgecolor=\"black\")     plt.title(f\"{col} - Depois da Normaliza\u00e7\u00e3o\")     plt.xlabel(f\"{col} (normalizado)\")     plt.ylabel(\"Frequ\u00eancia\")      plt.tight_layout()     plt.show()"},{"location":"Exercicios/EX1/data/#1-data","title":"1. Data\u00b6","text":""},{"location":"Exercicios/EX1/data/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks\u00b6","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"Exercicios/EX1/data/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"Exercicios/EX1/data/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"Exercicios/EX1/data/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Preparing Real-World Data for a Neural Network This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"}]}